"2011.10468","Jen-Tsung Hsiang","Jen-Tsung Hsiang and B. L. Hu","Nonequilibrium Quantum Free Energy and Effective Temperature, Generating
  Functional and Influence Action","26 pages, 1 figure. No change in v2, added further comments","Phys. Rev. D 103, 065001 (2021)","10.1103/PhysRevD.103.065001",,"cond-mat.stat-mech hep-th quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A definition of nonequilibrium free energy $\mathcal{F}_{\textsc{s}}$ is
proposed for dynamical Gaussian quantum open systems strongly coupled to a heat
bath and a formal derivation is provided by way of the generating functional in
terms of the coarse-grained effective action and the influence action. For
Gaussian open quantum systems exemplified by the quantum Brownian motion model
studied here, a time-varying effective temperature can be introduced in a
natural way, and with it, the nonequilibrium free energy
$\mathcal{F}_{\textsc{s}}$, von Neumann entropy $\mathcal{S}_{vN}$ and internal
energy $\mathcal{U}_{\textsc{s}}$ of the reduced system ($S$) can be defined
accordingly. In contrast to the nonequilibrium free energy found in the
literature which references the bath temperature, the nonequilibrium
thermodynamic functions we find here obey the familiar relation
$\mathcal{F}_{\textsc{s}}(t)=\mathcal{U}_{\textsc{s}}(t)- T_{\textsc{eff}}
(t)\,\mathcal{S}_{vN}(t)$ {\it at any and all moments of time} in the system's
fully nonequilibrium evolution history. After the system equilibrates they
coincide, in the weak coupling limit, with their counterparts in conventional
equilibrium thermodynamics. Since the effective temperature captures both the
state of the system and its interaction with the bath, upon the system's
equilibration, it approaches a value slightly higher than the initial bath
temperature. Notably, it remains nonzero for a zero-temperature bath, signaling
the existence of system-bath entanglement. Reasonably, at high bath
temperatures and under ultra-weak couplings, it becomes indistinguishable from
the bath temperature. The nonequilibrium thermodynamic functions and relations
discovered here for dynamical Gaussian quantum systems should open up useful
pathways toward establishing meaningful theories of nonequilibrium quantum
thermodynamics.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:01:11 GMT""},{""version"":""v2"",""created"":""Fri, 4 Dec 2020 15:18:47 GMT""}]","2021-03-10"
"2011.10469","Sam Davis","Sam Davis, Giuseppe Coccia, Sam Gooch, Julian Mack","Empirical Evaluation of Deep Learning Model Compression Techniques on
  the WaveNet Vocoder",,,,,"cs.LG cs.SD eess.AS","http://creativecommons.org/licenses/by/4.0/","  WaveNet is a state-of-the-art text-to-speech vocoder that remains challenging
to deploy due to its autoregressive loop. In this work we focus on ways to
accelerate the original WaveNet architecture directly, as opposed to modifying
the architecture, such that the model can be deployed as part of a scalable
text-to-speech system. We survey a wide variety of model compression techniques
that are amenable to deployment on a range of hardware platforms. In
particular, we compare different model sparsity methods and levels, and seven
widely used precisions as targets for quantization; and are able to achieve
models with a compression ratio of up to 13.84 without loss in audio fidelity
compared to a dense, single-precision floating-point baseline. All techniques
are implemented using existing open source deep learning frameworks and
libraries to encourage their wider adoption.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:01:56 GMT""}]","2020-11-23"
"2011.10473","Charlotte Langlais","Israa Khaled (IMT Atlantique - ELEC, LU), Charlotte Langlais (IMT
  Atlantique - ELEC), Ammar Falou (LU), Bachar Elhassan (LU), Michel Jezequel
  (IMT Atlantique - ELEC)","Low-Complexity Angle-Domain MIMO NOMA System with partial channel state
  information for MmWave Communications *",,,,,"cs.NI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In millimeter-wave communication, digital beamsteering (DBS), only based on
the user direction, is a promising angle-domain multi-antenna technique to
mitigate the severe path loss and multiuser interference, with low-complexity
and partial channel state information (CSI). In this paper, we design a
power-domain non-orthogonal multiple access (NOMA) scheme that enhances the DBS
performance trading-off complexity , energy-consumption and capacity
performance. In particular, we propose a user-clustering algorithm to pair
users, based on a geometric-interference metric, so that the inter-user
interference is reduced. Afterward, based on a fixed inter-cluster power
allocation, we derive analytically a sub-optimal intra-cluster power allocation
optimization problem to maximize the network throughput. To address the issue
of partial CSI, we rewrite the aforementioned optimization problem, by relying
only on the user direction. Performance evaluation of the proposed schemes is
developed in rural environment, based on the New York University
millimeter-wave simulator. The obtained results demonstrate that the proposed
low-complexity NOMA-DBS schemes with either full-or partial-CSI achieve
significant performance improvement over the classical DBS, in terms of
spectral-and energy-efficiencies (up to 26.8% bps/Hz rate gain for 45 users
using the proposed scheme with partial CSI).
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:09:25 GMT""}]","2020-11-23"
"2011.10474","Luca Varotto","L. Varotto, A. Cenedese, A. Cavallaro","Probabilistic Radio-Visual Active Sensing for Search and Tracking","6 pages, 3 figures, 1 table, accepted at ECC 2021",,,,"cs.RO cs.SY eess.SY","http://creativecommons.org/publicdomain/zero/1.0/","  Active Search and Tracking for search and rescue missions or collaborative
mobile robotics relies on the actuation of a sensing platform to detect and
localize a target. In this paper we focus on visually detecting a
radio-emitting target with an aerial robot equipped with a radio receiver and a
camera. Visual-based tracking provides high accuracy, but the directionality of
the sensing domain may require long search times before detecting the target.
Conversely, radio signals have larger coverage, but lower tracking accuracy.
Thus, we design a Recursive Bayesian Estimation scheme that uses camera
observations to refine radio measurements. To regulate the camera pose, we
design an optimal controller whose cost function is built upon a probabilistic
map. Theoretical results support the proposed algorithm, while numerical
analyses show higher robustness and efficiency with respect to visual and
radio-only baselines.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:09:51 GMT""},{""version"":""v2"",""created"":""Sun, 11 Apr 2021 13:19:01 GMT""}]","2021-04-13"
"2011.10475","Jong Chul Ye","Eunju Cha, Chanseok Lee, Mooseok Jang, and Jong Chul Ye","DeepPhaseCut: Deep Relaxation in Phase for Unsupervised Fourier Phase
  Retrieval",,,,,"cs.CV cs.LG eess.IV stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Fourier phase retrieval is a classical problem of restoring a signal only
from the measured magnitude of its Fourier transform. Although Fienup-type
algorithms, which use prior knowledge in both spatial and Fourier domains, have
been widely used in practice, they can often stall in local minima. Modern
methods such as PhaseLift and PhaseCut may offer performance guarantees with
the help of convex relaxation. However, these algorithms are usually
computationally intensive for practical use. To address this problem, we
propose a novel, unsupervised, feed-forward neural network for Fourier phase
retrieval which enables immediate high quality reconstruction. Unlike the
existing deep learning approaches that use a neural network as a regularization
term or an end-to-end blackbox model for supervised training, our algorithm is
a feed-forward neural network implementation of PhaseCut algorithm in an
unsupervised learning framework. Specifically, our network is composed of two
generators: one for the phase estimation using PhaseCut loss, followed by
another generator for image reconstruction, all of which are trained
simultaneously using a cycleGAN framework without matched data. The link to the
classical Fienup-type algorithms and the recent symmetry-breaking learning
approach is also revealed. Extensive experiments demonstrate that the proposed
method outperforms all existing approaches in Fourier phase retrieval problems.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:10:08 GMT""}]","2020-11-23"
"2011.10476","Charanpreet Singh","Charanpreet Singh, Vikram Singh, Gyandeep Pradhan, Velaga Srihari,
  Himanshu Kumar Poswal, Ramesh Nath, Ashis K. Nandy, and Ajaya K. Nayak","Pressure controlled trimerization for switching of anomalous Hall effect
  in triangular antiferromagnet Mn$_3$Sn",,"Phys. Rev. Research 2, 043366 (2020)","10.1103/PhysRevResearch.2.043366",,"cond-mat.mtrl-sci","http://creativecommons.org/licenses/by/4.0/","  Here, we present a detailed theoretical and experimental study on the
pressure induced switching of anomalous Hall effect (AHE) in the triangular
antiferromagnetic (AFM) compound Mn$_3$Sn. Our theoretical model suggests
pressure driven significant splitting of the in-plane Mn bond lengths $i.e.$ an
effective trimerization, which in turn stabilizes a helical AFM ground state by
modifying the inter-plane exchange parameters in the system. We experimentally
demonstrate that the AHE in Mn$_3$Sn reduces from 5$\mu\Omega$ cm at ambient
pressure to zero at an applied pressure of about 1.5 GPa. Furthermore, our
pressure dependent magnetization study reveals that the conventional triangular
AFM ground state of Mn$_3$Sn systematically transforms into the helical AFM
phase where the symmetry does not support a non-vanishing Berry curvature
required for the realization of a finite AHE. The pressure dependent x-ray
diffraction (XRD) study rules out any role of structural phase transition in
the observed phenomenon. In addition, the temperature dependent in-plane
lattice parameter at ambient pressure is found to deviate from the monotonic
behavior when the system enters into the helical AFM phase, thereby, supporting
the proposed impact of trimerization in controlling the AHE. We believe that
the present study makes an important contribution towards understanding the
stabilization mechanism of different magnetic ground states in Mn$_3$Sn and
related materials for their potential applications pertaining to AHE switching.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:10:55 GMT""}]","2020-12-18"
"2011.10477","Chenyang Xu","Chenyang Xu","K-stability of Fano varieties: an algebro-geometric approach",,,,,"math.AG","http://creativecommons.org/publicdomain/zero/1.0/","  We give a survey of the recent progress on the study of K-stability of Fano
varieties by an algebro-geometric approach.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:11:52 GMT""}]","2020-11-23"
"2011.10478","Grigorios G. Anagnostopoulos Dr.","Grigorios G. Anagnostopoulos and Alexandros Kalousis","Analysing the Data-Driven Approach of Dynamically Estimating Positioning
  Accuracy","Author's accepted manuscript version. Accepted for publication in
  IEEE ICC 2021, IoT and Sensor Networks Symposium",,,,"eess.SP cs.LG","http://creativecommons.org/licenses/by/4.0/","  The primary expectation from positioning systems is for them to provide the
users with reliable estimates of their position. An additional piece of
information that can greatly help the users utilize position estimates is the
level of uncertainty that a positioning system assigns to the position estimate
it produced. The concept of dynamically estimating the accuracy of position
estimates of fingerprinting positioning systems has been sporadically discussed
over the last decade in the literature of the field, where mainly handcrafted
rules based on domain knowledge have been proposed. The emergence of IoT
devices and the proliferation of data from Low Power Wide Area Networks
(LPWANs) have facilitated the conceptualization of data-driven methods of
determining the estimated certainty over position estimates. In this work, we
analyze the data-driven approach of determining the Dynamic Accuracy Estimation
(DAE), considering it in the broader context of a positioning system. More
specifically, with the use of a public LoRaWAN dataset, the current work
analyses: the repartition of the available training set between the tasks of
determining the location estimates and the DAE, the concept of selecting a
subset of the most reliable estimates, and the impact that the spatial
distribution of the data has to the accuracy of the DAE. The work provides a
wide overview of the data-driven approach of DAE determination in the context
of the overall design of a positioning system.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:18:27 GMT""},{""version"":""v2"",""created"":""Wed, 24 Feb 2021 13:09:10 GMT""}]","2021-02-25"
"2011.10479","Rodrigo Mendez Rojano","R. Mendez Rojano, M. Zhussupbekov, J. F. Antaki","Multi-scale simulation of thrombus formation at LVAD inlet cannula
  connection: Importance of Virchow's triad","11 pages, 10 figures, original article",,,,"q-bio.TO","http://creativecommons.org/licenses/by/4.0/","  As pump thrombosis is reduced in current-generation ventricular assist
devices (VAD), adverse events such as bleeding or stroke remain at unacceptable
rates. Thrombosis around the VAD inlet cannula (IC) has been highlighted as a
possible source of stroke events. Recent computational fluid dynamics (CFD)
studies have attempted to characterize the thrombosis risk of different
IC-ventricle configurations. However, purely CFD simulations relate thrombosis
risk to ad-hoc criteria based on flow characteristics, with little
consideration of biochemical factors. This study investigates the genesis of IC
thrombosis including two elements of the Virchow's triad: Endothelial injury
and Hypercoagulability. To this end a multi-scale thrombosis simulation that
includes platelet activity and coagulation reactions was performed. Our results
show significant thrombin formation in stagnation regions (|u|< 0.002 m/s)
close to the IC wall. In addition, high shear-mediated platelet activation was
observed over the leading-edge tip of the cannula which mirrors the thrombus
deposition pattern observed clinically. The current study reveals the
importance of biochemical factors to the genesis of thrombosis at the
ventricular-cannula junction which can inform clinical decisions in terms of
anticoagulation/antiplatelet therapy and guide engineers to develop more robust
designs.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:18:40 GMT""}]","2020-11-23"
"2011.10480","Zhongyang Li","Zhongyang Li and Fei Lu","On the coercivity condition in the learning of interacting particle
  systems",,,,,"stat.ML cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the learning of systems of interacting particles or agents, coercivity
condition ensures identifiability of the interaction functions, providing the
foundation of learning by nonparametric regression. The coercivity condition is
equivalent to the strictly positive definiteness of an integral kernel arising
in the learning. We show that for a class of interaction functions such that
the system is ergodic, the integral kernel is strictly positive definite, and
hence the coercivity condition holds true.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:20:06 GMT""},{""version"":""v2"",""created"":""Thu, 21 Oct 2021 05:51:24 GMT""}]","2021-10-22"
"2011.10481","Ana Carpio","A. Carpio, E. Cebrian","Positivity preserving high order schemes for angiogenesis models",,"International Journal of Nonlinear Sciences and Numerical
  Simulation 2021","10.1515/ijnsns-2021-0112",,"math.NA cs.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Hypoxy induced angiogenesis processes can be described coupling an
integrodifferential kinetic equation of Fokker-Planck type with a diffusion
equation for the angiogenic factor. We propose high order positivity preserving
schemes to approximate the marginal tip density by combining an asymptotic
reduction with weighted essentially non oscillatory and strong stability
preserving time discretization. We show that soliton-like solutions
representing blood vessel formation and spread towards hypoxic regions are
captured.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:20:21 GMT""}]","2021-03-22"
"2011.10483","Pawe{\l} W\'ojcik dr","P. W\'ojcik, A. Bertoni, G. Goldoni","Anisotropy of the spin-orbit coupling driven by a magnetic field in InAs
  nanowires","12 pages, 14 figures","Phys. Rev. B 103, 085434 (2021)","10.1103/PhysRevB.103.085434",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We use the $\mathbf{k} \cdot \mathbf{p}$ theory and the envelope function
approach to evaluate the Rashba spin-orbit coupling induced in a semiconductor
nanowire by a magnetic field at different orientations, taking explicitely into
account the prismatic symmetry of typical nano-crystals. We make the case for
the strongly spin-orbit-coupled InAs semiconductor nanowires and investigate
the anisotropy of the spin-orbit constant with respect to the field direction.
At sufficiently high magnetic fields perpendicular to the nanowire, a 6-fold
anisotropy results from the interplay between the orbital effect of field and
the prismatic symmetry of the nanowire. A back-gate potential, breaking the
native symmetry of the nano-crystal, couples to the magnetic field inducing a
2-fold anisotropy, with the spin-orbit coupling being maximized or minimized
depending on the relative orientation of the two fields. We also investigate
in-wire field configurations, which shows a trivial 2-fold symmetry when the
field is rotated off the axis. However, isotropic spin-orbit coupling is
restored if a sufficiently high gate potential is applied. Our calculations are
shown to agree with recent experimental analysis of the vectorial character of
the spin-orbit coupling for the same nanomaterial, providing a microscopic
interpretation of the latter.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:25:23 GMT""}]","2021-03-03"
"2011.10484","He Zhao","H. Zhao, M. Schultheis, A. Recio-Blanco, G. Kordopatis, P. de Laverny,
  A. Rojas-Arriagada, M. Zoccali, F. Surot, and E. Valenti","The diffuse interstellar band around 8620 {\AA} I. Methods and
  application to the GIBS data set","17 pages, 16 figures","A&A 645, A14 (2021)","10.1051/0004-6361/202039736",,"astro-ph.GA","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We developed a set of procedures to automatically detect and measure the DIB
around 8620 {\AA} (the Gaia DIB) for a wide range of temperatures. The DIB
profile is fit with a Gaussian function. Specifically, the DIB feature is
extracted from the spectra of late-type stars by subtracting the corresponding
synthetic spectra. For early-type stars we applied a specific model based on
the Gaussian process that needs no prior knowledge of the stellar parameters.
The method was tested on $\sim$5000 spectra from the Giraffe Inner Bulge Survey
(GIBS). After validation, we obtained 4194 reasonable fitting results from the
GIBS database. An EW versus $E(J\,{-}\,K_{\rm S})$ relation is derived as
$E(J\,{-}\,K_{\rm S})\,{=}\,1.875\,({\pm}\,0.152)\,{\times}\,{\rm
EW}\,{-}\,0.011\,({\pm}\,0.048)$, according to $E(B\,{-}\,V)/{\rm
EW}\,{=}\,2.721$, which is highly consistent with previous results toward
similar sightlines. After a correction based on the VVV database for both EW
and reddening, the coefficient derived from individual GIBS fields,
$E(J\,{-}\,K_{\rm S})/{\rm EW}\,{=}\,1.884\,{\pm}\,0.225$, is also in perfect
agreement with literature values. Based on a subsample of 1015 stars toward the
Galactic center within $-3^{\circ}\,{<}\,b\,{<}\,3^{\circ}$ and
$-6^{\circ}\,{<}\,l\,{<}\,3^{\circ}$, we determined a rest-frame wavelength of
the Gaia DIB as 8620.55 {\AA}. A Gaussian profile is proved to be a proper and
stable assumption for the Gaia DIB as no intrinsic asymmetry is found.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:27:20 GMT""}]","2021-01-04"
"2011.10485","P\'al Andr\'as Papp","P\'al Andr\'as Papp, Roger Wattenhofer","Sequential Defaulting in Financial Networks",,,,,"cs.CE q-fin.RM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider financial networks, where banks are connected by contracts such
as debts or credit default swaps. We study the clearing problem in these
systems: we want to know which banks end up in a default, and what portion of
their liabilities can these defaulting banks fulfill. We analyze these networks
in a sequential model where banks announce their default one at a time, and the
system evolves in a step-by-step manner.
  We first consider the reversible model of these systems, where banks may
return from a default. We show that the stabilization time in this model can
heavily depend on the ordering of announcements. However, we also show that
there are systems where for any choice of ordering, the process lasts for an
exponential number of steps before an eventual stabilization. We also show that
finding the ordering with the smallest (or largest) number of banks ending up
in default is an NP-hard problem. Furthermore, we prove that defaulting early
can be an advantageous strategy for banks in some cases, and in general,
finding the best time for a default announcement is NP-hard. Finally, we
discuss how changing some properties of this setting affects the stabilization
time of the process, and then use these techniques to devise a monotone model
of the systems, which ensures that every network stabilizes eventually.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:30:44 GMT""}]","2020-11-23"
"2011.10486","\""Ozg\""un \c{C}i\c{c}ek","\""Ozg\""un \c{C}i\c{c}ek, Yassine Marrakchi, Enoch Boasiako Antwi,
  Barbara Di Ventura and Thomas Brox","Recovering the Imperfect: Cell Segmentation in the Presence of
  Dynamically Localized Proteins","Accepted at MICCAI Workshop on Medical Image Learning with Less
  Labels and Imperfect Data, 2020",,"10.1007/978-3-030-61166-8_9",,"cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deploying off-the-shelf segmentation networks on biomedical data has become
common practice, yet if structures of interest in an image sequence are visible
only temporarily, existing frame-by-frame methods fail. In this paper, we
provide a solution to segmentation of imperfect data through time based on
temporal propagation and uncertainty estimation. We integrate uncertainty
estimation into Mask R-CNN network and propagate motion-corrected segmentation
masks from frames with low uncertainty to those frames with high uncertainty to
handle temporary loss of signal for segmentation. We demonstrate the value of
this approach over frame-by-frame segmentation and regular temporal propagation
on data from human embryonic kidney (HEK293T) cells transiently transfected
with a fluorescent protein that moves in and out of the nucleus over time. The
method presented here will empower microscopic experiments aimed at
understanding molecular and cellular function.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:30:55 GMT""}]","2020-11-23"
"2011.10487","Konstantinos Spiliopoulos","Jiahui Yu and Konstantinos Spiliopoulos","Normalization effects on shallow neural networks and related asymptotic
  expansions","Added link to code on GitHub:
  https://github.com/kspiliopoulos/NormalizationEffectsNeuralNetworks","AIMS Journal on Foundations of Data Science, June 2021, Vol. 3,
  Issue 2, pp. 151-200",,,"stat.ML cs.LG math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider shallow (single hidden layer) neural networks and characterize
their performance when trained with stochastic gradient descent as the number
of hidden units $N$ and gradient descent steps grow to infinity. In particular,
we investigate the effect of different scaling schemes, which lead to different
normalizations of the neural network, on the network's statistical output,
closing the gap between the $1/\sqrt{N}$ and the mean-field $1/N$
normalization. We develop an asymptotic expansion for the neural network's
statistical output pointwise with respect to the scaling parameter as the
number of hidden units grows to infinity. Based on this expansion, we
demonstrate mathematically that to leading order in $N$, there is no
bias-variance trade off, in that both bias and variance (both explicitly
characterized) decrease as the number of hidden units increases and time grows.
In addition, we show that to leading order in $N$, the variance of the neural
network's statistical output decays as the implied normalization by the scaling
parameter approaches the mean field normalization. Numerical studies on the
MNIST and CIFAR10 datasets show that test and train accuracy monotonically
improve as the neural network's normalization gets closer to the mean field
normalization.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:33:28 GMT""},{""version"":""v2"",""created"":""Mon, 26 Apr 2021 17:59:19 GMT""},{""version"":""v3"",""created"":""Wed, 1 Jun 2022 16:08:37 GMT""}]","2022-06-02"
"2011.10488","Adam Schroeder","Corey Williams, Adam Schroeder","Utilizing ROS 1 and the Turtlebot3 in a Multi-Robot System","Technical Report of the Robotics and Automation Design Lab",,,,"cs.RO cs.SY eess.SY","http://creativecommons.org/licenses/by/4.0/","  ROS (Robot Operating System) has become ubiquitous for testing new
algorithms, alternative hardware configurations, and prototyping. By performing
research with its modular framework, it can streamline sharing new work and
integrations. However, it has many features and new terms that can take a
considerable amount of time to learn for a new user. This paper will explore
how to set up and configure ROS and ROS packages to work with a multi-robot
system on a single master network.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:33:51 GMT""}]","2020-11-23"
"2011.10489","Chiara Pinto","Chiara Pinto (on behalf of the ALICE Collaboration)","Latest results on light (anti)nuclei production in Pb-Pb collisions with
  ALICE at the LHC","6 pages, 4 figures, Proceedings of 40th International Conference on
  High Energy physics - ICHEP2020, July 28 - August 6, 2020, Prague, Czech
  Republic (virtual meeting)",,,,"nucl-ex hep-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent results on the production of light nuclei, including deuterons,
tritons, $^3$He, $^4$He and the corresponding antinuclei in Pb-Pb collisions at
$\sqrt{s_{\mathrm{NN}}}$ = 5.02 TeV are presented and compared with theoretical
predictions to provide insight into their production mechanisms in heavy-ion
collisions. The large variety of measurements performed with the ALICE
apparatus at different energies and collision systems allows us to constrain
the models of the production mechanisms of light flavour baryon clusters, in
particular those based on the coalescence and statistical hadronisation
approaches. Furthermore, new measurements of the elliptic and triangular flow
of deuteron and $^3$He produced in Pb-Pb collisions at $\sqrt{s_{\mathrm{NN}}}$
= 5.02 TeV are presented and compared to the expectations from coalescence and
hydrodynamic models. The measurement of the elliptic and triangular flow of
light nuclei provides a powerful tool to give insight into their production
mechanism and freeze-out properties at a late stage of the collision evolution.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:34:03 GMT""}]","2020-11-23"
"2011.10490","Alexander Potekhin","A. Y. Potekhin, G. Chabrier","Crust structure and thermal evolution of neutron stars in soft X-ray
  transients","17 pages, 20 figures,accepted for publication in A&A","A&A 645, A102 (2021)","10.1051/0004-6361/202039006",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the effect of physics input on thermal evolution of neutron stars in
soft X-ray transients (SXTs). In particular, we consider different modern
models of the sources of deep crustal heating during accretion episodes and the
effects brought about by impurities embedded in the crust during its formation.
We simulate thermal structure and evolution of episodically accreting neutron
stars under different assumptions on the crust composition and on the
distribution of heat sources and impurities. For the nonaccreted crust, we
consider the nuclear charge fluctuations that arise at crust formation. For the
accreted crust, we compare different theoretical models of composition and
internal heating. We also compare results of numerical simulations with
observations of the crust cooling in SXT MXB 1659-29. We found that the
nonaccreted part of the inner crust of a neutron star can have a layered
structure, with almost pure crystalline layers interchanging with layers
composed of mixtures of different nuclei. The latter layers have relatively low
thermal conductivities, which affects thermal evolution of the transients. The
impurity distribution in the crust strongly depends on the models of the dense
matter and the crust formation scenario. The shallow heating that is needed to
reach agreement between the theory and observations depends on characteristics
of the crust and envelope.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:37:59 GMT""}]","2021-01-27"
"2011.10491","Lorenzo Panebianco","Lorenzo Panebianco","Loop Groups and QNEC",,,"10.1007/s00220-021-04170-3",,"math-ph math.MP","http://creativecommons.org/licenses/by/4.0/","  We investigate some analytical properties of loop group models, showing that
a Positive Energy Representation (PER) of a loop group $LG$ can be extended to
a PER of $H^{3/2}(S^1,G)$ for any compact, simple and simply connected Lie
group $G$. We then explicitly compute the adjoint action of $H^{5/2}(S^1,G)$ on
the stress energy tensor and we use these results to prove the Quantum Null
Energy Condition (QNEC) and the Bekenstein Bound for states obtained by
applying a Sobolev loop to the vacuum. We also give a simpler proof of these
last results in the case $G=SU(n)$. Finally, we construct and study solitonic
representations of the loop group conformal nets induced by the conjugation by
a loop with a discontinuity in $-1$.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:38:07 GMT""},{""version"":""v2"",""created"":""Wed, 2 Dec 2020 17:40:40 GMT""},{""version"":""v3"",""created"":""Mon, 14 Dec 2020 00:25:24 GMT""}]","2021-08-18"
"2011.10492","Thai Le","Thai Le, Noseong Park, Dongwon Lee","A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal
  Trigger's Adversarial Attacks","Accepted to the 59th Annual Meeting of the Association for
  Computational Linguistics (ACL) 2021",,,,"cs.CR cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Universal Trigger (UniTrigger) is a recently-proposed powerful
adversarial textual attack method. Utilizing a learning-based mechanism,
UniTrigger generates a fixed phrase that, when added to any benign inputs, can
drop the prediction accuracy of a textual neural network (NN) model to near
zero on a target class. To defend against this attack that can cause
significant harm, in this paper, we borrow the ""honeypot"" concept from the
cybersecurity community and propose DARCY, a honeypot-based defense framework
against UniTrigger. DARCY greedily searches and injects multiple trapdoors into
an NN model to ""bait and catch"" potential attacks. Through comprehensive
experiments across four public datasets, we show that DARCY detects
UniTrigger's adversarial attacks with up to 99% TPR and less than 2% FPR in
most cases, while maintaining the prediction accuracy (in F1) for clean inputs
within a 1% margin. We also demonstrate that DARCY with multiple trapdoors is
also robust to a diverse set of attack scenarios with attackers' varying levels
of knowledge and skills. Source code will be released upon the acceptance of
this paper.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:38:28 GMT""},{""version"":""v2"",""created"":""Mon, 28 Dec 2020 18:14:53 GMT""},{""version"":""v3"",""created"":""Thu, 6 May 2021 20:53:25 GMT""}]","2021-05-10"
"2011.10493","Sevag Abadian","Sevag Abadian, Giovanni Magno, Vy Yam, B\'eatrice Dagens","Broad-band plasmonic isolator compatible with low-gyrotropy
  magneto-optical material","9 pages, 9 figures",,"10.1364/OE.415969",,"physics.optics","http://creativecommons.org/licenses/by/4.0/","  Optical isolator remains one of the main missing elements for photonic
integrated circuits despite several decades of research. The best solutions up
to now are based on transverse magneto-optical effect using either narrow-band
resonators or high-gyrotropy magneto-optical materials with difficult
integration on usual photonic platforms. We propose in this paper a radically
new concept which enables performing broad-band non-reciprocal transmission
even in the case of low-gyrotropy material. The principle explores the
separation of back and forth light paths, due to the magneto-biplasmonic
effect, i.e., the coupled mode asymmetry induced in plasmonic slot waveguides
loaded with a magneto-optical (MO) layer. We show numerically that such a
metal-MO dielectric-metal slot waveguide combined with suitable side-coupled
lossy rectangular nanocavities gives more than 18 dB isolation ratio on several
tens of nanometers bandwidth, with only 2 dB insertion losses. We propose an
analytical approach describing such a magneto-plasmonic slot waveguide to
identify the involved physical mechanisms and the optimization rules of the
isolator. Additionally, we show that low-gyrotropy material (down to ~0.005)
can be considered for isolation ratio up to 20 dB, opening the road to a new
class of integrated isolators using easy-to-integrate hybrid or composite
materials.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:39:09 GMT""}]","2021-02-24"
"2011.10494","Neil Sheeley Jr.","Neil R. Sheeley Jr","A Mathematical Model For the Spread of a Virus","86 pages, 44 figures",,,,"q-bio.PE","http://creativecommons.org/licenses/by/4.0/","  This paper describes a mathematical model for the spread of a virus through
an isolated population of a given size. The model uses three, color-coded
components, called molecules (red for infected and still contagious; green for
infected, but no longer contagious; and blue for uninfected). In retrospect,
the model turns out to be a digital analogue for the well-known SIR model of
Kermac and McKendrick (1927). In our RGB model, the number of accumulated
infections goes through three phases, beginning at a very low level, then
changing to a transition ramp of rapid growth, and ending in a plateau of final
values. Consequently, the differential change or growth rate begins at 0, rises
to a peak corresponding to the maximum slope of the transition ramp, and then
falls back to 0. The properties of these time variations, including the slope,
duration, and height of the transition ramp, and the width and height of the
infection rate, depend on a single parameter - the time that a red molecule is
contagious divided by the average time between collisions of the molecules.
Various temporal milestones, including the starting time of the transition
ramp, the time that the accumulating number of infections obtains its maximum
slope, and the location of the peak of the infection rate depend on the size of
the population in addition to the contagious lifetime ratio. Explicit formulas
for these quantities are derived and summarized. Finally, Appendix E has been
added to describe the effect of vaccinations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:39:49 GMT""},{""version"":""v2"",""created"":""Fri, 28 May 2021 21:10:31 GMT""}]","2021-06-01"
"2011.10495","Wei Chen","Qi-Nan Wang, Wei Chen, and Hua-Xing Chen","Exotic $\bar{D}_s^{(*)}D^{(*)}$ molecular states and $sc\bar q\bar c$
  tetraquark states with $J^P=0^+, 1^+, 2^+$","19 pages, 3 figures. Accepted by Chinese Physics C","Chin. Phys. C 45(2021), 093102","10.1088/1674-1137/ac0b3b",,"hep-ph hep-ex","http://creativecommons.org/licenses/by/4.0/","  We have calculated the mass spectra for the $\bar{D}_s^{(*)}D^{(*)}$
molecular states and $sc\bar q\bar c$ tetraquark states with $J^P=0^+, 1^+,
2^+$. The masses of the axial-vector $\bar{D}_sD^{*}$, $\bar{D}_s^{*}D$
molecular states and $\mathbf{1}_{[sc]} \oplus \mathbf{0}_{[\bar q \bar{c}]}$,
$\mathbf{0}_{[sc]} \oplus \mathbf{1}_{[\bar q \bar{c}]}$ tetraquark states are
predicted to be around 3.98 GeV, which are in good agreement with the mass of
$Z_{cs}(3985)^-$ from BESIII \cite{besiii2020Zcs}. In both the molecular and
diquark-antidiquark pictures, our results suggest that there may exist two
almost degenerate states, as the strange partners of the $X(3872)$ and
$Z_c(3900)$. We propose to carefully examine the $Z_{cs}(3985)$ in future
experiments to verify this. One may also search for more hidden-charm
four-quark states with strangeness not only in the open-charm
$\bar{D}_s^{(*)}D^{(*)}$ channels, but also in the hidden-charm channels
$\eta_c K/K^\ast$, $J/\psi K/K^\ast$.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:40:32 GMT""},{""version"":""v2"",""created"":""Mon, 30 Nov 2020 15:50:39 GMT""},{""version"":""v3"",""created"":""Sun, 13 Jun 2021 05:10:17 GMT""}]","2021-09-02"
"2011.10496","Hussein Sibai","Hussein Sibai and Sayan Mitra","State Estimation of Open Dynamical Systems with Slow Inputs: Entropy,
  Bit Rates, and relation with Switched Systems",,,,,"eess.SY cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Finding the minimal bit rate needed to estimate the state of a dynamical
system is a fundamental problem. Several notions of topological entropy have
been proposed to solve this problem for closed and switched systems. In this
paper, we extend these notions to open nonlinear dynamical systems with
slowly-varying inputs to lower bound the bit rate needed to estimate their
states. Our entropy definition represents the rate of exponential increase of
the number of functions needed to approximate the trajectories of the system up
to a specified $\eps$ error. We show that alternative entropy definitions using
spanning or separating trajectories bound ours from both sides. On the other
hand, we show that the existing definitions of entropy that consider supremum
over all $\eps$ or require exponential convergence of estimation error, are not
suitable for open systems. Since the actual value of entropy is generally hard
to compute, we derive an upper bound instead and compute it for two examples.
We show that as the bound on the input variation decreases, we recover a
previously known bound on estimation entropy for closed nonlinear systems. For
the sake of computing the bound, we present an algorithm that, given sampled
and quantized measurements from a trajectory and an input signal up to a time
bound $T>0$, constructs a function that approximates the trajectory up to an
$\eps$ error. We show that this algorithm can also be used for state estimation
if the input signal can indeed be sensed. Finally, we relate the computed bound
with a previously known upper bound on the entropy for switched nonlinear
systems. We show that a bound on the divergence between the different modes of
a switched system is needed to get a meaningful bound on its entropy.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:44:58 GMT""}]","2020-11-23"
"2011.10497","Ricardo P\'erez-Marco","Ricardo P\'erez-Marco","Local monodromy formula of Hadamard products","18 pages, 2 figures",,,,"math.CV math.CA math.NT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We find an explicit general formula for the iterated local monodromy of
singularities of the Hadamard product of functions with integrable
singularities. The formula implies the invariance by Hadamard product of the
class of functions with integrable singularities with recurrent monodromies. In
particular, it implies the recurrence of the local monodromy of functions with
finite Hadamard grade as defined by Allouche and Mend\`es-France. We give other
examples of natural classes of functions with recurrent monodromies, functions
with algebro-logarithmic singularities, and more generally with polylogarithm
monodromies. We sketch applications to elliptic integrals, hypergeometric
functions, and to fractional integration.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:52:00 GMT""}]","2020-11-23"
"2011.10498","Artem Kaznatcheev","Artem Kaznatcheev and Prakash Panangaden","Weighted automata are compact and actively learnable","6 pages, 3 figures, to appear in Information Processing Letters",,,,"cs.FL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that weighted automata over the field of two elements can be
exponentially more compact than non-deterministic finite state automata. To
show this, we combine ideas from automata theory and communication complexity.
However, weighted automata are also efficiently learnable in Angluin's minimal
adequate teacher model in a number of queries that is polynomial in the size of
the minimal weighted automaton.. We include an algorithm for learning WAs over
any field based on a linear algebraic generalization of the Angluin-Schapire
algorithm. Together, this produces a surprising result: weighted automata over
fields are structured enough that even though they can be very compact, they
are still efficiently learnable.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:53:18 GMT""},{""version"":""v2"",""created"":""Wed, 17 Mar 2021 05:33:37 GMT""},{""version"":""v3"",""created"":""Thu, 22 Apr 2021 18:27:53 GMT""}]","2021-04-26"
"2011.10499","Haakon Andresen","Haakon Andresen (1), Robert Glas (2,3), H-Thomas Janka (2) ((1) MPI
  Gravitational Physics, Potsdam-Golm, (2) MPI Astrophysics, Garching, (3)
  Excellence Cluster ORIGINS, Garching)","Gravitational-wave Signals From Three-dimensional Supernova Simulations
  With Different Neutrino-Transport Methods","Fixed two issues that came up during the proof. Two sentences was
  added to the beginning of 4.2 to ensure that the figure number agrees with
  the order of which figures are mentioned in the text. Corrected duplicate
  figure reference in the second to last paragraph of the conclusion",,"10.1093/mnras/stab675",,"astro-ph.HE astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We compare gravitational-wave (GW) signals from eight three-dimensional
simulations of core-collapse supernovae, using two different progenitors with
zero-age main sequence masses of 9 and 20 solar masses. The collapse of each
progenitor was simulated four times, at two different grid resolutions and with
two different neutrino transport methods, using the Aenus-Alcar code. The main
goal of this study is to assess the validity of recent concerns that the
so-called ""Ray-by-Ray+"" (RbR+) approximation is problematic in core-collapse
simulations and can adversely affect theoretical GW predictions. Therefore,
signals from simulations using RbR+ are compared to signals from corresponding
simulations using a fully multidimensional (FMD) transport scheme. The 9
solar-mass progenitor successfully explodes, whereas the 20 solar-mass model
does not. Both the standing accretion shock instability and hot-bubble
convection develop in the postshock layer of the non-exploding models. In the
exploding models, neutrino-driven convection in the postshock flow is
established around 100 ms after core bounce and lasts until the onset of shock
revival. We can, therefore, judge the impact of the numerical resolution and
neutrino transport under all conditions typically seen in non-rotating
core-collapse simulations. We find excellent qualitative agreement in all GW
features. We find minor quantitative differences between simulations, but find
no systematic differences between simulations using different transport
schemes. Resolution-dependent differences in the hydrodynamic behaviour of
low-resolution and high-resolution models have a greater impact on the GW
signals than consequences of the different transport methods. Furthermore,
increasing the resolution decreases the discrepancies between models with
different neutrino transport.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:56:04 GMT""},{""version"":""v2"",""created"":""Fri, 5 Mar 2021 13:09:31 GMT""},{""version"":""v3"",""created"":""Mon, 12 Apr 2021 08:18:18 GMT""}]","2021-04-13"
"2011.10501","Pedro Gajardo","Diego Vicencio, Olga Vasilieva, Pedro Gajardo","Monotonicity properties arising in a simple model of Wolbachia invasion
  for wild mosquito populations",,,,,"math.DS q-bio.PE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we propose a simplified bidimensional Wolbachia infestation
model in a population of Aedes aegypti mosquitoes, preserving the main features
associated with the biology of this species that can be found in
higher-dimensional models. Namely, our model represents the maternal
transmission of the Wolbachia symbiont, expresses the reproductive phenotype of
cytoplasmic incompatibility, accounts for different fecundities and mortalities
of infected and wild insects, and exhibits the bistable nature leading to the
so-called principle of competitive exclusion. Since Wolbachia-based biocontrol
is now accepted as an ecologically friendly and potentially cost-effective
method for prevention and control of dengue and other arboviral infections, it
is essential to have reduced models with the main biological characteristics of
Aedes aegypti in the presence of Wolbachia-carriers because such models help to
simplify the mathematical analysis for determining appropriate biocontrol
strategies. Using tools borrowed from monotone dynamical system theory, in the
proposed model, we prove the existence of an invariant threshold manifold that
allows us to provide practical recommendations for performing single and
periodic releases of Wolbachia-carrying mosquitoes, seeking the eventual
elimination of wild insects that are capable of transmitting infections to
humans. We illustrate these findings with numerical simulations using parameter
values corresponding to the wMelPop strain of Wolbachia that is considered the
best virus blocker but induces fitness loss in its carriers.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:59:58 GMT""}]","2020-11-23"
"2011.10502","Domenico Gaglione","Paolo Braca, Domenico Gaglione, Stefano Marano, Leonardo M.
  Millefiori, Peter Willett, Krishna Pattipati","Quickest Detection of COVID-19 Pandemic Onset","Accepted to be published in IEEE Signal Processing Letters",,"10.1109/LSP.2021.3068072",,"eess.SP physics.soc-ph q-bio.PE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper develops an easily-implementable version of Page's CUSUM
quickest-detection test, designed to work in certain composite hypothesis
scenarios with time-varying data statistics. The decision statistic can be cast
in a recursive form and is particularly suited for on-line analysis. By
back-testing our approach on publicly-available COVID-19 data we find reliable
early warning of infection flare-ups, in fact sufficiently early that the tool
may be of use to decision-makers on the timing of restrictive measures that may
in the future need to be taken.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:01:44 GMT""},{""version"":""v2"",""created"":""Thu, 31 Dec 2020 11:35:29 GMT""},{""version"":""v3"",""created"":""Tue, 20 Apr 2021 20:27:31 GMT""}]","2021-04-22"
"2011.10503","Oliver Fabio Piattella","J\'ulio C. Fabris, Oliver F. Piattella, and Davi C. Rodrigues","On Rastall gravity formulation as a $f(R,\mathcal{L}_m)$ and a $f(R,T)$
  theory","11 pages, accepted for publication in the European Physical Journal
  Plus",,,,"gr-qc astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Rastall introduced a stress-energy tensor whose divergence is proportional to
the gradient of the Ricci scalar. This proposal leads to a change in the form
of the field equations of General Relativity, but it preserves the number of
degrees of freedom. Rastall's field equations can be either interpreted as GR
with a redefined SET, or it can imply different physical consequences inside
the matter sector. We investigate limits under which the Rastall field
equations can be directly derived from an action, in particular from two
$f(R)$-gravity extensions: $f(R,\mathcal L_m)$ and $f(R,T)$. We show that there
are similarities between these theories, but the Rastall SET cannot be fully
recovered from them, apart from certain particular cases here discussed. It is
remarkable that a simple, covariant and invertible redefinition of the SET, as
the one proposed by Rastall, is hard to be directly implemented in the action.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:03:48 GMT""},{""version"":""v2"",""created"":""Thu, 24 Nov 2022 09:18:01 GMT""},{""version"":""v3"",""created"":""Sat, 25 Feb 2023 14:43:41 GMT""}]","2023-02-28"
"2011.10505","Leonid Mill","Leonid Mill, David Wolff, Nele Gerrits, Patrick Philipp, Lasse Kling,
  Florian Vollnhals, Andrew Ignatenko, Christian Jaremenko, Yixing Huang,
  Olivier De Castro, Jean-Nicolas Audinot, Inge Nelissen, Tom Wirtz, Andreas
  Maier, Silke Christiansen","Synthetic Image Rendering Solves Annotation Problem in Deep Learning
  Nanoparticle Segmentation",,,,,"cs.LG cond-mat.mtrl-sci cs.CV eess.IV physics.app-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Nanoparticles occur in various environments as a consequence of man-made
processes, which raises concerns about their impact on the environment and
human health. To allow for proper risk assessment, a precise and statistically
relevant analysis of particle characteristics (such as e.g. size, shape and
composition) is required that would greatly benefit from automated image
analysis procedures. While deep learning shows impressive results in object
detection tasks, its applicability is limited by the amount of representative,
experimentally collected and manually annotated training data. Here, we present
an elegant, flexible and versatile method to bypass this costly and tedious
data acquisition process. We show that using a rendering software allows to
generate realistic, synthetic training data to train a state-of-the art deep
neural network. Using this approach, we derive a segmentation accuracy that is
comparable to man-made annotations for toxicologically relevant metal-oxide
nanoparticle ensembles which we chose as examples. Our study paves the way
towards the use of deep learning for automated, high-throughput particle
detection in a variety of imaging techniques such as microscopies and
spectroscopies, for a wide variety of studies and applications, including the
detection of plastic micro- and nanoparticles.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:05:36 GMT""}]","2020-11-23"
"2011.10506","Md. Anwar Hossain","F. Parvin, M. A. Hossain, M. I. Ahmed, K. Akter and A.K.M.A. Islam","Mechanical, optoelectronic and thermoelectric properties of half-Heusler
  p-type semiconductor BaAgP: A DFT investigation","31 pages, 9 figures",,,,"cond-mat.mtrl-sci","http://creativecommons.org/licenses/by/4.0/","  We have explored the mechanical, electronic, optical and thermoelectric
properties of p-type half-Heusler compound BaAgP for the first time using
density functional theory based calculations. The mechanical and dynamical
stability of this compound is confirmed by studying the Born stability criteria
and phonon dispersion curve, respectively. It is soft, ductile and elastically
anisotropic. The atomic bonding along a-axis is stronger than that along
c-axis. The calculated electronic structure reveals that the studied compound
is an indirect band gap semiconductor. The analysis of charge density
distribution map and Mulliken population reveals that the bonding in BaAgP is a
mixture of covalent and ionic. The optical features confirm that BaAgP is
optically anisotropic. The high absorption coefficient and low reflectivity in
the visible to ultraviolet region make this compound a possible candidate for
solar cell and optoelectronic device applications. The thermoelectric
properties have been evaluated by solving the Boltzmann semi-classical
transport equations. The calculated power factor at 1000K along a-axis is 35.2
micro-W/cmK2 (with tau=10-14 s) which is ~3.5 times larger than that of SnSe, a
promising layered thermoelectric materials. The thermoelectric figure of merit,
ZT of BaAgP is 0.44 which is small due to high thermal conductivity. So the
reduction of thermal conductivity is essential to enhance thermoelectric
performance of BaAgP in device applications.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:07:16 GMT""}]","2020-11-23"
"2011.10507","Tasio Gonzalez-Raya","Tasio Gonzalez-Raya, Rodrigo Asensio-Perea, Ana Martin, Lucas C.
  C\'eleri, Mikel Sanz, Pavel Lougovski, and Eugene F. Dumitrescu","Digital-Analog Quantum Simulations Using The Cross-Resonance Effect",,"PRX Quantum 2, 020328 (2021)","10.1103/PRXQuantum.2.020328",,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Digital-analog quantum computation aims to reduce the currently infeasible
resource requirements needed for near-term quantum information processing by
replacing sequences of one- and two-qubit gates with a unitary transformation
generated by the systems' underlying Hamiltonian. Inspired by this paradigm, we
consider superconducting architectures and extend the cross-resonance effect,
up to first order in perturbation theory, from a two-qubit interaction to an
analog Hamiltonian acting on 1D chains and 2D square lattices which, in an
appropriate reference frame, results in a purely two-local Hamiltonian. By
augmenting the analog Hamiltonian dynamics with single-qubit gates we show how
one may generate a larger variety of distinct analog Hamiltonians. We then
synthesize unitary sequences, in which we toggle between the various analog
Hamiltonians as needed, simulating the dynamics of Ising, $XY$, and Heisenberg
spin models. Our dynamics simulations are Trotter error-free for the Ising and
$XY$ models in 1D. We also show that the Trotter errors for 2D $XY$ and 1D
Heisenberg chains are reduced, with respect to a digital decomposition, by a
constant factor. In order to realize these important near-term speedups, we
discuss the practical considerations needed to accurately characterize and
calibrate our analog Hamiltonians for use in quantum simulations. We conclude
with a discussion of how the Hamiltonian toggling techniques could be extended
to derive new analog Hamiltonians which may be of use in more complex
digital-analog quantum simulations for various models of interacting spins.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:07:28 GMT""},{""version"":""v2"",""created"":""Wed, 2 Jun 2021 14:20:40 GMT""}]","2021-06-03"
"2011.10508","Yue Hao","Yue Hao, Weilin Guan, Edwin A Peraza Hernandez, and Jyh-Ming Lien","Planning Folding Motion with Simulation in the Loop Using Laser Forming
  Origami and Thermal Behaviors as an Example",,,,,"cs.RO cs.CG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Designing a robot or structure that can fold itself into a target shape is a
process that involves challenges originated from multiple sources. For example,
the designer of rigid self-folding robots must consider foldability from
geometric and kinematic aspects to avoid self-intersection and undesired
deformations. Recent works have shown success in estimating foldability of a
design using robot motion planners. However, many foldable structures are
actuated using physically coupled reactions (i.e., folding originated from
thermal, chemical, or electromagnetic loads). Therefore, a reliable foldability
analysis must consider additional constraints that resulted from these critical
phenomena. This work investigates the idea of efficiently incorporating
computationally expensive physics simulation within the folding motion planner
to provide a better estimation of the foldability. In this paper, we will use
laser forming origami as an example to demonstrate the benefits of considering
the properties beyond geometry. We show that the design produced by the
proposed method can be folded more efficiently.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:07:49 GMT""}]","2020-11-23"
"2011.10509","Tobias Gabel Christiansen Mr.","Melvyn Weeks and Tobias Gabel Christiansen","Understanding the Distributional Aspects of Microcredit Expansions","39 pages",,,,"econ.GN q-fin.EC","http://creativecommons.org/licenses/by/4.0/","  Various poverty reduction strategies are being implemented in the pursuit of
eliminating extreme poverty. One such strategy is increased access to
microcredit in poor areas around the world. Microcredit, typically defined as
the supply of small loans to underserved entrepreneurs that originally aimed at
displacing expensive local money-lenders, has been both praised and criticized
as a development tool (Banerjee et al., 2015b). This paper presents an analysis
of heterogeneous impacts from increased access to microcredit using data from
three randomised trials. In the spirit of recognising that in general the
impact of a policy intervention varies conditional on an unknown set of
factors, particular, we investigate whether heterogeneity presents itself as
groups of winners and losers, and whether such subgroups share characteristics
across RCTs. We find no evidence of impacts, neither average nor
distributional, from increased access to microcredit on consumption levels. In
contrast, the lack of average effects on profits seems to mask heterogeneous
impacts. The findings are, however, not robust to the specific machine learning
algorithm applied. Switching from the better performing Elastic Net to the
worse performing Random Forest leads to a sharp increase in the variance of the
estimates. In this context, methods to evaluate the relative performing machine
learning algorithm developed by Chernozhukov et al. (2019) provide a
disciplined way for the analyst to counter the uncertainty as to which
algorithm to deploy.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:08:39 GMT""}]","2020-11-23"
"2011.10510","M Quamer Nasim","M Quamer Nasim, Tannistha Maiti, Ayush Srivastava, Tarry Singh, and
  Jie Mei","Seismic Facies Analysis: A Deep Domain Adaptation Approach","22 pages, 13 figures, 5 tables, and supplementary material included
  in the end of the paper",,,,"physics.geo-ph cs.AI cs.CV cs.LG eess.IV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Deep neural networks (DNNs) can learn accurately from large quantities of
labeled input data, but often fail to do so when labelled data are scarce. DNNs
sometimes fail to generalize ontest data sampled from different input
distributions. Unsupervised Deep Domain Adaptation (DDA)techniques have been
proven useful when no labels are available, and when distribution shifts are
observed in the target domain (TD). In the present study, experiments are
performed on seismic images of the F3 block 3D dataset from offshore
Netherlands (source domain; SD) and Penobscot 3D survey data from Canada
(target domain; TD). Three geological classes from SD and TD that have similar
reflection patterns are considered. A deep neural network architecture named
EarthAdaptNet (EAN) is proposed to semantically segment the seismic images when
few classes have data scarcity, and we use a transposed residual unit to
replace the traditional dilated convolution in the decoder block. The EAN
achieved a pixel-level accuracy >84% and an accuracy of ~70% for the minority
classes, showing improved performance compared to existing architectures. In
addition, we introduce the CORAL (Correlation Alignment) method to the EAN to
create an unsupervised deep domain adaptation network (EAN-DDA) for the
classification of seismic reflections from F3 and Penobscot, to demonstrate
possible approaches when labelled data are unavailable. Maximum class accuracy
achieved was ~99% for class 2 of Penobscot, with an overall accuracy>50%. Taken
together, the EAN-DDA has the potential to classify target domain seismic
facies classes with high accuracy.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:09:06 GMT""},{""version"":""v2"",""created"":""Fri, 25 Jun 2021 18:27:06 GMT""},{""version"":""v3"",""created"":""Wed, 27 Oct 2021 04:25:17 GMT""}]","2021-10-28"
"2011.10511","Matthew Shelley","Matthew Shelley, Alessandro Pastore","Comparison between the Thomas-Fermi and Hartree-Fock-Bogoliubov Methods
  in the Inner Crust of a Neutron Star: The Role of Pairing Correlations","17 pages, 8 figures, published in Universe","Universe 6 (2020) 206","10.3390/universe6110206",,"nucl-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigated the role of a pairing correlation in the chemical composition
of the inner crust of a neutron star with the extended Thomas-Fermi method,
using the Strutinsky integral correction. We compare our results with the fully
self-consistent Hartree-Fock-Bogoliubov approach, showing that the resulting
discrepancy, apart from the very low density region, is compatible with the
typical accuracy we can achieve with standard mean-field methods.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:10:12 GMT""}]","2020-11-23"
"2011.10512","David Forsyth","D. A. Forsyth and Jason J. Rock","Intrinsic Image Decomposition using Paradigms",,,,,"cs.CV","http://creativecommons.org/licenses/by/4.0/","  Intrinsic image decomposition is the classical task of mapping image to
albedo. The WHDR dataset allows methods to be evaluated by comparing
predictions to human judgements (""lighter"", ""same as"", ""darker""). The best
modern intrinsic image methods learn a map from image to albedo using rendered
models and human judgements. This is convenient for practical methods, but
cannot explain how a visual agent without geometric, surface and illumination
models and a renderer could learn to recover intrinsic images.
  This paper describes a method that learns intrinsic image decomposition
without seeing WHDR annotations, rendered data, or ground truth data. The
method relies on paradigms - fake albedos and fake shading fields - together
with a novel smoothing procedure that ensures good behavior at short scales on
real images. Long scale error is controlled by averaging. Our method achieves
WHDR scores competitive with those of strong recent methods allowed to see
training WHDR annotations, rendered data, and ground truth data. Because our
method is unsupervised, we can compute estimates of the test/train variance of
WHDR scores; these are quite large, and it is unsafe to rely small differences
in reported WHDR.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:10:12 GMT""}]","2020-11-23"
"2011.10513","Gabriel Landi Dr.","Karen V. Hovhannisyan, Mathias R. J{\o}rgensen, Gabriel T. Landi,
  \'Alvaro M. Alhambra, Jonatan B. Brask and Mart\'i Perarnau-Llobet","Optimal Quantum Thermometry with Coarse-grained Measurements",,"PRX Quantum 2, 020322 (2021)","10.1103/PRXQuantum.2.020322",,"quant-ph cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Precise thermometry for quantum systems is important to the development of
new technology, and understanding the ultimate limits to precision presents a
fundamental challenge. It is well known that optimal thermometry requires
projective measurements of the total energy of the sample. However, this is
infeasible in even moderately-sized systems, where realistic energy
measurements will necessarily involve some coarse graining. Here, we explore
the precision limits for temperature estimation when only coarse-grained
measurements are available. Utilizing tools from signal processing, we derive
the structure of optimal coarse-grained measurements and find that good
temperature estimates can generally be attained even with a small number of
outcomes. We apply our results to many-body systems and nonequilibrium
thermometry. For the former, we focus on interacting spin lattices, both at and
away from criticality, and find that the Fisher-information scaling with system
size is unchanged after coarse-graining. For the latter, we consider a probe of
given dimension interacting with the sample, followed by a measurement of the
probe. We derive an upper bound on arbitrary, nonequilibrium strategies for
such probe-based thermometry and illustrate it for thermometry on a
Bose-Einstein condensate using an atomic quantum-dot probe.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:12:55 GMT""},{""version"":""v2"",""created"":""Thu, 20 May 2021 13:10:07 GMT""}]","2021-05-26"
"2011.10515","Rahav Gowtham Venkateswaran","Rahav Gowtham Venkateswaran, Ursula Kowalsky and Dieter Dinkler","A modified bond model for describing isotropic linear elastic material
  behaviour with the particle method",,,"10.1007/s40571-021-00422-0",,"cs.CE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Particle based methods such as the Discrete Element Method and the Lattice
Spring Method may be used for describing the behaviour of isotropic linear
elastic materials. However, the common bond models employed to describe the
interaction between particles restrict the range of Poisson's ratio that can be
represented. In this paper, to overcome the restriction, a modified bond model
that includes the coupling of shear strain energy of neighbouring bonds is
proposed. The coupling is described by a multi-bond term that enables the model
to distinguish between shear deformations and rigid-body rotations. The
positive definiteness of the strain energy function of the modified bond model
is verified. To validate the model, uniaxial tension, pure shear, pure bending
and cantilever bending tests are performed. Comparison of the particle
displacements with continuum mechanics solution demonstrates the ability of the
model to describe the behaviour of isotropic linear elastic material for values
of Poisson's ratio in the range $0 \leq \nu < 0.5$.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:19:15 GMT""}]","2021-07-06"
"2011.10516","Theresa Lange","Theresa Lange, Wilhelm Stannat","Mean field limit of Ensemble Square Root Filters -- discrete and
  continuous time","Accepted for publication in Foundations of Data Science. Revised
  version: corrected typos, added more explanations, updated Theorem 3.1 (holds
  for all p >=2)",,"10.3934/fods.2021003",,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Consider the class of Ensemble Square Root filtering algorithms for the
numerical approximation of the posterior distribution of nonlinear Markovian
signals partially observed with linear observations corrupted with independent
measurement noise. We analyze the asymptotic behavior of these algorithms in
the large ensemble limit both in discrete and continuous time. We identify
limiting mean-field processes on the level of the ensemble members, prove
corresponding propagation of chaos results and derive associated convergence
rates in terms of the ensemble size. In continuous time we also identify the
stochastic partial differential equation driving the distribution of the
mean-field process and perform a comparison with the Kushner-Stratonovich
equation.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:20:11 GMT""},{""version"":""v2"",""created"":""Wed, 6 Jan 2021 09:25:03 GMT""}]","2021-02-01"
"2011.10517","Emma Shen","Emma Shen, Dominic Anstey, Eloy de Lera Acedo, Anastasia Fialkov, Will
  Handley","Quantifying Ionospheric Effects on Global 21-cm Observations","11 pages, 13 figures, 4 tables",,"10.1093/mnras/stab429",,"astro-ph.IM astro-ph.CO astro-ph.EP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We modelled the two major layer of Earth's ionosphere, the F-layer and the
D-layer, by a simplified spatial model with temporal variance to study the
chromatic ionospheric effects on global 21-cm observations. From the analyses,
we found that the magnitude of the ionospheric disruptions due to ionospheric
refraction and absorption can be greater than the expected global 21-cm signal,
and the variation of its magnitude can differ, depending on the ionospheric
conditions. Within the parameter space adopted in the model, the shape of the
global 21-cm signal is distorted after propagating through the ionosphere,
while its amplitude is weakened. It is observed that the ionospheric effects do
not cancel out over time, and thus should be accounted for in the foreground
calibration at each timestep to account for the chromaticity introduced by the
ionosphere.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:23:09 GMT""}]","2021-03-03"
"2011.10518","Amanuel Alambo","Amanuel Alambo, Swati Padhee, Tanvi Banerjee, and Krishnaprasad
  Thirunarayan","COVID-19 and Mental Health/Substance Use Disorders on Reddit: A
  Longitudinal Study","First workshop on computational & affective intelligence in
  healthcare applications in conjunction with ICPR 2021",,,,"cs.CY cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  COVID-19 pandemic has adversely and disproportionately impacted people
suffering from mental health issues and substance use problems. This has been
exacerbated by social isolation during the pandemic and the social stigma
associated with mental health and substance use disorders, making people
reluctant to share their struggles and seek help. Due to the anonymity and
privacy they provide, social media emerged as a convenient medium for people to
share their experiences about their day to day struggles. Reddit is a
well-recognized social media platform that provides focused and structured
forums called subreddits, that users subscribe to and discuss their experiences
with others. Temporal assessment of the topical correlation between social
media postings about mental health/substance use and postings about Coronavirus
is crucial to better understand public sentiment on the pandemic and its
evolving impact, especially related to vulnerable populations. In this study,
we conduct a longitudinal topical analysis of postings between subreddits
r/depression, r/Anxiety, r/SuicideWatch, and r/Coronavirus, and postings
between subreddits r/opiates, r/OpiatesRecovery, r/addiction, and r/Coronavirus
from January 2020 - October 2020. Our results show a high topical correlation
between postings in r/depression and r/Coronavirus in September 2020. Further,
the topical correlation between postings on substance use disorders and
Coronavirus fluctuates, showing the highest correlation in August 2020. By
monitoring these trends from platforms such as Reddit, epidemiologists, and
mental health professionals can gain insights into the challenges faced by
communities for targeted interventions.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:23:49 GMT""}]","2020-11-23"
"2011.10519","Jan Nagel","Tabea Glatzel and Jan Nagel","The speed of random walk on Galton-Watson trees with vanishing
  conductances",,,,,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we consider random walks on Galton-Watson trees with random
conductances. On these trees, the distance of the walker to the root satisfies
a law of large numbers with limit the effective velocity, or speed of the walk.
We study the regularity of the speed as a function of the distribution of
conductances, in particular when the distribution of conductances converges to
a non-elliptic limit.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:24:13 GMT""}]","2020-11-23"
"2011.10520","Hugo Tessier","Hugo Tessier, Vincent Gripon, Mathieu L\'eonardon, Matthieu Arzel,
  Thomas Hannagan, David Bertrand","Rethinking Weight Decay For Efficient Neural Network Pruning","23 pages, 18 figures, published at Journal of Imaging, update : added
  new results, rewrite","Journal of Imaging 8 (2022), no. 3: 64","10.3390/jimaging8030064",,"cs.NE","http://creativecommons.org/licenses/by/4.0/","  Introduced in the late 1980s for generalization purposes, pruning has now
become a staple for compressing deep neural networks. Despite many innovations
in recent decades, pruning approaches still face core issues that hinder their
performance or scalability. Drawing inspiration from early work in the field,
and especially the use of weight decay to achieve sparsity, we introduce
Selective Weight Decay (SWD), which carries out efficient, continuous pruning
throughout training. Our approach, theoretically grounded on Lagrangian
smoothing, is versatile and can be applied to multiple tasks, networks, and
pruning structures. We show that SWD compares favorably to state-of-the-art
approaches, in terms of performance-to-parameters ratio, on the CIFAR-10, Cora,
and ImageNet ILSVRC2012 datasets.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:25:53 GMT""},{""version"":""v2"",""created"":""Tue, 22 Dec 2020 15:30:19 GMT""},{""version"":""v3"",""created"":""Tue, 16 Feb 2021 08:28:53 GMT""},{""version"":""v4"",""created"":""Wed, 9 Mar 2022 15:06:05 GMT""}]","2022-03-10"
"2011.10521","Weina Wang","Weina Wang, Qiaomin Xie and Mor Harchol-Balter","Zero Queueing for Multi-Server Jobs",,,,,"cs.PF math.PR","http://creativecommons.org/licenses/by/4.0/","  Cloud computing today is dominated by multi-server jobs. These are jobs that
request multiple servers simultaneously and hold onto all of these servers for
the duration of the job. Multi-server jobs add a lot of complexity to the
traditional one-job-per-server model: an arrival might not ""fit"" into the
available servers and might have to queue, blocking later arrivals and leaving
servers idle. From a queueing perspective, almost nothing is understood about
multi-server job queueing systems; even understanding the exact stability
region is a very hard problem.
  In this paper, we investigate a multi-server job queueing model under scaling
regimes where the number of servers in the system grows. Specifically, we
consider a system with multiple classes of jobs, where jobs from different
classes can request different numbers of servers and have different service
time distributions, and jobs are served in first-come-first-served order. The
multi-server job model opens up new scaling regimes where both the number of
servers that a job needs and the system load scale with the total number of
servers. Within these scaling regimes, we derive the first results on
stability, queueing probability, and the transient analysis of the number of
jobs in the system for each class. In particular we derive sufficient
conditions for zero queueing. Our analysis introduces a novel way of extracting
information from the Lyapunov drift, which can be applicable to a broader scope
of problems in queueing systems.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:28:38 GMT""},{""version"":""v2"",""created"":""Thu, 4 Feb 2021 05:26:25 GMT""}]","2021-02-05"
"2011.10522","James Dawber","James Dawber, Nicola Salvati, Timo Schmid and Nikos Tzavidis","Scale estimation and data-driven tuning constant selection for
  M-quantile regression","30 pages, 6 figures",,,,"stat.ME","http://creativecommons.org/licenses/by/4.0/","  M-quantile regression is a general form of quantile-like regression which
usually utilises the Huber influence function and corresponding tuning
constant. Estimation requires a nuisance scale parameter to ensure the
M-quantile estimates are scale invariant, with several scale estimators having
previously been proposed. In this paper we assess these scale estimators and
evaluate their suitability, as well as proposing a new scale estimator based on
the method of moments. Further, we present two approaches for estimating
data-driven tuning constant selection for M-quantile regression. The tuning
constants are obtained by i) minimising the estimated asymptotic variance of
the regression parameters and ii) utilising an inverse M-quantile function to
reduce the effect of outlying observations. We investigate whether data-driven
tuning constants, as opposed to the usual fixed constant, for instance, at
c=1.345, can improve the efficiency of the estimators of M-quantile regression
parameters. The performance of the data-driven tuning constant is investigated
in different scenarios using model-based simulations. Finally, we illustrate
the proposed methods using a European Union Statistics on Income and Living
Conditions data set.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:28:45 GMT""}]","2020-11-23"
"2011.10524","Gaojie Chen","Chong Huang, Gaojie Chen and Yu Gong","Delay Constrained Buffer-Aided Relay Selection in the Internet of Things
  with Decision-Assisted Reinforcement Learning",,,,,"eess.SP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper investigates the reinforcement learning for the relay selection in
the delay-constrained buffer-aided networks. The buffer-aided relay selection
significantly improves the outage performance but often at the price of higher
latency. On the other hand, modern communication systems such as the Internet
of Things often have strict requirement on the latency. It is thus necessary to
find relay selection policies to achieve good throughput performance in the
buffer-aided relay network while stratifying the delay constraint. With the
buffers employed at the relays and delay constraints imposed on the data
transmission, obtaining the best relay selection becomes a complicated
high-dimensional problem, making it hard for the reinforcement learning to
converge. In this paper, we propose the novel decision-assisted deep
reinforcement learning to improve the convergence. This is achieved by
exploring the a-priori information from the buffer-aided relay system. The
proposed approaches can achieve high throughput subject to delay constraints.
Extensive simulation results are provided to verify the proposed algorithms.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:45:39 GMT""}]","2020-11-23"
"2011.10525","Richa Kundu","Richa Kundu, Camila Navarrete, Jos\'e G. Fern\'andez-Trincado, Dante
  Minniti, Harinder P. Singh, Luca Sbordone, Andr\'es E. Piatti, C\'eline
  Reyl\'e","The search for extratidal star candidates around Galactic globular
  clusters NGC 2808, NGC 6266, and NGC 6397 with {\textit{Gaia}} DR2 astrometry","Accepted for publication in Astronomy and Astrophysics","A&A 645, A116 (2021)","10.1051/0004-6361/202038720",,"astro-ph.GA astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Context. Extratidal stars are stellar bodies that end up outside the tidal
radius of a cluster as a result of internal processes or external forces acting
upon it. The presence and spatial distribution of these stars can give us
insights into the past evolution of a cluster inside our Galaxy. Aims. Previous
works suggest that globular clusters, when explored in detail, show evidence of
extratidal stars. We aim to search for possible extratidal stars in the
Galactic globular clusters NGC 6397, NGC 2808, and NGC 6266 using the
photometry and proper motion measurements from Gaia DR2 database (Gaia
Collaboration et al. 2018). Results. Finally, 120, 126, and 107 extratidal
candidate stars were found lying outside the tidal radius of the globular
clusters NGC 6397, NGC 2808, and NGC 6266, respectively. 70%, 25.4%, and 72.9%
of the extratidal stars found are located outside the Jacobi radius of NGC
6397, NGC 2808, and NGC 6266, respectively. The spatial distribution of the
extratidal stars belonging to NGC 6397 appears S-like, extending along the
curved leading and trailing arms. NGC 2808 has an overdensity of stars in the
trailing part of the cluster and NGC 6266 seems to have overdensities of
extratidal stars in its eastern and northern sides. Conclusions. Proper motions
and color-magnitude diagrams can be used to identify extratidal candidate stars
around GCs. Nonetheless, depending on how different the kinematics and stellar
populations of a cluster are compared to the Milky Way field, the fraction of
contamination can be larger. All three clusters are found to have extratidal
stars outside their tidal radii. For NGC 6397 and NGC 2808, these stars may be
the result of a combined effect of the disc shocks and tidal disruptions. For
NGC 6266, the distribution of extratidal stars is symmetrical around it, most
likely indicating that the cluster has an extended stellar envelope.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:50:30 GMT""},{""version"":""v2"",""created"":""Tue, 24 Nov 2020 17:18:32 GMT""}]","2021-01-27"
"2011.10526","Hugh R. A. Jones","H.R.A. Jones, W. E. Martin, G. Anglada-Escud\'e, R. Errmann, D. A.
  Campbell, C. Baker, C. Boonsri, P. Choochalerm","A small actively-controlled high-resolution spectrograph based on
  off-the-shelf components","21 pages, 9 figures, 2 tables, accepted PASP",,"10.1088/1538-3873/abc7ee",,"astro-ph.IM","http://creativecommons.org/licenses/by/4.0/","  We present the design and testing of a prototype in-plane echelle
spectrograph based on an actively controlled fibre-fed double-pass design. This
system aims to be small and efficient with the minimum number of optical
surfaces - currently a collimator/camera lens, cross-dispersing prism, grating
and a reflector to send light to the detector. It is built from catalogue
optical components and has dimensions of approximately 20x30 cm. It works in
the optical regime with a resolution of >70,000. The spectrograph is fed by a
bifurcated fibre with one fibre to a telescope and the other used to provide
simultaneous Thorium Argon light illumination for wavelength calibration. The
positions of the arc lines on the detector are processed in real time and
commercial auto-guiding software is used to treat the positions of the arc
lines as guide stars. The guiding software sends any required adjustments to
mechanical piezo-electric actuators which move the mirror sending light to the
camera removing any drift in the position of the arc lines. The current
configuration using an sCMOS detector provides a precision of 3.5 milli-pixels
equivalent to 4 m/s in a standard laboratory environment.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:54:31 GMT""}]","2021-03-17"
"2011.10527","Taejin Park","Tae Jin Park, Manoj Kumar and Shrikanth Narayanan","Multi-Scale Speaker Diarization With Neural Affinity Score Fusion","Submitted to ICASSP 2021",,,,"eess.AS","http://creativecommons.org/licenses/by/4.0/","  Identifying the identity of the speaker of short segments in human dialogue
has been considered one of the most challenging problems in speech signal
processing. Speaker representations of short speech segments tend to be
unreliable, resulting in poor fidelity of speaker representations in tasks
requiring speaker recognition. In this paper, we propose an unconventional
method that tackles the trade-off between temporal resolution and the quality
of the speaker representations. To find a set of weights that balance the
scores from multiple temporal scales of segments, a neural affinity score
fusion model is presented. Using the CALLHOME dataset, we show that our
proposed multi-scale segmentation and integration approach can achieve a
state-of-the-art diarization performance.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:57:12 GMT""}]","2020-11-23"
"2011.10528","Paritosh Verma","Paritosh Verma","Space Lower Bounds for Graph Stream Problems","Published in the conference on. Theory and Applications of Models of
  Computation (TAMC) 2019 pp 635-646","Springer International Publishing. Theory and Applications of
  Models of Computation. (2019) 635--646","10.1007/978-3-030-14812-6_39",,"cs.CC","http://creativecommons.org/licenses/by/4.0/","  This work concerns with proving space lower bounds for graph problems in the
streaming model. It is known that computing the length of shortest path between
two nodes in the streaming model requires $\Omega(n)$ space, where $n$ is the
number of nodes in the graph. We study the problem of finding the depth of a
given node in a rooted tree in the streaming model. For this problem we prove a
tight single pass space lower bound and a multipass space lower bound. As this
is a special case of computing shortest paths on graphs, the above lower bounds
also apply to the shortest path problem in the streaming model. The results are
obtained by using known communication complexity lower bounds or by
constructing hard instances for the problem. Additionally, we apply the
techniques used in proving the above lower bound results to prove space lower
bounds (single and multipass) for other graph problems like finding min $s-t$
cut, detecting negative weight cycle and finding whether two nodes lie in the
same strongly connected component.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:57:42 GMT""}]","2020-11-23"
"2011.10529","Ali Abdi","Iman Habibi, Effat S Emamian, Osvaldo Simeone and Ali Abdi","Computation capacities of a broad class of signaling networks are higher
  than their communication capacities","51 pages, 8 figures","Phys. Biol. 16 064001 (2019)","10.1088/1478-3975/ab4345",,"q-bio.MN cs.IT math.IT q-bio.SC stat.CO","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Due to structural and functional abnormalities or genetic variations and
mutations, there may be dysfunctional molecules within an intracellular
signaling network that do not allow the network to correctly regulate its
output molecules, such as transcription factors. This disruption in signaling
interrupts normal cellular functions and may eventually develop some
pathological conditions. In this paper, computation capacity of signaling
networks is introduced as a fundamental limit on signaling capability and
performance of such networks. The computation capacity measures the maximum
number of computable inputs, that is, the maximum number of input values for
which the correct functional output values can be recovered from the erroneous
network outputs, when the network contains some dysfunctional molecules. This
contrasts with the conventional communication capacity that measures instead
the maximum number of input values that can be correctly distinguished based on
the erroneous network outputs.
  The computation capacity is higher than the communication capacity, if the
network response function is not a one-to-one function of the input signals. By
explicitly incorporating the effect of signaling errors that result in the
network dysfunction, the computation capacity provides more information about
the network and its malfunction. Two examples of signaling networks are studied
here, one regulating caspase3 and another regulating NFkB, for which
computation and communication capacities are analyzed. Higher computation
capacities are observed for both networks. One biological implication of this
finding is that signaling networks may have more capacity than that specified
by the conventional communication capacity metric. The effect of feedback is
also studied. In summary, this paper reports findings on a new fundamental
feature of the signaling capability of cell signaling networks.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:59:02 GMT""}]","2020-11-23"
"2011.10530","Alexey Uvarov","Alexey Uvarov and Jacob Biamonte","On barren plateaus and cost function locality in variational quantum
  algorithms","26 pages, RevTeX","Journal of Physics A: Mathematical and Theoretical 54 (24), 245301
  (2021)","10.1088/1751-8121/abfac7",,"quant-ph cond-mat.dis-nn cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Variational quantum algorithms rely on gradient based optimization to
iteratively minimize a cost function evaluated by measuring output(s) of a
quantum processor. A barren plateau is the phenomenon of exponentially
vanishing gradients in sufficiently expressive parametrized quantum circuits.
It has been established that the onset of a barren plateau regime depends on
the cost function, although the particular behavior has been demonstrated only
for certain classes of cost functions. Here we derive a lower bound on the
variance of the gradient, which depends mainly on the width of the circuit
causal cone of each term in the Pauli decomposition of the cost function. Our
result further clarifies the conditions under which barren plateaus can occur.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:00:01 GMT""},{""version"":""v2"",""created"":""Mon, 25 Oct 2021 15:13:52 GMT""}]","2021-10-26"
"2011.10531","Thomas Liz\'ee","T. Liz\'ee, B. Vollmer, J. Braine, F. Nehlig","Gas compression and stellar feedback in the tidally interacting and
  ram-pressure stripped Virgo spiral galaxy NGC 4654","27 pages, 48 figures, 4 tables, accepted for publication in A&A","A&A 645, A111 (2021)","10.1051/0004-6361/202038910",,"astro-ph.GA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  NGC 4654 is a Virgo galaxy seen almost face-on, which undergoes nearly
edge-on gas ram pressure stripping and a fly-by gravitational interaction with
another massive galaxy, NGC 4639. NGC 4654 shows a strongly compressed gas
region near the outer edge of the optical disk, with HI surface densities
(HSDR), exceeding the canonical value of 10-15 Msun/pc2. New IRAM 30m HERA
CO(2-1) data of NGC 4654 are used to study the physical conditions of the ISM.
The CO-to-H$_2$ conversion factor was estimated and found to be one to two
times the Galactic value with significant decrease in the ratio between the
molecular fraction and the total ISM pressure in the HSDR, self-gravitating
gas, a Toomre parameter below $Q=1$ and star-formation efficiency 1.5-2 times
higher. Analytical models were used to reproduce radial profiles of the SFR and
the atomic and molecular surface densities. A Toomre parameter of $\rm Q \sim
0.8$ combined with an increase in the velocity dispersion of 5 km/s are
necessary conditions to simultaneously reproduce the gas surface densities and
the SFR. A dynamical model was used to reproduce the gas distribution of NGC
4654. The comparison between the velocity dispersion given by the moment 2 map
and the intrinsic 3D velocity dispersion from the model were used to
discriminate between regions of broader linewidths caused by a real increase in
the velocity dispersion and those caused by an unresolved velocity gradient
only. We found that the 5 km/s increase in the intrinsic velocity dispersion is
compatible with observations. During a period of gas compression through
external interactions, the gas surface density is enhanced, leading to an
increased SFR and stellar feedback. Under the influence of stellar feedback,
the gas density increases only moderately. The stellar feedback acts as a
regulator of star-formation, increasing the turbulent velocity within the
region.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:00:42 GMT""}]","2021-01-27"
"2011.10532","Mariangela Bondi'","M.Battaglieri, P. Bisio, M. Bond\'i, A. Celentano, P.L. Cole, M. De
  Napoli, R. De Vita, L. Marsicano, G. Ottonello, F. Parodi, N. Randazzo, E.S.
  Smith, D. Snowden-Ifft, M. Spreafico, T. Whitlatch, M.H. Wood","The BDX-MINI detector for Light Dark Matter search at JLab",,,"10.1140/epjc/s10052-021-08957-5",,"physics.ins-det hep-ex nucl-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper describes the design and performance of a compact detector,
BDX-MINI, that incorporates all features of a concept that optimized the
detection of light dark matter produced by electrons in a beam dump. It
represents a reduced version of the future BDX experiment expected to run at
JLAB. BDX-MINI was exposed to penetrating particles produced by a 2.176 GeV
electron beam incident on the beam dump of Hall A at Jefferson Lab. The
detector consists of 30.5 kg of PbWO4 crystals with sufficient material
following the beam dump to eliminate all known particles except neutrinos. The
crystals are read out using silicon photomultipliers. Completely surrounding
the detector are a passive layer of tungsten and two active scintillator veto
systems, which are also read out using silicon photomultipliers. The design was
validated and the performance of the robust detector was shown to be stable
during a six month period during which the detector was operated with minimal
access.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:03:52 GMT""}]","2021-03-17"
"2011.10533","Ilaria Musella","Ilaria Musella, Marcella Marconi, Roberto Molinaro, Giuliana
  Fiorentino, Vincenzo Ripepi, Giulia De Somma and Maria Ida Moretti","New insights into the use of Ultra Long Period Cepheids as cosmological
  standard candles","MNRAS - Accepted 2020 November 19. Received 2020 November 19; in
  original form 2020 July 15 - 9 pages and 8 figures",,"10.1093/mnras/staa3678",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Ultra Long Period Cepheids (ULPs) are pulsating variable stars with a period
longer than 80d and have been hypothesized to be the extension of the Classical
Cepheids (CCs) at higher masses and luminosities. If confirmed as standard
candles, their intrinsic luminosities, 1 to 3 mag brighter than typical CCs,
would allow to reach the Hubble flow and, in turn, to determine the Hubble
constant, H_0, in one step, avoiding the uncertainties associated with the
calibration of primary and secondary indicators. To investigate the accuracy of
ULPs as cosmological standard candles, we first collect all the ULPs known in
the literature. The resulting sample includes 63 objects with a very large
metallicity spread with 12 + log([O/H]) ranging from 7.2 to 9.2 dex. The
analysis of their properties in the VI period-Wesenheit plane and in the
color-magnitude diagram (CMD) supports the hypothesis that the ULPs are the
extension of CCs at longer periods, higher masses and luminosities, even if,
additional accurate and homogeneous data and a devoted theoretical scenario are
needed to get firm conclusions. Finally, the three M31 ULPs, 8-0326, 8-1498 and
H42, are investigated in more detail. For 8-1498 and H42, we cannot confirm
their nature as ULPs, due to the inconsistency between their position in the
CMD and the measured periods. For 8-0326, the light curve model fitting
technique applied to the available time-series data allows us to constrain its
intrinsic stellar parameters, distance and reddening.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:05:29 GMT""}]","2020-12-02"
"2011.10534","Olivier Carton","Olivier Carton","Ambiguity through the lens of measure theory",,,,,"cs.FL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we establish a strong link between the ambiguity for finite
words of a B\""uchi automaton and the ambiguity for infinite words of the same
automaton. This link is based on measure theory. More precisely, we show that
such an automaton is unambiguous, in the sense that no finite word labels two
runs with the same starting state and the same ending state if and only if for
each state, the set of infinite sequences labelling two runs starting from that
state has measure zero. The measure used to define these negligible sets, that
is sets of measure zero, can be any measure computed by a weighted automaton
which is compatible with the B\""uchi automaton. This latter condition is very
natural: the measure must put weight on cylinders [w] where w is the label of
some run in the B\""uchi automaton.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:06:10 GMT""},{""version"":""v2"",""created"":""Thu, 27 Jan 2022 17:48:57 GMT""},{""version"":""v3"",""created"":""Tue, 8 Feb 2022 14:26:46 GMT""},{""version"":""v4"",""created"":""Fri, 22 Apr 2022 16:13:46 GMT""}]","2022-04-25"
"2011.10535","Yago Ferreiros","Yago Ferreiros and Karl Landsteiner","On chiral responses to geometric torsion",,,"10.1016/j.physletb.2021.136419",,"cond-mat.mes-hall hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that geometric torsion does not lead to new chiral dissipationless
transport effects. Instead apparent response to torsion can be viewed as a
manifestation of the chiral vortical effect.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:07:22 GMT""},{""version"":""v2"",""created"":""Tue, 24 Nov 2020 17:51:54 GMT""},{""version"":""v3"",""created"":""Thu, 30 Dec 2021 14:47:10 GMT""}]","2022-01-03"
"2011.10536","Lucas Hofer","Lucas R. Hofer, Milan Krstaji\'c, P\'eter Juh\'asz, Anna L. Marchant,
  Robert P. Smith","Atom Cloud Detection Using a Deep Neural Network",,"Mach. Learn.: Sci. Technol. 2 045008 (2021)","10.1088/2632-2153/abf5ee",,"cond-mat.quant-gas physics.atom-ph","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We use a deep neural network to detect and place region-of-interest boxes
around ultracold atom clouds in absorption and fluorescence images---with the
ability to identify and bound multiple clouds within a single image. The neural
network also outputs segmentation masks that identify the size, shape and
orientation of each cloud from which we extract the clouds' Gaussian
parameters. This allows 2D Gaussian fits to be reliably seeded thereby enabling
fully automatic image processing.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:07:45 GMT""}]","2021-08-03"
"2011.10537","Alice Merryweather","Alice J. Merryweather, Christoph Schnedermann, Quentin Jacquet, Clare
  P. Grey, Akshay Rao","Operando optical tracking of single-particle ion dynamics and phase
  transitions in battery electrodes","18 pages, including 5 figures","Nature 594, 522-528 (2021)","10.1038/s41586-021-03584-2",,"cond-mat.mtrl-sci","http://creativecommons.org/licenses/by/4.0/","  Key to advancing lithium-ion battery technology, and in particular fast
charging capabilities, is our ability to follow and understand the dynamic
processes occurring in operating materials under realistic conditions, in real
time, and on the nano- to meso-scale. Currently, operando imaging of
lithium-ion dynamics requires sophisticated synchrotron X-ray or electron
microscopy techniques, which do not lend themselves to high-throughput material
screening. This limits rapid and rational materials improvements. Here we
introduce a simple lab-based, optical interferometric scattering microscope to
resolve nanoscopic lithium-ion dynamics in battery materials and apply it to
follow the repeated cycling of the archetypical cathode material
Li$_\textit{x}$CoO$_2$. The method allows us to visualise directly the
insulator-metal, solid solution and lithium ordering phase transitions in this
material. We determine rates of lithium insertion and removal at the
single-particle level and identify different mechanisms that occur on charge
vs. discharge. Finally, we capture the dynamic formation of domain boundaries
between different crystal orientations associated with the monoclinic lattice
distortion at around Li$_{0.5}$CoO$_2$. The high throughput nature of our
methodology allows many particles to be sampled across the entire electrode
and, moving forward, will enable exploration of the role of dislocations,
morphologies and cycling rate on battery degradation. The generality of our
imaging concept means that it can be applied to study any battery electrode,
and more broadly, systems where the transport of ions is associated with
electronic or structural changes, including nanoionic films, ionic conducting
polymers, photocatalytic materials and memristors.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:15:14 GMT""}]","2021-06-28"
"2011.10538","Andreas Schwarz","Andreas Schwarz, Ilya Sklyar, Simon Wiesler","Improving RNN-T ASR Accuracy Using Context Audio",,,,,"eess.AS cs.SD","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a training scheme for streaming automatic speech recognition (ASR)
based on recurrent neural network transducers (RNN-T) which allows the encoder
network to learn to exploit context audio from a stream, using segmented or
partially labeled sequences of the stream during training. We show that the use
of context audio during training and inference can lead to word error rate
reductions of more than 6% in a realistic production setting for a voice
assistant ASR system. We investigate the effect of the proposed training
approach on acoustically challenging data containing background speech and
present data points which indicate that this approach helps the network learn
both speaker and environment adaptation. To gain further insight into the
ability of a long short-term memory (LSTM) based ASR encoder to exploit
long-term context, we also visualize RNN-T loss gradients with respect to the
input.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:16:04 GMT""},{""version"":""v2"",""created"":""Thu, 27 May 2021 08:38:10 GMT""},{""version"":""v3"",""created"":""Tue, 15 Jun 2021 15:49:49 GMT""}]","2021-06-16"
"2011.10539","Hongki Jung","Hongki Jung","A sharp $L^{10}$ decoupling for the twisted cubic","43 pages",,,,"math.CA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We prove a sharp $l^{10}(L^{10})$ decoupling for the moment curve in
$\mathbb{R}^3$. The proof involves a two-step decoupling combined with new
incidence estimates for planks, tubes and plates.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:22:34 GMT""}]","2020-11-23"
"2011.10540","Yordan Yordanov","Yordan S. Yordanov, V. Armaos, Crispin H. W. Barnes, David R. M.
  Arvidsson-Shukur","Qubit-excitation-based adaptive variational quantum eigensolver",,,"10.1038/s42005-021-00730-0",,"quant-ph","http://creativecommons.org/licenses/by/4.0/","  Molecular simulations with the variational quantum eigensolver (VQE) are a
promising application for emerging noisy intermediate-scale quantum computers.
Constructing accurate molecular ans\""atze that are easy to optimize and
implemented by shallow quantum circuits is crucial for the successful
implementation of such simulations. Ans\""atze are, generally, constructed as
series of fermionic-excitation evolutions. Instead, we demonstrate the
usefulness of constructing ans\""atze with ""qubit-excitation evolutions"", which,
contrary to fermionic excitation evolutions, obey ""qubit commutation
relations"". We show that qubit excitation evolutions, despite the lack of some
of the physical features of fermionic excitation evolutions, accurately
construct ans\""atze, while requiring asymptotically fewer gates. Utilizing
qubit excitation evolutions, we introduce the qubit-excitation-based adaptive
(QEB-ADAPT)-VQE protocol. The QEB-ADAPT-VQE is a modification of the ADAPT-VQE
that performs molecular simulations using a problem-tailored ansatz, grown
iteratively by appending evolutions of qubit excitation operators. By
performing classical numerical simulations for small molecules, we benchmark
the QEB-ADAPT-VQE, and compare it against the original fermionic-ADAPT-VQE and
the qubit-ADAPT-VQE. In terms of circuit efficiency and convergence speed, we
demonstrate that the QEB-ADAPT-VQE outperforms the qubit-ADAPT-VQE, which to
our knowledge was the previous most circuit-efficient scalable VQE protocol for
molecular simulations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:27:08 GMT""},{""version"":""v2"",""created"":""Thu, 14 Oct 2021 11:24:56 GMT""}]","2021-10-15"
"2011.10541","Karim Kassan","Karim Kassan, Ha\""ifa Far\`es, D. Christian Glattli and Yves Lou\""et","Performance vs. Spectral Properties For Single-Sideband Continuous Phase
  Modulation","- We added a new optimization method to take into consideration the
  complexity. - We added a new comparison between new SSB-FSK schemes and the
  CPM modulations (RC, GMSK). - We presented a new SSB-FSK scheme with some
  good synchronization advantages. - We fixed some typo mistakes. - We updated
  the overall quality of the paper",,,,"eess.SP quant-ph","http://creativecommons.org/licenses/by/4.0/","  This study revokes the performance of continuous phase modulation (CPM) able
to generate a single-sideband (SSB) spectrum directly. This signal is analyzed
in terms of modulation indices, pulse lengths, and pulse widths, all of which
affect error probability, bandwidth, SSB property, and receiver complexity. The
error probability performance is based on an approximation of the minimum
Euclidean distance. A numerical power spectral density calculation for this
particular SSB modulation in terms of modulation index is presented. Reasonable
tradeoffs in designing modulation schemes have been proposed using
multi-objective optimization to ensure sizable improvements in bit error rate
(BER), spectral efficiencies, and complexity without losing the property of
being a SSB signal. Performance comparisons are made with known CPM schemes,
e.g., Gaussian Minimum Shift Keying (GMSK) and Raised Cosine based CPM (RC)
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:27:19 GMT""},{""version"":""v2"",""created"":""Tue, 16 Mar 2021 16:14:08 GMT""}]","2021-03-17"
"2011.10542","Carlo Mercuri carlomercuri","Bj\""orn Baumeier, Onur \c{C}aylak, Carlo Mercuri, Mark Peletier, Georg
  Prokert, Wouter Scharpach","Existence and uniqueness of solutions to the time-dependent Kohn-Sham
  equations coupled with classical nuclear dynamics",,,,,"math.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We prove existence and uniqueness of solutions to the initial-value problem
associated with a class of time-dependent Kohn-Sham equations coupled with
Newtonian nuclear dynamics. We consider a pure power exchange term within a
generalisation of the local density approximation, identifying a range of
exponents for the existence and uniqueness of $H^2$ solutions to the Kohn-Sham
equations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:29:17 GMT""},{""version"":""v2"",""created"":""Tue, 14 Dec 2021 10:38:29 GMT""}]","2021-12-15"
"2011.10543","The ATLAS Collaboration","ATLAS Collaboration","Search for trilepton resonances from chargino and neutralino pair
  production in $\sqrt{s}$ = 13 TeV $pp$ collisions with the ATLAS detector","51 pages in total, author list starting page 35, 9 figures, 4 tables,
  submitted to Physical Review D. All figures including auxiliary figures are
  available at
  http://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2018-36","Phys. Rev. D 103, 112003 (2021)","10.1103/PhysRevD.103.112003","CERN-EP-2020-201","hep-ex","http://creativecommons.org/licenses/by/4.0/","  A search is performed for the electroweak pair production of charginos and
associated production of a chargino and neutralino, each of which decays
through an $R$-parity-violating coupling into a lepton and a $W$, $Z$, or Higgs
boson. The trilepton invariant-mass spectrum is constructed from events with
three or more leptons, targeting chargino decays that include an electron or
muon and a leptonically decaying $Z$ boson. The analyzed dataset corresponds to
an integrated luminosity of 139 fb$^{-1}$ of proton-proton collision data
produced by the Large Hadron Collider at a center-of-mass energy of $\sqrt{s}$
= 13 TeV and collected by the ATLAS experiment between 2015 and 2018. The data
are found to be consistent with predictions from the Standard Model. The
results are interpreted as limits at 95% confidence level on model-independent
cross sections for processes beyond the Standard Model. Limits are also set on
the production of charginos and neutralinos for a Minimal Supersymmetric
Standard Model with an approximate $B$-$L$ symmetry. Charginos and neutralinos
with masses between 100 GeV and 1100 GeV are excluded depending on the assumed
decay branching fractions into a lepton (electron, muon, or $\tau$-lepton) plus
a boson ($W$, $Z$, or Higgs).
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:29:58 GMT""},{""version"":""v2"",""created"":""Mon, 14 Jun 2021 16:47:28 GMT""}]","2021-06-16"
"2011.10544","Sanhan Khasraw","Sanhan Khasraw","On Intersection Graph of Dihedral Group",,,,,"math.CO","http://creativecommons.org/licenses/by/4.0/","  Let $G$ be a finite group. The intersection graph of $G$ is a graph whose
vertex set is the set of all proper non-trivial subgroups of $G$ and two
distinct vertices $H$ and $K$ are adjacent if and only if $H\cap K \neq \{e\}$,
where $e$ is the identity of the group $G$. In this paper, we investigate some
properties and exploring some topological indices such as Wiener, Hyper-Wiener,
first and second Zagreb, Schultz, Gutman and eccentric connectivity indices of
the intersection graph of $D_{2n}$ for $n=p^2$, $p$ is prime. We also find the
metric dimension and the resolving polynomial of the intersection graph of
$D_{2p^2}$.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:32:13 GMT""},{""version"":""v2"",""created"":""Sun, 27 Dec 2020 18:41:25 GMT""},{""version"":""v3"",""created"":""Tue, 29 Dec 2020 19:58:48 GMT""}]","2021-01-01"
"2011.10545","Evaldas Vaiciukynas Dr.","Evaldas Vaiciukynas, Paulius Danenas, Vilius Kontrimas, Rimantas
  Butleris","Two-Step Meta-Learning for Time-Series Forecasting Ensemble","Accepted to IEEE Access journal in April 22, 2021","in IEEE Access, vol. 9, pp. 62687-62696, 2021","10.1109/ACCESS.2021.3074891",,"stat.ML cs.LG stat.ME","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Amounts of historical data collected increase and business intelligence
applicability with automatic forecasting of time series are in high demand.
While no single time series modeling method is universal to all types of
dynamics, forecasting using an ensemble of several methods is often seen as a
compromise. Instead of fixing ensemble diversity and size, we propose to
predict these aspects adaptively using meta-learning. Meta-learning here
considers two separate random forest regression models, built on 390
time-series features, to rank 22 univariate forecasting methods and recommend
ensemble size. The forecasting ensemble is consequently formed from methods
ranked as the best, and forecasts are pooled using either simple or weighted
average (with a weight corresponding to reciprocal rank). The proposed approach
was tested on 12561 micro-economic time-series (expanded to 38633 for various
forecasting horizons) of M4 competition where meta-learning outperformed Theta
and Comb benchmarks by relative forecasting errors for all data types and
horizons. Best overall results were achieved by weighted pooling with a
symmetric mean absolute percentage error of 9.21% versus 11.05% obtained using
the Theta method.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:35:02 GMT""},{""version"":""v2"",""created"":""Fri, 15 Apr 2022 10:07:10 GMT""}]","2022-04-18"
"2011.10546","Paul Oxby PhD","Paul W. Oxby","A Function Based on Chebyshev Polynomials as an Alternative to the Sinc
  Function in FIR Filter Design","27 pages, 7 figures, 10 tables. First revision incorporated a
  comprehensive analysis of the Fourier transform of the proposed alternative
  to the sinc function. Second revision incorporated Tikhonev regularization
  into the discretization compensation algorithm and revised the analysis of
  the performance of this algorithm",,,,"eess.SP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The sinc function is often used as the basis for the design of discrete
linear-phase FIR filters. However the Fourier transform of the truncated sinc
function exhibits ripple in the pass band due to the Gibbs phenomenon. This
paper introduces an alternative function based on Chebyshev polynomials whose
Fourier transform decreases monotonically in the pass band. Furthermore this
function features an intrinsic window function with an adjustable parameter
influencing the Fourier transform in the transition and stop bands.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:39:10 GMT""},{""version"":""v2"",""created"":""Tue, 9 Feb 2021 10:32:21 GMT""},{""version"":""v3"",""created"":""Wed, 26 May 2021 04:57:25 GMT""}]","2021-05-27"
"2011.10547","Raphael Gobat","Raphael Gobat, Georgios Magdis, Chiara D'Eugenio, Francesco Valentino","The evolution of the gas fraction of quiescent galaxies modeled as a
  consequence of their creation rate","7 pages, 5 figures, 2 tables (including appendices). Accepted for
  publication in Astronomy & Astrophysics",,"10.1051/0004-6361/202039593",,"astro-ph.GA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We discuss the evolution of the interstellar medium of quiescent galaxies,
currently emerging from recent analyses, with the help of a simple model based
on well-established empirical relations such as the stellar mass functions and
the main sequence of star formation. This model is meant to describe observed
quantities without making specific assumptions on the nature of quenching
processes, but relying on their observable consequences. We find that the high
gas fractions seen or suggested at high redshift in quiescent galaxies, and
their apparent mild evolution at early times, can be mostly attributed to a
progenitor effect where recently quenched galaxies with ~10% gas fractions
dominate the quiescent galaxy population until z~1. In the same context, the
much lower gas and dust fractions measured in local early-type galaxies are
interpreted as the product of the steady depletion of their interstellar medium
on a ~2 Gyr timescale, coupled with a higher fraction of more gas-exhaustive
events.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:40:22 GMT""}]","2020-12-16"
"2011.10548","Stephane Perrard","St\'ephane Perrard, Ali\'enor Rivi\`ere, Wouter Mostert, Luc Deike","Bubble deformation by a turbulent flow",,,"10.1017/jfm.2021.379",,"physics.flu-dyn","http://creativecommons.org/licenses/by/4.0/","  We investigate the modes of deformation of an initially spherical bubble
immersed in a homogeneous and isotropic turbulent background flow. We perform
direct numerical simulations of the two-phase incompressible Navier-Stokes
equations, considering a low-density bubble in the high density turbulent flow
at various Weber number (the ratio of turbulent and surface tension forces)
using the air-water density ratio. We discuss a theoretical framework for the
bubble deformation in a turbulent flow using a spherical harmonic
decomposition. We propose, for each mode of bubble deformation, a forcing term
given by the statistics of velocity and pressure fluctuations, evaluated on a
sphere of the same radius. This approach formally relates the bubble
deformation and the background turbulent velocity fluctuations, in the limit of
small deformations. The growth of the total surface deformation and of each
individual mode is computed from the direct numerical simulations using an
appropriate Voronoi decomposition of the bubble surface. We show that two
successive temporal regimes occur: the first regime corresponds to deformations
driven only by inertial forces, with the interface deformation growing linearly
in time, in agreement with the model predictions, whereas the second regime
results from a balance between inertial forces and surface tension. The
transition time between the two regimes is given by the period of the first
Rayleigh mode of bubble oscillation. We discuss how our approach can be used to
relate the bubble lifetime to the turbulence statistics and eventually show
that at high Weber number, bubble lifetime can be deduced from the statistics
of turbulent fluctuations at the bubble scale.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:40:25 GMT""},{""version"":""v2"",""created"":""Tue, 13 Apr 2021 20:11:07 GMT""}]","2021-07-07"
"2011.10549","Ankith Mohan","Ankith Mohan, Aiichiro Nakano, Emilio Ferrara","Graph Signal Recovery Using Restricted Boltzmann Machines","Paper: 27 pages, 9 figures. Appendix: 5 pages, 12 figures. Submitted
  to Expert Systems with Applications",,,,"cs.LG cs.AI cs.NE cs.SI","http://creativecommons.org/licenses/by-sa/4.0/","  We propose a model-agnostic pipeline to recover graph signals from an expert
system by exploiting the content addressable memory property of restricted
Boltzmann machine and the representational ability of a neural network. The
proposed pipeline requires the deep neural network that is trained on a
downward machine learning task with clean data, data which is free from any
form of corruption or incompletion. We show that denoising the representations
learned by the deep neural networks is usually more effective than denoising
the data itself. Although this pipeline can deal with noise in any dataset, it
is particularly effective for graph-structured datasets.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:43:53 GMT""}]","2020-11-23"
"2011.10550","Mark Stephenson","Mark W. Stephenson and Ram Rangan","AZP: Automatic Specialization for Zero Values in Gaming Applications",,,,,"cs.AR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent research has shown that dynamic zeros in shader programs of gaming
applications can be effectively leveraged with a profile-guided,
code-versioning transform. This transform duplicates code, specializes one path
assuming certain key program operands, called versioning variables, are zero,
and leaves the other path unspecialized. Dynamically, depending on the
versioning variable's value, either the specialized fast path or the default
slow path will execute. Prior work applied this transform manually and showed
promising gains on gaming applications. In this paper, we present AZP, an
automatic compiler approach to perform the above code-versioning transform. Our
framework automatically determines which versioning variables or combinations
of them are profitable, and determines the code region to duplicate and
specialize (called the versioning scope). AZP takes operand zero value
probabilities as input and it then uses classical techniques such as constant
folding and dead-code elimination to determine the most profitable versioning
variables and their versioning scopes. This information is then used to affect
the final transform in a straightforward manner. We demonstrate that AZP is
able to achieve an average speedup of 16.4% for targeted shader programs,
amounting to an average frame-rate speedup of 3.5% across a collection of
modern gaming applications on an NVIDIA GeForce RTX 2080 GPU GPU.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:44:01 GMT""}]","2020-11-23"
"2011.10551","John M. Lawson","John M. Lawson","Mass transfer to freely suspended particles at high P\'eclet number","Under consideration for publication in the Journal of Fluid Mechanics","J. Fluid Mech. 913 (2021) A32","10.1017/jfm.2020.1177",,"physics.flu-dyn cond-mat.soft","http://creativecommons.org/licenses/by/4.0/","  In a theoretical analysis, we generalise well known asymptotic results to
obtain expressions for the rate of transfer of material from the surface of an
arbitrary, rigid particle suspended in an open pathline flow at large P\'eclet
number, $\textrm{Pe}$. The flow may be steady or periodic in time. We apply
this result to numerically evaluate expressions for the surface flux to a
freely suspended, axisymmetric ellipsoid (spheroid) in Stokes flow driven by a
steady linear shear. We complement these analytical predictions with numerical
simulations conducted over a range of $\textrm{Pe} = 10^1 - 10^4$ and confirm
good agreement at large P\'eclet number. Our results allow us to examine the
influence of particle shape upon the surface flux for a broad class of flows.
When the background flow is irrotational, the surface flux is steady and is
prescribed by three parameters only: the P\'eclet number, the particle aspect
ratio and the strain topology. We observe that slender prolate spheroids tend
to experience a higher surface flux compared to oblate spheroids with
equivalent surface area. For rotational flows, particles may begin to spin or
tumble, which may suppress or augment the convective transfer due to a
realignment of the particle with respect to the strain field.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:44:19 GMT""}]","2021-07-01"
"2011.10552","Michael Schlosser","Michael J. Schlosser and Nian Hong Zhou","On the infinite Borwein product raised to a positive real power","24 pages; an appendix with several new conjectures added; to appear
  in the Ramanujan Journal; dedicated to the memory of Richard Allen Askey",,,,"math.NT","http://creativecommons.org/licenses/by/4.0/","  In this paper, we study properties of the coefficients appearing in the
$q$-series expansion of $\prod_{n\ge 1}[(1-q^n)/(1-q^{pn})]^\delta$, the
infinite Borwein product for an arbitrary prime $p$, raised to an arbitrary
positive real power $\delta$. We use the Hardy--Ramanujan--Rademacher circle
method to give an asymptotic formula for the coefficients. For $p=3$ we give an
estimate of their growth which enables us to partially confirm an earlier
conjecture of the first author concerning an observed sign pattern of the
coefficients when the exponent $\delta$ is within a specified range of positive
real numbers. We further establish some vanishing and divisibility properties
of the coefficients of the cube of the infinite Borwein product. We conclude
with an Appendix presenting several new conjectures on precise sign patterns of
infinite products raised to a real power which are similar to the conjecture we
made in the $p=3$ case.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:46:15 GMT""},{""version"":""v2"",""created"":""Mon, 13 Sep 2021 11:13:00 GMT""}]","2021-09-14"
"2011.10553","Mark Fardal","Mark A. Fardal, Roeland van der Marel, Andres del Pino, and Sangmo
  Tony Sohn","Mapping Gaia parallax systematic errors over the sky with faint Milky
  Way stars","Accepted in AJ. 17 pages, 11 figures. Higher resolution figures
  available in journal. Eagerly anticipating Gaia EDR3",,"10.3847/1538-3881/abcccf",,"astro-ph.GA astro-ph.IM","http://creativecommons.org/licenses/by/4.0/","  Parallaxes measured by the Gaia mission have huge significance for astronomy,
but parallaxes in Gaia DR2 are known to have systematic errors that depend on
the source position and other quantities. We use the abundant information in
faint Milky Way stars, along with the GOG simulation of the Gaia catalog, to
probe the spatial dependence of Gaia DR2 parallax systematic errors in an
empirical way. The parallax signal, concentrated in thick disk turnoff stars
with magnitude G ~ 17, is sufficient to construct maps of the parallax
systematic error over the majority of the sky. These maps show a locally
regular ""waffle pattern"" on ~1 degree scales following Gaia scan directions,
stronger linear ""scar"" features, and coherent variations on larger scales. The
parallax bias maps also retain traces of astrophysical effects such as dust
clouds. The waffle pattern, known from earlier maps of the Magellanic Clouds,
extends over the entire sky; its local rms amplitude averages 15 microarcsec
and varies by about a factor of two. The strength of this pattern increases by
a factor ~6 from magnitude G = 13 to G = 20. Correlations with parallaxes of
quasars and of stars with independent distance estimates support our bias
estimates. Using similar methods, we map systematic errors in the proper motion
and examine the relationship with the parallax systematics. We provide a code
package to access and query our bias maps. Similar tests on the general stellar
population should be useful in quantifying systematic errors in future Gaia
releases.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:46:27 GMT""}]","2021-01-20"
"2011.10554","Eric Bobrow","Eric Bobrow, Junjia Zhang, Yi Li","Hund's coupling-assisted ferromagnetic percolation transition in a
  multiorbital flat band",,"Phys. Rev. B 104, 064442 (2021)","10.1103/PhysRevB.104.064442",,"cond-mat.str-el math-ph math.MP","http://creativecommons.org/licenses/by/4.0/","  By connecting Hund's physics with flat band physics, we establish an exact
result for studying ferromagnetism in a multiorbital system. We consider a
two-layer model consisting of a $p_x$, $p_y$-orbital honeycomb lattice layer
and an $f$-orbital triangular lattice layer with sites aligned with the centers
of the honeycomb plaquettes. The system features a flat band that admits a
percolation representation for an appropriate chemical potential difference
between the two layers. In this representation, the ground state space is
spanned by maximum-spin clusters of localized single-particle states, and
averaging over the ground states yields a correlated percolation problem with
weights due to the spin degeneracy of the clusters. A
paramagnetic-ferromagnetic transition occurs as the band approaches half
filling and the ground states become dominated by states with a large
maximum-spin cluster, as shown by Monte Carlo simulation.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:48:48 GMT""},{""version"":""v2"",""created"":""Thu, 3 Dec 2020 03:09:00 GMT""},{""version"":""v3"",""created"":""Thu, 29 Apr 2021 19:14:24 GMT""},{""version"":""v4"",""created"":""Thu, 26 Aug 2021 19:38:49 GMT""}]","2021-08-30"
"2011.10555","Fehmi Ekmek\c{c}i","Osman Karaku\c{s} and Fehmi Ekmek\c{c}i","New Spectral Analysis Results Within The Scope Of Extended Matter
  Research in The AR Lacertae Active Binary System","23 pages,6 tables,11 figures: This study was accepted for publication
  in RMxAA(the paper will appear in Vol. 57, N{\deg}1, April 2021)",,"10.22201/ia.01851101p.2021.57.01.12",,"astro-ph.SR","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Within the scope of extended matter research, we present new spectral
analysis results of an active binary system AR Lac. The low and high resolution
spectra of this system were taken during the period 2013-2016. The evaluation
of low dispersion spectra together with the B, V, Rc, Ic and WISE photometric
data showed that AR Lac has an excess radiation in W2 band. In addition, the
spectral energy distribution and the minima depth ratios of the light curves of
this active binary system were studied to examine the flux contributions of the
components of the system depending on wavelengths and on orbital phase.
Furthermore, high resolution spectral analysis showed evidence of
prominence-like structures and a possible extended matter around the cooler
component of AR Lac binary system.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:51:46 GMT""}]","2021-03-31"
"2011.10556","N. E. J. Bjerrum-Bohr","N. Emil J. Bjerrum-Bohr, Taro V. Brown and Humberto Gomez","Scattering of Gravitons and Spinning Massive States from Compact
  Numerators","32 pages, 5 figures, version to be published in JHEP",,"10.1007/JHEP04(2021)234",,"hep-th gr-qc","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We provide a new efficient diagrammatic tool, in the context of the
scattering equations, for computation of covariant $D$-dimensional tree-level
$n$-point amplitudes with pairs of spinning massive particles using compact
exponential numerators. We discuss how this framework allows non-integer spin
extensions of recurrence relations for amplitudes developed for integer spin.
Our results facilitate the on-going program for generating observables in
classical general relativity from on-shell tree amplitudes through the
Kawai-Lewellen-Tye relations and generalized unitarity.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:53:32 GMT""},{""version"":""v2"",""created"":""Thu, 22 Apr 2021 22:03:41 GMT""}]","2021-05-12"
"2011.10557","Giovanni Camelio","Giovanni Camelio and Tim Dietrich and Stephan Rosswog and Brynmor
  Haskell","Axisymmetric models for neutron star merger remnants with realistic
  thermal and rotational profiles","18 pages, 10 figures, 1 appendix, published on PRD. Companion dataset
  can be downloaded from DOI:10.5281/zenodo.4268501","Phys. Rev. D 103, 063014 (2021)","10.1103/PhysRevD.103.063014",,"astro-ph.HE gr-qc","http://creativecommons.org/licenses/by/4.0/","  Merging neutron stars are expected to produce hot, metastable remnants in
rapid differential rotation, which subsequently cool and evolve into rigidly
rotating neutron stars or collapse to black holes. Studying this metastable
phase and its further evolution is essential for the prediction and
interpretation of the electromagnetic, neutrino, and gravitational signals from
such a merger. In this work, we model binary neutron star merger remnants and
propose new rotation and thermal laws that describe post-merger remnants. Our
framework is capable to reproduce quasi-equilibrium configurations for generic
equations of state, rotation and temperature profiles, including nonbarotropic
ones. We demonstrate that our results are in agreement with numerical
relativity simulations concerning bulk remnant properties like the mass,
angular momentum, and the formation of a massive accretion disk. Because of the
low computational cost for our axisymmetric code compared to full
3+1-dimensional simulations, we can perform an extensive exploration of the
binary neutron star remnant parameter space studying several hundred thousand
configurations for different equation of states.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:54:16 GMT""},{""version"":""v2"",""created"":""Thu, 18 Feb 2021 19:26:10 GMT""},{""version"":""v3"",""created"":""Thu, 18 Mar 2021 14:17:18 GMT""}]","2021-03-19"
"2011.10558","Marrick Braam","Marrick Braam, Floris F. S. van der Tak, Katy L. Chubb, Michiel Min","Evidence for chromium hydride in the atmosphere of Hot Jupiter WASP-31b","Accepted for publication in A&A (Nov 24, 2020). 12 pages, 6 figures,
  9 tables","A&A 646, A17 (2021)","10.1051/0004-6361/202039509",,"astro-ph.EP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Context. The characterisation of exoplanet atmospheres has shown a wide
diversity in compositions. Hot Jupiters have the appropriate temperatures to
host metallic compounds, which should be detectable through transmission
spectroscopy. Aims. We aim to detect exotic species in transmission spectra of
hot Jupiters, specifically WASP-31b, by testing a variety of chemical species
to explain the spectrum. Methods. We conduct a re-analysis of publicly
available transmission data of WASP-31b using the Bayesian retrieval framework
TauREx II. We retrieve various combinations of the opacities of 25 species to
determine the minimum set needed to fit the observed spectrum. Results. We
report evidence for the spectroscopic signatures of chromium hydride (CrH), H2O
and K in WASP-31b. Compared to a flat model without any signatures, a CrH-only
model is preferred with a statistical significance of ~3.9 sigma. A model
consisting of both CrH+H2O is found with ~2.6 sigma and ~3 sigma confidence
over a CrH-only and H2O-only model respectively. Weak evidence for the addition
of K is found at ~2.2 sigma over the H2O+CrH model, although the fidelity of
the data point associated with this signature was questioned in earlier
studies. Finally, the inclusion of collision-induced absorption and a Rayleigh
scattering slope is found with ~3.5 sigma confidence over a flat model. This
analysis presents the first evidence for signatures of CrH in a hot Jupiter
atmosphere. At a retrieved temperature of ~1481 K, the atmosphere of WASP-31b
is hot enough to host gaseous Cr-bearing species. The retrieved abundances
agree well with predictions from thermal equilibrium chemistry and with the
abundance found in an L-type brown dwarf atmosphere. However, additional
retrievals using VLT FORS2 data lead to a non-detection of CrH. Future coverage
with JWST has the potential to confirm the detection and discover other CrH
features.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:54:20 GMT""},{""version"":""v2"",""created"":""Tue, 22 Dec 2020 16:47:15 GMT""}]","2021-02-03"
"2011.10559","Fabrizio Renzi","Fabrizio Renzi and Alessandra Silvestri","A look at the Hubble speed from first principles","7 pages, 5 figures. Any comments are welcome",,,,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce a novel way of measuring $H_0$ from a combination of independent
geometrical datasets, with no need of calibration nor of the choice of a
cosmological model. We build on the {\it distance duality relation} which sets
the ratio of the luminosity and angular diameter distance to a fixed scaling in
redshift for any metric theory of gravity with standard photon propagation and
constitutes a founding block of any theory describing our Universe. Our method
provides the unprecedented possibility of determining $H_0$ from first
principles, unleashing the measurement of this fundamental constant from
calibration and assumption of a cosmological model. We find $H_0=69.5 \pm 1.7$
km/s/Mpc at $68\%$ C.L. showing that the Hubble constant can be constrained at
percent level with minimal assumptions.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:55:01 GMT""},{""version"":""v2"",""created"":""Fri, 17 Jun 2022 13:42:46 GMT""}]","2022-06-20"
"2011.10560","Mansur Zhussupbekov","Mansur Zhussupbekov, Wei-Tao Wu, Megan A. Jamiolkowski, Mehrdad
  Massoudi, James F. Antaki","Influence of shear rate and surface chemistry on thrombus formation in
  micro-crevice",,"Journal of Biomechanics, Volume 121, 2021, 110397, ISSN 0021-9290","10.1016/j.jbiomech.2021.110397",,"q-bio.TO physics.bio-ph physics.flu-dyn","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Thromboembolic complications remain a central issue in management of patients
on mechanical circulatory support. Despite the best practices employed in
design and manufacturing of modern ventricular assist devices, complexity and
modular nature of these systems often introduces internal steps and crevices in
the flow path which can serve as nidus for thrombus formation. Thrombotic
potential is influenced by multiple factors including the characteristics of
the flow and surface chemistry of the biomaterial. This study explored these
elements in the setting of blood flow over a micro-crevice using a
multi-constituent numerical model of thrombosis. The simulations reproduced the
platelet deposition patterns observed experimentally and elucidated the role of
flow, shear rate, and surface chemistry in shaping the deposition. The results
offer insights for design and operation of blood-contacting devices.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:55:21 GMT""},{""version"":""v2"",""created"":""Thu, 31 Dec 2020 07:44:58 GMT""},{""version"":""v3"",""created"":""Thu, 1 Apr 2021 19:45:54 GMT""}]","2021-08-31"
"2011.10561","I-Da Chiang","I-Da Chiang, Karin M. Sandstrom, J\'er\'emy Chastenet, Cinthya N.
  Herrera, Eric W. Koch, Kathryn Kreckel, Adam K. Leroy, J\'er\^ome Pety,
  Andreas Schruba, Dyas Utomo, Thomas Williams","Resolving the Dust-to-Metals Ratio and CO-to-H$_2$ Conversion Factor in
  the Nearby Universe","22 pages, 8 figures, 4 tables; accepted by ApJ",,"10.3847/1538-4357/abceb6",,"astro-ph.GA","http://creativecommons.org/licenses/by/4.0/","  We investigate the relationship between the dust-to-metals ratio (D/M) and
the local interstellar medium environment at ~2 kpc resolution in five nearby
galaxies: IC342, M31, M33, M101, and NGC628. A modified blackbody model with a
broken power-law emissivity is used to model the dust emission from 100 to 500
um observed by Herschel. We utilize the metallicity gradient derived from
auroral line measurements in HII regions whenever possible. Both archival and
new CO rotational line and HI 21 cm maps are adopted to calculate gas surface
density, including new wide field CO and HI maps for IC342 from IRAM and the
VLA, respectively. We experiment with several prescriptions of CO-to-H$_2$
conversion factor, and compare the resulting D/M-metallicity and D/M-density
correlations, both of which are expected to be non-negative from depletion
studies. The D/M is sensitive to the choice of the conversion factor. The
conversion factor prescriptions based on metallicity only yield too much
molecular gas in the center of IC342 to obtain the expected correlations. Among
the prescriptions tested, the one that yields the expected correlations depends
on both metallicity and surface density. The 1-$\sigma$ range of the derived
D/M spans 0.40-0.58. Compared to chemical evolution models, our measurements
suggest that the dust growth time scale is much shorter than the dust
destruction time scale. The measured D/M is consistent with D/M in
galaxy-integrated studies derived from infrared dust emission. Meanwhile, the
measured D/M is systematically higher than the D/M derived from absorption,
which likely indicates a systematic offset between the two methods.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:55:45 GMT""},{""version"":""v2"",""created"":""Mon, 30 Nov 2020 22:23:22 GMT""},{""version"":""v3"",""created"":""Fri, 22 Jan 2021 23:13:35 GMT""}]","2021-01-26"
"2011.10562","Anubhav Guha","Anubhav Guha and Anuradha Annaswamy","MRAC-RL: A Framework for On-Line Policy Adaptation Under Parametric
  Model Uncertainty","Short version submitted to Learning for Dynamics & Control (L4DC)
  2021 Conference",,,,"eess.SY cs.LG cs.RO cs.SY","http://creativecommons.org/licenses/by/4.0/","  Reinforcement learning (RL) algorithms have been successfully used to develop
control policies for dynamical systems. For many such systems, these policies
are trained in a simulated environment. Due to discrepancies between the
simulated model and the true system dynamics, RL trained policies often fail to
generalize and adapt appropriately when deployed in the real-world environment.
Current research in bridging this sim-to-real gap has largely focused on
improvements in simulation design and on the development of improved and
specialized RL algorithms for robust control policy generation. In this paper
we apply principles from adaptive control and system identification to develop
the model-reference adaptive control & reinforcement learning (MRAC-RL)
framework. We propose a set of novel MRAC algorithms applicable to a broad
range of linear and nonlinear systems, and derive the associated control laws.
The MRAC-RL framework utilizes an inner-loop adaptive controller that allows a
simulation-trained outer-loop policy to adapt and operate effectively in a test
environment, even when parametric model uncertainty exists. We demonstrate that
the MRAC-RL approach improves upon state-of-the-art RL algorithms in developing
control policies that can be applied to systems with modeling errors.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:55:53 GMT""}]","2020-11-23"
"2011.10563","Konstantinos Kousias","Konstantinos Kousias, Apostolos Pappas, Ozgu Alay, Antonios Argyriou
  and Michael Riegler","Long Short Term Memory Networks for Bandwidth Forecasting in Mobile
  Broadband Networks under Mobility",,,,,"cs.NI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Bandwidth forecasting in Mobile Broadband (MBB) networks is a challenging
task, particularly when coupled with a degree of mobility. In this work, we
introduce HINDSIGHT++, an open-source R-based framework for bandwidth
forecasting experimentation in MBB networks with Long Short Term Memory (LSTM)
networks. We instrument HINDSIGHT++ following an Automated Machine Learning
(AutoML) paradigm to first, alleviate the burden of data preprocessing, and
second, enhance performance related aspects. We primarily focus on bandwidth
forecasting for Fifth Generation (5G) networks. In particular, we leverage
5Gophers, the first open-source attempt to measure network performance on
operational 5G networks in the US. We further explore the LSTM performance
boundaries on Fourth Generation (4G) commercial settings using NYU-METS, an
open-source dataset comprising of hundreds of bandwidth traces spanning
different mobility scenarios. Our study aims to investigate the impact of
hyperparameter optimization on achieving state-of-the-art performance and
beyond. Results highlight its significance under 5G scenarios showing an
average Mean Absolute Error (MAE) decrease of near 30% when compared to prior
state-of-the-art values. Due to its universal design, we argue that HINDSIGHT++
can serve as a handy software tool for a multitude of applications in other
scientific fields.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:59:27 GMT""}]","2020-11-23"
"2011.10564","Dawei Ding","Dawei Ding, Hsiang-Sheng Ku, Yaoyun Shi, Hui-Hai Zhao","Free Mode Removal and Mode Decoupling for Simulating General
  Superconducting Quantum Circuits","18 pages, 5 figures","Phys. Rev. B 103, 174501 (2021)","10.1103/PhysRevB.103.174501",,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Superconducting quantum circuits is one of the leading candidates for a
universal quantum computer. Designing novel qubit and multiqubit
superconducting circuits requires the ability to simulate and analyze the
properties of a general circuit. In particular, going outside the transmon
approach, we cannot make assumptions on anharmonicity, thus precluding blackbox
quantization approaches and necessitating the formal circuit quantization
approach. We consider and solve two issues involved in simulating general
superconducting circuits. One of the issues is the handling of free modes in
the circuit, that is, circuit modes with no potential term in the Hamiltonian.
Another issue is circuit size, namely the challenge of simulating strongly
coupled multimode circuits. The main mathematical tool we use to address these
issues is the linear canonical transformation in the setting of quantum
mechanics. We address the first issue by giving a provably correct algorithm
for removing free modes by performing a linear canonical transformation to
completely decouple the free modes from other circuit modes. We address the
second by giving a series of different linear canonical transformations to
reduce intermode couplings, thereby reducing the problem to the weakly coupled
case and greatly mitigating the overhead for classical simulation. We benchmark
our decoupling methods by applying them to the circuit of two inductively
coupled fluxonium qubits, obtaining several orders of magnitude reduction in
the size of the Hilbert space that needs to be simulated.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:59:29 GMT""},{""version"":""v2"",""created"":""Sat, 12 Dec 2020 02:14:17 GMT""},{""version"":""v3"",""created"":""Wed, 5 May 2021 19:09:57 GMT""}]","2021-05-07"
"2011.10565","Andrea Tesi","Michele Redi, Andrea Tesi, Hannah Tillim","Gravitational Production of a Conformal Dark Sector","24 pages, 2 figures. v2) factor 2 fixed, references and
  clarifications added, results unchanged",,"10.1007/JHEP05(2021)010",,"hep-ph astro-ph.CO hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Dark sectors with purely gravitational couplings to the Standard Model are
unavoidably populated from the SM plasma by graviton exchange, and naturally
provide DM candidates. We examine the production in the relativistic regime
where the dark sector is approximately scale invariant, providing general
analytical formulas that depend solely on the central charge of the dark
sector. We then assess the relevance of interactions that can lead to a variety
of phenomena including thermalisation, nonperturbative mass gaps,
out-of-equilibrium phase transitions and cannibalism in the dark sector. As an
illustrative example we consider the dark glueball scenario in this light and
show it to be a viable DM candidate due to the suppression of gravitational
production. We go on to extend these results to strongly coupled CFTs and their
holographic duals at large-N with the dark dilaton as the DM candidate.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:59:32 GMT""},{""version"":""v2"",""created"":""Tue, 16 Mar 2021 13:39:24 GMT""}]","2021-05-19"
"2011.10566","Xinlei Chen","Xinlei Chen and Kaiming He","Exploring Simple Siamese Representation Learning","Technical report, 10 pages",,,,"cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Siamese networks have become a common structure in various recent models for
unsupervised visual representation learning. These models maximize the
similarity between two augmentations of one image, subject to certain
conditions for avoiding collapsing solutions. In this paper, we report
surprising empirical results that simple Siamese networks can learn meaningful
representations even using none of the following: (i) negative sample pairs,
(ii) large batches, (iii) momentum encoders. Our experiments show that
collapsing solutions do exist for the loss and structure, but a stop-gradient
operation plays an essential role in preventing collapsing. We provide a
hypothesis on the implication of stop-gradient, and further show
proof-of-concept experiments verifying it. Our ""SimSiam"" method achieves
competitive results on ImageNet and downstream tasks. We hope this simple
baseline will motivate people to rethink the roles of Siamese architectures for
unsupervised representation learning. Code will be made available.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:59:33 GMT""}]","2020-11-23"
"2011.10568","Nishant Sinha","Azhar Shaikh, Nishant Sinha","Learn to Bind and Grow Neural Structures","Accepted to 8th ACM IKDD CODS and 26th COMAD (CODS-COMAD '21)
  conference",,,,"cs.LG cs.AI cs.NE","http://creativecommons.org/licenses/by/4.0/","  Task-incremental learning involves the challenging problem of learning new
tasks continually, without forgetting past knowledge. Many approaches address
the problem by expanding the structure of a shared neural network as tasks
arrive, but struggle to grow optimally, without losing past knowledge. We
present a new framework, Learn to Bind and Grow, which learns a neural
architecture for a new task incrementally, either by binding with layers of a
similar task or by expanding layers which are more likely to conflict between
tasks. Central to our approach is a novel, interpretable, parameterization of
the shared, multi-task architecture space, which then enables computing
globally optimal architectures using Bayesian optimization. Experiments on
continual learning benchmarks show that our framework performs comparably with
earlier expansion based approaches and is able to flexibly compute multiple
optimal solutions with performance-size trade-offs.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:40:26 GMT""}]","2020-11-24"
"2011.10577","Luisa Lucie-Smith","Luisa Lucie-Smith, Hiranya V. Peiris, Andrew Pontzen, Brian Nord,
  Jeyan Thiyagalingam","Deep learning insights into cosmological structure formation","17 pages, 7 figures",,,,"astro-ph.CO astro-ph.IM cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While the evolution of linear initial conditions present in the early
universe into extended halos of dark matter at late times can be computed using
cosmological simulations, a theoretical understanding of this complex process
remains elusive. Here, we build a deep learning framework to learn this
non-linear relationship, and develop techniques to physically interpret the
learnt mapping. A three-dimensional convolutional neural network (CNN) is
trained to predict the mass of dark matter halos from the initial conditions.
N-body simulations follow the microphysical laws of gravity, whereas the CNN
model provides a simplified description of halo collapse where features are
extracted from the initial conditions through convolutions and combined in a
non-linear way to provide a halo mass prediction. We find no significant change
in the predictive accuracy of the model if we retrain it removing anisotropic
information from the inputs, suggesting that the features learnt by the CNN are
equivalent to spherical averages over the initial conditions. Despite including
all possible feature combinations that can be extracted by convolutions in the
model, the final halo mass predictions do not depend on anisotropic aspects of
the initial conditions. Our results indicate that deep learning frameworks can
provide a powerful tool for extracting physical insight into cosmological
structure formation.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:00 GMT""},{""version"":""v2"",""created"":""Mon, 25 Oct 2021 09:01:21 GMT""}]","2021-10-26"
"2011.10578","Douglas Boubert","Douglas Boubert, Andrew Everall, Jack Fraser, Amery Gration and Berry
  Holl","Completeness of the Gaia-verse III: using hidden states to infer gaps,
  detection efficiencies and the scanning law from the DR2 light curves","16 pages, resubmitted to MNRAS after very minor revisions. The
  Completeness of the Gaia-verse project website can be found at
  https://www.gaiaverse.space",,"10.1093/mnras/staa3791",,"astro-ph.GA astro-ph.IM astro-ph.SR","http://creativecommons.org/licenses/by/4.0/","  The completeness of the Gaia catalogues heavily depends on the status of that
space telescope through time. Stars are only published with each of the
astrometric, photometric and spectroscopic data products if they are detected a
minimum number of times. If there is a gap in scientific operations, a drop in
the detection efficiency or Gaia deviates from the commanded scanning law, then
stars will miss out on potential detections and thus be less likely to make it
into the Gaia catalogues. We lay the groundwork to retrospectively ascertain
the status of Gaia throughout the mission from the tens of individual
measurements of the billions of stars, by developing novel methodologies to
infer both the orientation and angular velocity of Gaia through time and gaps
and efficiency drops in the detections. We have applied these methodologies to
the Gaia DR2 variable star epoch photometry -- which are the only publicly
available Gaia time-series at the present time -- and make the results publicly
available. We accompany these results with a new Python package scanninglaw
(https://github.com/gaiaverse/scanninglaw) that you can use to easily predict
Gaia observation times and detection probabilities for arbitrary locations on
the sky.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:01 GMT""}]","2020-12-16"
"2011.10579","Pietro Ferrero","Pietro Ferrero, Jerome P. Gauntlett, Juan Manuel P\'erez Ipi\~na,
  Dario Martelli, James Sparks","D3-branes wrapped on a spindle","6 pages. Minor changes. Matches published version","Phys. Rev. Lett. 126, 111601 (2021)","10.1103/PhysRevLett.126.111601",,"hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We construct supersymmetric $AdS_3\times \Sigma$ solutions of minimal gauged
supergravity in $D=5$, where $\Sigma$ is a two-dimensional orbifold known as a
spindle. Remarkably, these uplift on $S^5$, or more generally on any regular
Sasaki-Einstein manifold, to smooth solutions of type IIB supergravity. The
solutions are dual to $d=2$, $\mathcal{N}=(0,2)$ SCFTs and we show that the
central charge for the gravity solution agrees with a field theory calculation
associated with D3-branes wrapped on $\Sigma$. Unlike for smooth $\Sigma$ the
superconformal R-symmetry mixes with the $U(1)$ isometry of the spindle.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:01 GMT""},{""version"":""v2"",""created"":""Wed, 3 Mar 2021 08:08:22 GMT""}]","2021-03-24"
"2011.10580","Rosie Talbot","Rosie Y. Talbot, Martin A. Bourne, Debora Sijacki","Blandford-Znajek jets in galaxy formation simulations: method and
  implementation","33 pages, 12 figures, 2 appendices, published in MNRAS","MNRAS, Volume 504, Issue 3, July 2021","10.1093/mnras/stab804",,"astro-ph.GA astro-ph.HE","http://creativecommons.org/licenses/by/4.0/","  Jets launched by active galactic nuclei (AGN) are believed to play a
significant role in shaping the properties of galaxies and provide an
energetically viable mechanism through which galaxies can become quenched. Here
we present a novel AGN feedback model, which we have incorporated into the
AREPO code, that evolves the black hole mass and spin as the accretion flow
proceeds through a thin $\alpha$-disc which we self-consistently couple to a
Blandford-Znajek jet. We apply our model to the central region of a typical
radio-loud Seyfert galaxy embedded in a hot circumgalactic medium (CGM). We
find that jets launched into high pressure environments thermalise efficiently
due to the formation of recollimation shocks and the vigorous instabilities
that these shocks excite increase the efficiency of the mixing of CGM and jet
material. The beams of more overpressured jets, however, are not as readily
disrupted by instabilities so the majority of the momentum flux at the jet base
is retained out to the head, where the jet terminates in a reverse shock. All
jets entrain a significant amount of cold circumnuclear disc material which,
while energetically insignificant, dominates the lobe mass together with the
hot, entrained CGM material. The jet power evolves significantly due to
effective self-regulation by the black hole, fed by secularly-driven,
intermittent mass flows. The direction of jets launched directly into the
circumnuclear disc changes considerably due to effective Bardeen-Petterson
torquing. Interestingly, these jets obliterate the innermost regions of the
disc and drive large-scale, multi-phase, turbulent, bipolar outflows.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:01 GMT""},{""version"":""v2"",""created"":""Thu, 29 Jul 2021 05:41:14 GMT""}]","2021-07-30"
"2011.10581","Michael Petersen","Michael S. Petersen and Jorge Pe\~narrubia","Detection of the Milky Way reflex motion due to the Large Magellanic
  Cloud infall","Author's version of the paper published 23 November 2020 in Nature
  Astronomy",,"10.1038/s41550-020-01254-3",,"astro-ph.GA","http://creativecommons.org/licenses/by/4.0/","  The Large Magellanic Cloud is the most massive satellite galaxy of the Milky
Way, with an estimated mass exceeding a tenth of the mass of the Milky Way.
Just past its closest approach of about 50 kpc, and flying by the Milky Way at
an astonishing speed of 327 km/s, the Large Magellanic Cloud can affect our
Galaxy in a number of ways, including dislodging the Milky Way disc from the
Galactic centre-of-mass. Here, we report evidence that the Milky Way disc is
moving with respect to stellar tracers in the outer halo ($40<r<120$ kpc) at
$v_{\rm travel}=32^{+4}_{-4}$ km/s, in the direction $(\ell,b)_{\rm
apex}=(56^{+9}_{-9},-34^{+10}_{-9})$ degrees, which points at an earlier
location on the LMC trajectory. The resulting reflex motion is detected in the
kinematics of outer halo stars and Milky Way satellite galaxies with accurate
distances, proper motions and line-of-sight velocities. Our results indicate
that dynamical models of our Galaxy cannot neglect gravitational perturbations
induced by the Large Magellanic Cloud infall, nor can observations of the
stellar halo be treated in a reference frame that does not correct for disc
reflex motion. Future spectroscopic surveys of the stellar halo combined with
Gaia astrometry will allow for sophisticated modelling of the Large Magellanic
Cloud trajectory across the Milky Way, constraining the dark matter
distribution in both galaxies with unprecedented detail.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:02 GMT""}]","2020-11-24"
"2011.10582","Robert Simcoe","Robert A. Simcoe, Masafusa Onoue, Anna-Christina Eilers, Eduardo
  Banados, Thomas J. Cooper, Gabor Furesz, Joseph F. Hennawi, and Bram Venemans","Interstellar and Circumgalactic Properties of an Unseen $z=6.84$ Galaxy:
  Abundances, Ionization, and Heating in the Earliest Known Quasar Absorber","(21 pages, 8 figures)",,,,"astro-ph.GA astro-ph.CO","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We analyze relative abundances and ionization conditions in a strong
absorption system at z=6.84, seen in the spectrum of the z=7.54 background
quasar ULAS J134208.10+092838.61. Singly ionized C, Si, Fe, Mg, and Al
measurements are consistent with a warm neutral medium that is metal-poor but
not chemically pristine. Firm non-detections of C IV and Si IV imply that any
warm ionized phase of the IGM or CGM has not yet been enriched past the
ultra-metal-poor regime (<0.001Z_{solar}), unlike lower redshift DLAs where
these lines are nearly ubiquitous. Relative abundances of the heavy elements
794 Myr after the Big Bang resemble those of metal-poor damped Lyman Alpha
systems at intermediate redshift and Milky Way halo stars, and show no evidence
of enhanced [alpha/Fe], [C/Fe] or other signatures of yields dominated by
massive stars. A detection of the CII* fine structure line reveals local
sources of excitation from heating, beyond the level of photo-excitation
supplied by the CMB. We estimate the total and [CII] cooling rates, balancing
against ISM heating sources to develop an heuristic two-phase model of the
neutral medium. The implied heating requires a surface density of star
formation slightly exceeding that of the Milky Way but not at the level of a
strong starburst. For a typical (assumed) NHI=10^{20.6}, an abundance of
[Fe/H]=-2.2 matches the columns of species in the neutral phase. To remain
undetected in C IV, a warm ionized phase would either need much lower
[C/H]<-4.2 over an absorption path of 1 kpc, or else a very small absorption
path (a few pc). While still speculative, these results suggest a significant
reduction in heavy element enrichment outside of neutral star forming regions
of the ISM, as would be expected in early stages of galactic chemical
evolution.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:04 GMT""}]","2020-11-24"
"2011.10583","Federica Maria Surace","Federica Maria Surace and Alessio Lerose","Scattering of mesons in quantum simulators","24 pages, 6 figures, v4: minor text rephrasing","New J. Phys. 23 (2021) 062001","10.1088/1367-2630/abfc40",,"cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el hep-lat quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Simulating real-time evolution in theories of fundamental interactions
represents one of the central challenges in contemporary theoretical physics.
Cold-atom platforms stand as promising candidates to realize quantum
simulations of non-perturbative phenomena in gauge theories, such as vacuum
decay and hadron collisions, in prohibitive conditions for direct experiments.
In this work, we demonstrate that present-day quantum simulators can imitate
linear particle accelerators, giving access to S-matrix measurements of elastic
and inelastic meson collisions in low-dimensional Abelian gauge theories.
Considering for definiteness a $(1+1)$-dimensional $\mathbb{Z}_2$-lattice gauge
theory realizable with Rydberg-atom arrays, we present protocols to observe and
measure selected meson-meson scattering processes. We provide a benchmark
theoretical study of scattering amplitudes in the regime of large fermion mass,
including an exact solution valid for arbitrary coupling strength. This allows
us to discuss the occurrence of inelastic scattering channels, featuring the
production of new mesons with different internal structures. We present
numerical simulations of realistic wavepacket collisions, which reproduce the
predicted cross section peaks. This work highlights the potential of quantum
simulations to give unprecedented access to real-time scattering dynamics.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:04 GMT""},{""version"":""v2"",""created"":""Sat, 12 Dec 2020 17:43:43 GMT""},{""version"":""v3"",""created"":""Thu, 7 Jan 2021 15:07:48 GMT""},{""version"":""v4"",""created"":""Thu, 10 Jun 2021 10:26:48 GMT""}]","2021-06-11"
"2011.10584","Paola Santini","P. Santini, M. Castellano, E. Merlin, A. Fontana, F. Fortuni, D.
  Kodra, B. Magnelli, N. Menci, A. Calabr\`o, C. C. Lovell, L. Pentericci, V.
  Testa, S. M. Wilkins","The emergence of passive galaxies in the early Universe","A&A in press, version updated to match the accepted version","A&A 652, A30 (2021)","10.1051/0004-6361/202039738",,"astro-ph.GA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The emergence of passive galaxies in the early Universe results from the
interplay among the processes responsible for their rapid assembly and for the
abrupt shut-down of their SF. Investigating the individual properties and
demographics of early passive galaxies will improve our understanding of these
mechanisms. In this work we present a follow-up analysis of the z>3 passive
galaxy candidates selected by Merlin et al. (2019) in the CANDELS fields. We
begin by first confirming the accuracy of their passive classification by
exploiting their sub-mm emission to demonstrate the lack of ongoing SF. Using
archival ALMA observations we are able to confirm at least 61% of the observed
candidates as passive. While the remainder lack sufficiently deep data for
confirmation, we are able to validate the entire sample in a statistical sense.
We then estimate the Stellar Mass Function (SMF) of all 101 passive candidates
in three redshift bins from z=5 to z=3. We adopt a stepwise approach that has
the advantage of taking into account photometric errors, observational
incompleteness, and the Eddington bias without any a-posteriori correction. We
observe a pronounced evolution in the SMF around z~4, indicating that we are
witnessing the emergence of the passive population at this epoch. Massive
(M>10^11Msun) passive galaxies, only accounting for a small (<10%) fraction of
galaxies at z>4, become dominant at later epochs. Thanks to a combination of
photometric quality, sample selection and methodology, we overall find a higher
density of passive galaxies than previous works. The comparison with
theoretical predictions, despite a qualitative agreement, denotes a still
incomplete understanding of the physical processes responsible for the
formation of these galaxies. Finally, we extrapolate our results to predict the
number of early passive galaxies expected in surveys carried out with future
facilities.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:05 GMT""},{""version"":""v2"",""created"":""Mon, 19 Apr 2021 19:35:16 GMT""},{""version"":""v3"",""created"":""Tue, 11 May 2021 13:11:48 GMT""}]","2021-08-04"
"2011.10585","Antonios Alvertis","Timothy J.H. Hele, Bartomeu Monserrat and Antonios M. Alvertis","Systematic improvement of molecular excited state calculations by
  inclusion of nuclear quantum motion: a mode-resolved picture and the effect
  of molecular size",,"The Journal of Chemical Physics 154, 244109 (2021)","10.1063/5.0052247",,"physics.chem-ph cond-mat.mtrl-sci physics.comp-ph","http://creativecommons.org/licenses/by/4.0/","  The energies of molecular excited states arise as solutions to the electronic
Schr\""{o}dinger equation and are often compared to experiment. At the same
time, nuclear quantum motion is known to be important and to induce a red-shift
of excited state energies. However, it is thus far unclear whether
incorporating nuclear quantum motion in molecular excited state calculations
leads to a systematic improvement of their predictive accuracy, making further
investigation necessary. Here we present such an investigation by employing two
first-principles methods for capturing the effect of quantum fluctuations on
excited state energies, which we apply to the Thiel set of organic molecules.
We show that accounting for zero-point motion leads to much improved agreement
with experiment, compared to `static' calculations which only account for
electronic effects, and the magnitude of the red-shift can become as large as
1.36 eV. Moreover, we show that the effect of nuclear quantum motion on excited
state energies largely depends on the molecular size, with smaller molecules
exhibiting larger red-shifts. Our methodology also makes it possible to analyze
the contribution of individual vibrational normal modes to the red-shift of
excited state energies, and in several molecules we identify a limited number
of modes dominating this effect. Overall, our study provides a foundation for
systematically quantifying the shift of excited state energies due to nuclear
quantum motion, and for understanding this effect at a microscopic level.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:08 GMT""},{""version"":""v2"",""created"":""Wed, 31 Mar 2021 18:00:05 GMT""},{""version"":""v3"",""created"":""Fri, 25 Jun 2021 11:41:21 GMT""}]","2021-06-28"
"2011.10586","Anish Ghoshal","Marco Frasca and Anish Ghoshal","Mass Gap in Infinite Derivative Non-local Higgs: Dyson-Schwinger
  Approach","Some minor texts revised. Version matches the one accepted for
  publication in Classical and Quantum Gravity journal","Class.Quant.Grav. 38 (2021) 17, 175013","10.1088/1361-6382/ac161b",,"hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigate the non-perturbative degrees of freedom in the class of
non-local Higgs theories that have been proposed as an ultraviolet completion
4-D Quantum Field Theory (QFT) generalizing the kinetic energy operators to an
infinite series of higher derivatives inspired by string field theory. At the
perturbative level, the degrees of freedom of non-local Higgs are the same of
the local theory. We prove that, at the non-perturbative level, the physical
spectrum of the Higgs mass is actually corrected from the ""infinite number of
derivatives"" present in the action. The technique we use permits to derive the
set of Dyson-Schwinger equations in differential form. This proves essentially
useful when exact solutions to the local equations are known. We show that all
the formalism of the local theory involving the Dyson-Schwinger approach
extends quite naturally to the non-local case. Using these methods, the
spectrum of non-local theories become accessible and predictable in the
non-perturbative regimes. We calculate the N-point correlation functions and
predict the mass-gap in the spectrum arising purely from the self-interaction
and the non-local scale M. The mass gap generated gets damped in the UV and it
reaches conformal limit. We discuss some implications of our result in particle
physics and cosmology.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:09 GMT""},{""version"":""v2"",""created"":""Thu, 10 Dec 2020 03:32:16 GMT""},{""version"":""v3"",""created"":""Thu, 15 Jul 2021 20:09:47 GMT""}]","2021-09-15"
"2011.10587","Dilovan Serindag","Dilovan B. Serindag, Stevanus K. Nugroho, Paul Molli\`ere, Ernst J. W.
  de Mooij, Neale P. Gibson, Ignas A. G. Snellen","Is TiO emission present in the ultra-hot Jupiter WASP-33b? A
  reassessment using the improved ExoMol Toto line list","17 pages, 15 figures, Accepted for publication in A&A","A&A 645, A90 (2021)","10.1051/0004-6361/202039135",,"astro-ph.EP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  [abridged] Efficient absorption of stellar UV and visible radiation by TiO
and VO is predicted to drive temperature inversions in the upper atmospheres of
hot Jupiters. However, few inversions or detections of TiO or VO have been
reported, and results are often contradictory. Using the improved ExoMol Toto
line list, we searched for TiO emission in the dayside spectrum of WASP-33b
using the same data in which the molecule was previously detected with an older
line list at 4.8$\sigma$. We intended to confirm the molecular detection and
quantify the signal improvement offered by the ExoMol Toto line list. Data from
the High Dispersion Spectrograph on the Subaru Telescope was extracted and
reduced in an identical manner to the previous study. Stellar and telluric
contamination were then removed. High-resolution TiO emission models of
WASP-33b were created using the radiative transfer code petitRADTRANS and
cross-correlated with the data. We measure a 4.3$\sigma$ TiO emission signature
using the ExoMol Toto models, corresponding to a WASP-33b orbital velocity
semi-amplitude of $K_\mathrm{p}=252.9^{+5.0}_{-5.3}\ \mathrm{km\ s^{-1}}$ and a
system velocity of $v_\mathrm{sys}=-23.0^{+4.7}_{-4.6}\ \mathrm{km\ s^{-1}}$.
Injection-recovery tests using models based on the new and earlier line lists
indicate that if the new models provide a perfect match to the planet spectrum,
the significance of the TiO detection should have increased by a factor of
$\sim$2. Although the TiO signal we find is statistically significant,
comparison with previous works makes our result too ambiguous to claim a
clear-cut detection. Unexpectedly, the new ExoMol Toto models provide a weaker
signal than that found previously, which is offset in
$K_\mathrm{p}$-$v_\mathrm{sys}$ space. This sheds some doubt on both
detections, especially in light of a recently published TiO non-detection using
a different dataset.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:13 GMT""}]","2021-01-20"
"2011.10588","Andrea Pastorello","A. Pastorello, M. Fraser, G. Valerin, A. Reguitti, K. Itagaki, P.
  Ochner, S. C. Williams, D. Jones, J. Munday, S. J. Smartt, K. W. Smith, S.
  Srivastav, N. Elias-Rosa, E. Kankare, E. Karamehmetoglu, P. Lundqvist, P. A.
  Mazzali, U. Munari, M. D. Stritzinger, L. Tomasella, J. P. Anderson, K. C.
  Chambers, A. Rest","Luminous Red Nova AT 2019zhd, a new merger in M 31","17 pages, 10 figures, 3 tables. Accepted for publication in A&A","A&A 646, A119 (2021)","10.1051/0004-6361/202039952",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present the follow-up campaign of the luminous red nova (LRN) AT~2019zhd,
the third event of this class observed in M 31. The object was followed by
several sky surveys for about five months before the outburst, during which it
showed a slow luminosity rise. In this phase, the absolute magnitude ranged
from M_r=-2.8+-0.2 mag to M_r=-5.6+-0.1 mag. Then, over a four-five day period,
AT 2019zhd experienced a major brightening, reaching at peak M_r=-9.61+-0.08
mag, and an optical luminosity of 1.4x10^39 erg/s. After a fast decline, the
light curve settled onto a short-duration plateau in the red bands. Although
less pronounced, this feature is reminiscent of the second red maximum observed
in other LRNe. This phase was followed by a rapid linear decline in all bands.
At maximum, the spectra show a blue continuum with prominent Balmer emission
lines. The post-maximum spectra show a much redder continuum, resembling that
of an intermediate-type star. In this phase, Halpha becomes very weak, Hbeta is
no longer detectable and a forest of narrow absorption metal lines now dominate
the spectrum. The latest spectra, obtained during the post-plateau decline,
show a very red continuum (T_eff ~ 3000 K) with broad molecular bands of TiO,
similar to those of M-type stars. The long-lasting, slow photometric rise
observed before the peak resembles that of LRN V1309 Sco, which was interpreted
as the signature of the common-envelope ejection. The subsequent outburst is
likely due to the gas outflow following a stellar merging event. The inspection
of archival HST images taken 22 years before the LRN discovery reveals a faint
red source (M_F555W=0.21+-0.14 mag, with F555W-F814W = 2.96+-0.12 mag) at the
position of AT 2019zhd, which is the most likely quiescent precursor. The
source is consistent with expectations for a binary system including a
predominant M5-type star.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:14 GMT""},{""version"":""v2"",""created"":""Fri, 18 Dec 2020 09:29:13 GMT""}]","2021-02-17"
"2011.10589","Tony Pourmohamad","Tony Pourmohamad","CompModels: A suite of computer model test functions for Bayesian
  optimization",,,,,"stat.CO stat.OT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The CompModels package for R provides a suite of computer model test
functions that can be used for computer model prediction/emulation, uncertainty
quantification, and calibration, but in particular, the sequential optimization
of computer models. The package is a mix of real-world physics problems, known
mathematical functions, and black-box functions that have been converted into
computer models with the goal of Bayesian (i.e., sequential) optimization in
mind. Likewise, the package contains computer models that represent either the
constrained or unconstrained optimization case, each with varying levels of
difficulty. In this paper, we illustrate the use of the package with both
real-world examples and black-box functions by solving constrained optimization
problems via Bayesian optimization. Ultimately, the package is shown to provide
users with a source of computer model test functions that are reproducible,
shareable, and that can be used for benchmarking of novel optimization methods.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:30 GMT""},{""version"":""v2"",""created"":""Thu, 7 Jan 2021 16:16:54 GMT""}]","2021-01-08"
"2011.10590","Andrea Pastorello","A. Pastorello, G. Valerin, M. Fraser, N. Elias-Rosa, S. Valenti, A.
  Reguitti, P. A. Mazzali, R. C. Amaro, J. E. Andrews, Y. Dong, J. Jencson, M.
  Lundquist, D. E. Reichart, D. J. Sand, S. Wyatt, S. J. Smartt, K. W. Smith,
  S. Srivastav, Y.-Z. Cai, E. Cappellaro, S. Holmbo, A. Fiore, D. Jones, E.
  Kankare, E. Karamehmetoglu, P. Lundqvist, A. Morales-Garoffolo, T. M.
  Reynolds, M. D. Stritzinger, S. C. Williams, K. C. Chambers, T. J. L. de
  Boer, M. E. Huber, A. Rest, R. Wainscoat","The luminous red nova variety: AT 2020hat and AT 2020kog","14 pages, 9 figures, 2 tables. Accepted for publication in A&A","A&A 647, A93 (2021)","10.1051/0004-6361/202039953",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present the results of our monitoring campaigns of the luminous red novae
(LRNe) AT 2020hat in NGC 5068 and AT 2020kog in NGC 6106. The two objects were
imaged (and detected) before their discovery by routine survey operations. They
show a general trend of slow luminosity rise, lasting at least a few months.
The subsequent major LRN outbursts were extensively followed in photometry and
spectroscopy. The light curves present an initial short-duration peak, followed
by a redder plateau phase. AT 2020kog is a moderately luminous event peaking at
~7 x 10^40 erg/s, while AT 2020hat is almost one order of magnitude fainter
than AT 2020kog, although it is still more luminous than V838 Mon. In analogy
with other LRNe, the spectra of AT 2020kog change significantly with time. They
resemble those of type IIn supernovae at early phases, then they become similar
to those of K-type stars during the plateau, and to M-type stars at very late
phases. In contrast, AT 2020hat already shows a redder continuum at early
epochs, and its spectrum shows the late appearance of molecular bands. A
moderate-resolution spectrum of AT 2020hat taken at +37 d after maximum shows a
forest of narrow P Cygni lines of metals with velocities of 180 km/s, along
with an Halpha emission with a full-width at half-maximum velocity of 250 km/s.
For AT 2020hat, a robust constraint on its quiescent progenitor is provided by
archival images of the Hubble Space Telescope. The progenitor is clearly
detected as a mid-K type star, with an absolute magnitude of MF606W =
-3.33+-0.09 mag and a colour of F606W-F814W = 1.14+-0.05 mag, which are
inconsistent with the expectations from a massive star that could later produce
a core-collapse supernova. Although quite peculiar, the two objects nicely
match the progenitor versus light curve absolute magnitude correlations
discussed in the literature.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:35 GMT""},{""version"":""v2"",""created"":""Thu, 14 Jan 2021 18:11:45 GMT""}]","2021-03-17"
"2011.10591","Lukas Knips","Lukas Knips","A Moment for Random Measurements","8 pages, 7 figures. Written as Perspective for Quantum Journal on
  Ketterer et al., Quantum 4, 325 (2020), arXiv:2004.08402","Quantum Views 4, 47 (2020)","10.22331/qv-2020-11-19-47",,"quant-ph","http://creativecommons.org/licenses/by/4.0/","  Quantum entanglement is one of the core features of quantum theory. While it
is typically revealed by measurements along carefully chosen directions, here
we review different methods based on so-called random or randomized
measurements. Although this approach might seem inefficient at first, sampling
correlations in various random directions is a powerful tool to study
properties which are invariant under local-unitary transformations. Based on
random measurements, entanglement can be detected and characterized without a
shared reference frame between the observers or even if local reference frames
cannot be defined. This overview article discusses different methods using
random measurements to detect genuine multipartite entanglement and to
distinguish SLOCC classes. Furthermore, it reviews how measurement directions
can efficiently be obtained based on spherical designs.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:00:47 GMT""}]","2020-11-24"
"2011.10592","Avishek Das","Avishek Das, David T. Limmer","Variational design principles for nonequilibrium colloidal assembly","13 pages, 8 figures","J. Chem. Phys. 154, 014107 (2021)","10.1063/5.0038652",,"cond-mat.soft cond-mat.stat-mech physics.chem-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Using large deviation theory and principles of stochastic optimal control, we
show that rare molecular dynamics trajectories conditioned on assembling a
specific target structure encode a set of interactions and external forces that
lead to enhanced stability of that structure. Such a relationship can be
formulated into a variational principle, for which we have developed an
associated optimization algorithm and have used it to determine optimal forces
for targeted self-assembly within nonequilibrium steady-states. We illustrate
this perspective on inverse design in a model of colloidal cluster assembly
within linear shear flow. We find that colloidal clusters can be assembled with
high yield using specific short-range interactions of tunable complexity. Shear
decreases the yields of rigid clusters, while small values of shear increase
the yields of nonrigid clusters. The enhancement or suppression of the yield
due to shear is rationalized with a generalized linear response theory. By
studying 21 unique clusters made of 6, 7 or 8 particles, we uncover basic
design principles for targeted assembly out of equilibrium.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:01:23 GMT""},{""version"":""v2"",""created"":""Tue, 12 Jan 2021 22:41:10 GMT""}]","2021-01-14"
"2011.10593","Ahmad Lalti","Ahmad Lalti, Yuri Khotyaintsev, Daniel B. Graham, Andris Vaivads,
  Konrad Steinvall and Christopher T. Russell","Whistler Waves in the foot of Quasi-Perpendicular Super-Critical Shocks",,,"10.1029/2021JA029969",,"physics.space-ph","http://creativecommons.org/licenses/by/4.0/","  Whistler waves are thought to play an essential role in the dynamics of
collisionless shocks. We use the magnetospheric multiscale (MMS) spacecraft to
study whistler waves around the lower hybrid frequency, upstream of 11
quasi-perpendicular super-critical shocks. We apply the 4-spacecraft timing
method to unambiguously determine the wave vector $\mathbf{k}$ of whistler
waves. We find that the waves are oblique to the background magnetic field with
a wave-normal angle between $20^{\circ}$ and $42^{\circ}$, a wavelength around
100 km which is close to the ion inertial length. We also find that
$\mathbf{k}$ is predominantly in the same plane as the magnetic field and the
normal to the shock. By combining this precise knowledge of $\mathbf{k}$ with
high-resolution measurements of the 3D ion velocity distribution we show that a
reflected ion beam is in resonance with the waves, opening up the possibility
for wave-particle interaction between the reflected ions and the observed
whistlers. The linear stability analysis of a system mimicking the observed
distribution, suggests that such a system can produce the observed waves.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:02:21 GMT""},{""version"":""v2"",""created"":""Wed, 2 Dec 2020 12:08:53 GMT""},{""version"":""v3"",""created"":""Fri, 2 Jul 2021 14:02:56 GMT""},{""version"":""v4"",""created"":""Wed, 17 Nov 2021 08:46:22 GMT""}]","2022-05-27"
"2011.10594","Aoife Boyle","Aoife Boyle and Fabian Schmidt","Neutrino mass constraints beyond linear order: cosmology dependence and
  systematic biases","Prepared for submission to JCAP","JCAP 04 (2021) 022","10.1088/1475-7516/2021/04/022",,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We demonstrate the impact on forecasted neutrino mass constraints of
extending galaxy clustering and CMB lensing predictions from linear to
next-to-leading-order power spectra. The redshift-space 1-loop power spectrum
model we adopt requires an additional four free bias parameters, a velocity
bias parameter and two new stochastic parameters. These additional nuisance
parameters appreciably weaken the constraints on $M_\nu$. CMB lensing plays a
significant role in helping to alleviate these degeneracies and tighten the
final constraints. The constraint on the optical depth to reionisation $\tau$
has a strong effect on the constraint on $M_\nu$, but only when CMB lensing is
included in the analysis to keep the degeneracies with the nuisance parameters
under control. We also extract constraints when 1) using the BAO signature only
as a distance probe, and 2) isolating the scale-dependence of the power
spectrum, which, as shown in previous work, provides a cosmology-independent
probe of $M_\nu$. All constraints except the latter remain strongly sensitive
to the assumption of a flat $\Lambda$CDM universe. We perform an analysis of
the magnitude of the shift introduced in the inferred $M_\nu$ value when
neglecting nonlinear corrections, and show that, for a Euclid-like survey, this
shift becomes roughly equal to the 1$\sigma$ constraint itself even with a
conservative cut-off scale of $k_{max} = 0.1~h~{\rm Mpc}^{-1}$. We also perform
a calculation of the appropriate expected bias in neutrino mass caused by not
including the next, 2-loop order and expect a shift of only about 20% of the
1$\sigma$ error for $k_{max}=0.2~h~{\rm Mpc}^{-1}$ in a Euclid-like survey.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:02:27 GMT""},{""version"":""v2"",""created"":""Thu, 26 Nov 2020 18:57:55 GMT""},{""version"":""v3"",""created"":""Thu, 3 Jun 2021 11:22:01 GMT""}]","2021-06-04"
"2011.10595","Mattia Sensi","Sara Sottile, Ozan Kahramano\u{g}ullar{\i}, Mattia Sensi","How network properties and epidemic parameters influence stochastic SIR
  dynamics on scale-free random networks","22 pages, 9 figures",,,,"physics.soc-ph math.DS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the premise that social interactions are described by power-law
distributions, we study a SIR stochastic dynamic on a static scale-free random
network generated via configuration model. We verify our model with respect to
deterministic considerations and provide a theoretical result on the
probability of the extinction of the disease. Based on this calibration, we
explore the variability in disease spread by stochastic simulations. In
particular, we demonstrate how important epidemic indices change as a function
of the contagiousness of the disease and the connectivity of the network. Our
results quantify the role of starting node degree in determining these indices,
commonly used to describe epidemic spread.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:08:55 GMT""}]","2020-11-24"
"2011.10596","Armin Lederer","Armin Lederer, Alexandre Capone, Thomas Beckers, Jonas Umlauft, Sandra
  Hirche","The Impact of Data on the Stability of Learning-Based Control- Extended
  Version",,,,,"eess.SY cs.LG cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite the existence of formal guarantees for learning-based control
approaches, the relationship between data and control performance is still
poorly understood. In this paper, we propose a Lyapunov-based measure for
quantifying the impact of data on the certifiable control performance. By
modeling unknown system dynamics through Gaussian processes, we can determine
the interrelation between model uncertainty and satisfaction of stability
conditions. This allows us to directly asses the impact of data on the provable
stationary control performance, and thereby the value of the data for the
closed-loop system performance. Our approach is applicable to a wide variety of
unknown nonlinear systems that are to be controlled by a generic learning-based
control law, and the results obtained in numerical simulations indicate the
efficacy of the proposed measure.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:10:01 GMT""},{""version"":""v2"",""created"":""Fri, 30 Jul 2021 13:25:43 GMT""}]","2021-08-02"
"2011.10597","Ying-Cheng Lai","Huawei Fan, Ling-Wei Kong, Xingang Wang, Alan Hastings, and Ying-Cheng
  Lai","Synchronization within synchronization: transients and intermittency in
  ecological networks","17 pages, 7 figures",,,,"q-bio.PE math.DS nlin.CD","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transients are fundamental to ecological systems with significant
implications to management, conservation, and biological control. We uncover a
type of transient synchronization behavior in spatial ecological networks whose
local dynamics are of the chaotic, predator-prey type. In the parameter regime
where there is phase synchronization among all the patches, complete
synchronization (i.e., synchronization in both phase and amplitude) can arise
in certain pairs of patches as determined by the network symmetry - henceforth
the phenomenon of ""synchronization within synchronization."" Distinct patterns
of complete synchronization coexist but, due to intrinsic instability or noise,
each pattern is a transient and there is random, intermittent switching among
the patterns in the course of time evolution. The probability distribution of
the transient time is found to follow an algebraic scaling law with a divergent
average transient lifetime. Based on symmetry considerations, we develop a
stability analysis to understand these phenomena. The general principle of
symmetry can also be exploited to explain previously discovered,
counterintuitive synchronization behaviors in ecological networks.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:13:08 GMT""}]","2020-11-24"
"2011.10598","L\'eo Mathis","Antonio Lerario and L\'eo Mathis","On tameness of zonoids",,,,,"math.MG math.AG","http://creativecommons.org/licenses/by/4.0/","  We prove that in a globally subanalytic family of convex bodies the set of
zonoids is log-analytic, and in particular it is definable in the o-minimal
structure generated by globally subanalytic sets and the graph of the
exponential function.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:14:57 GMT""},{""version"":""v2"",""created"":""Mon, 25 Jan 2021 16:55:00 GMT""}]","2021-01-26"
"2011.10599","Zhaozhong Shi","Zhaozhong Shi (on Behalf of CMS Collaboration)","Measurements of Nuclear Modification Factors of $B^{0}_{s}$ and $B^+$
  Mesons in PbPb Collisions with the CMS Experiment","10th International Conference on Hard and Electromagnetic Probes of
  High-Energy Nuclear Collisions (Hard Probes 2020)",,,,"nucl-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Beauty quarks are considered as one of the best probes of the strongly
interacting medium created in relativistic heavy-ion collisions because they
are predominantly produced via initial hard scatterings. Measurements of B
meson production provide information about the diffusion of beauty quarks and
the flavor dependence of in-medium energy loss. In these studies, clarifying
the hadronization mechanism is crucial for understanding the transport
properties of beauty quarks. Measurements of $B^{0}_{s}$ production can shed
light on the mechanisms of beauty recombination in the medium and provide
information about strangeness enhancement in the quark-gluon plasma. In this
talk, we will present a new measurement of the ratio of $B^{0}_{s}$ to $B^+$
mesons in PbPb collisions at 5.02 TeV with the CMS detector, using data
recorded in 2018.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:19:21 GMT""}]","2020-11-24"
"2011.10600","Yasser Dahou","Yasser Dahou, Marouane Tliba, Kevin McGuinness, Noel O'Connor","ATSal: An Attention Based Architecture for Saliency Prediction in 360
  Videos",,,,,"cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  The spherical domain representation of 360 video/image presents many
challenges related to the storage, processing, transmission and rendering of
omnidirectional videos (ODV). Models of human visual attention can be used so
that only a single viewport is rendered at a time, which is important when
developing systems that allow users to explore ODV with head mounted displays
(HMD). Accordingly, researchers have proposed various saliency models for 360
video/images. This paper proposes ATSal, a novel attention based (head-eye)
saliency model for 360\degree videos. The attention mechanism explicitly
encodes global static visual attention allowing expert models to focus on
learning the saliency on local patches throughout consecutive frames. We
compare the proposed approach to other state-of-the-art saliency models on two
datasets: Salient360! and VR-EyeTracking. Experimental results on over 80 ODV
videos (75K+ frames) show that the proposed method outperforms the existing
state-of-the-art.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:19:48 GMT""}]","2020-11-24"
"2011.10601","Lukas Sieberer","Sharareh Sayyad, Jinlong Yu, Adolfo G. Grushin, and Lukas M. Sieberer","Entanglement Spectrum Crossings Reveal non-Hermitian Dynamical Topology","28 pages, 14 figures","Phys. Rev. Research 3, 033022 (2021)","10.1103/PhysRevResearch.3.033022",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The development of non-Hermitian topological band theory has led to
observations of novel topological phenomena in effectively classical, driven
and dissipative systems. However, for open quantum many-body systems, the
absence of a ground state presents a challenge to define robust signatures of
non-Hermitian topology. We show that such a signature is provided by crossings
in the time evolution of the entanglement spectrum. These crossings occur in
quenches from the trivial to the topological phase of a driven-dissipative
Kitaev chain that is described by a Markovian quantum master equation in
Lindblad form. At the topological transition, which can be crossed either by
changing parameters of the Hamiltonian of the system or by increasing the
strength of dissipation, the time scale at which the first entanglement
spectrum crossing occurs diverges with a dynamical critical exponent of
$\epsilon = 1/2$. We corroborate these numerical findings with an exact
analytical solution of the quench dynamics for a spectrally flat postquench
Liouvillian. This exact solution suggests an interpretation of the topological
quench dynamics as a fermion parity pump. Our work thus reveals signatures of
non-Hermitian topology which are unique to quantum many-body systems and cannot
be emulated in classical simulators of non-Hermitian wave physics.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:22:57 GMT""}]","2021-07-14"
"2011.10602","Thembelihle Dlamini","Thembelihle Dlamini, Sifiso Vilakati","LSTM-based Traffic Load Balancing and Resource Allocation for an Edge
  System","8 Figures, 13 pages","Wireless Communications and Mobile Computing, 2020",,,"cs.NI cs.SY eess.SY","http://creativecommons.org/licenses/by/4.0/","  The massive deployment of small cell Base Stations (SBSs) empowered with
computing capabilities presents one of the most ingenious solutions adopted for
5G cellular networks towards meeting the foreseen data explosion and the
ultra-low latency demanded by mobile applications. This empowerment of SBSs
with Multi-access Edge Computing (MEC) has emerged as a tentative solution to
overcome the latency demands and bandwidth consumption required by mobile
applications at the network edge. The MEC paradigm offers a limited amount of
resources to support computation, thus mandating the use of intelligence
mechanisms for resource allocation. The use of green energy for powering the
network apparatuses (e.g., Base Stations (BSs), MEC servers) has attracted
attention towards minimizing the carbon footprint and network operational
costs. However, due to their high intermittency and unpredictability, the
adoption of learning methods is a requisite. Towards intelligent edge system
management, this paper proposes a Green-based Edge Network Management (GENM)
algorithm, which is a online edge system management algorithm for enabling
green-based load balancing in BSs and energy savings within the MEC server. The
main goal is to minimize the overall energy consumption and guarantee the
Quality of Service (QoS) within the network. To achieve this, the GENM
algorithm performs dynamic management of BSs, autoscaling and reconfiguration
of the computing resources, and on/off switching of the fast tunable laser
drivers coupled with location-aware traffic scheduling in the MEC server. The
obtained simulation results validate our analysis and demonstrate the superior
performance of GENM compared to a benchmark algorithm.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:27:48 GMT""}]","2020-11-24"
"2011.10603","T. Kallman","T. Kallman, M. Bautista, J. Deprince, J. A. Garcia, C. Mendoza, A.
  Ogorzalek, P. Palmeri, P. Quinet","Photoionization Models for High Density Gas",,,"10.3847/1538-4357/abccd6",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Relativistically broadened and redshifted 6.4 -- 6.9 keV iron K lines are
observed from many accretion powered objects, including X-ray binaries and
active galactic nuclei (AGN). Existence of gas close to the central engine
implies large radiation intensities and correspondingly large gas densities if
the gas is to remain partially ionized. Simple estimates indicate that high gas
densities are needed to allow survival of iron against ionization. These are
high enough that rates for many atomic processes are affected by mechanisms
related to interactions with nearby ions and electrons. Radiation intensities
are high enough that stimulated processes can be important. Most models
currently in use for interpreting relativistic lines use atomic rate
coefficients designed for use at low densities and neglect stimulated
processes. In our work so far we have presented atomic structure calculations
with the goal of providing physically appropriate models at densities
consistent with line-emitting gas near compact objects. In this paper we apply
these rates to photoionization calculations, and produce ionization balance
curves and X-ray emissivities and opacities which are appropriate for high
densities and high radiation intensities. The final step in our program will be
presented in a subsequent paper: Model atmosphere calculations which
incorporate these rates into synthetic spectra.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:32:51 GMT""},{""version"":""v2"",""created"":""Wed, 25 Nov 2020 18:25:20 GMT""}]","2021-02-24"
"2011.10604","Zheng Gong","Zheng Gong, Yinren Shou, Yuhui Tang, Xueqing Yan","Energetic spin-polarized proton beams from two-stage coherent
  acceleration in laser-driven plasma",,"Phys. Rev. E 102, 053212 (2020)","10.1103/PhysRevE.102.053212",,"physics.plasm-ph physics.acc-ph physics.app-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose a scheme to overcome the great challenge of polarization loss in
spin-polarized ion acceleration. When a petawatt laser pulse penetrates through
a compound plasma target consisting of a double layer slab and prepolarized
hydrogen halide gas, a strong forward moving quasistatic longitudinal electric
field is constructed by the self-generated laser-driven plasma. This field with
a varying drift velocity efficiently boosts the prepolarized protons via a
two-stage coherent acceleration process. Its merit is not only achieving a
highly energetic beam but also eliminating the undesired polarization loss of
the accelerated protons. We study the proton dynamics via Hamiltonian analyses,
specifically deriving the threshold of triggering the two-stage coherent
acceleration. To confirm the theoretical predictions, we perform
three-dimensional PIC simulations, where unprecedented proton beams with energy
approximating half GeV and polarization ratio $\sim$ 94\% are obtained.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:36:10 GMT""}]","2020-11-24"
"2011.10605","Junhyeok Ahn","Junhyeok Ahn and Luis Sentis","Nested Mixture of Experts: Cooperative and Competitive Learning of
  Hybrid Dynamical System","Accepted to 2021 L4DC",,,,"cs.LG cs.RO","http://creativecommons.org/licenses/by/4.0/","  Model-based reinforcement learning (MBRL) algorithms can attain significant
sample efficiency but require an appropriate network structure to represent
system dynamics. Current approaches include white-box modeling using analytic
parameterizations and black-box modeling using deep neural networks. However,
both can suffer from a bias-variance trade-off in the learning process, and
neither provides a structured method for injecting domain knowledge into the
network. As an alternative, gray-box modeling leverages prior knowledge in
neural network training but only for simple systems. In this paper, we devise a
nested mixture of experts (NMOE) for representing and learning hybrid dynamical
systems. An NMOE combines both white-box and black-box models while optimizing
bias-variance trade-off. Moreover, an NMOE provides a structured method for
incorporating various types of prior knowledge by training the associative
experts cooperatively or competitively. The prior knowledge includes
information on robots' physical contacts with the environments as well as their
kinematic and dynamic properties. In this paper, we demonstrate how to
incorporate prior knowledge into our NMOE in various continuous control
domains, including hybrid dynamical systems. We also show the effectiveness of
our method in terms of data-efficiency, generalization to unseen data, and
bias-variance trade-off. Finally, we evaluate our NMOE using an MBRL setup,
where the model is integrated with a model-based controller and trained online.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:36:45 GMT""},{""version"":""v2"",""created"":""Thu, 29 Apr 2021 06:08:56 GMT""}]","2021-04-30"
"2011.10606","Zhu Yi","Zhu Yi, Qing Gao, Yungui Gong and Zong-hong Zhu","Primordial black holes and secondary gravitational waves from
  inflationary model with a non-canonical kinetic term","40 pages, 7figures, references added, typos corrected, published
  version","Phys. Rev. D 103, 063534 (2021)","10.1103/PhysRevD.103.063534",,"astro-ph.CO gr-qc","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the enhancement mechanism provided by a noncanonical kinetic term with a
peak, the amplitude of primordial curvature perturbations can be enhanced by
seven orders of magnitude at small scales while keeping to be consistent with
observations at large scales. The peak function and inflationary potential are
not restricted in this mechanism. We use the Higgs model and T-model as
examples to show how abundant primordial black hole dark matter with different
mass and scalar induced secondary gravitational waves with different peak
frequency are generated. We also show that the enhanced power spectrum for the
primordial curvature perturbations and the energy density of the scalar induced
secondary gravitational waves can have either a sharp peak or a broad peak. The
primordial black holes with the mass around $10^{-14}-10^{-12} M_{\odot}$
produced with the enhancement mechanism can make up almost all dark matter, and
the scalar induced secondary gravitational waves accompanied with the
production of primordial black holes can be tested by the pulsar timing arrays
and spaced based gravitational wave observatory. Therefore, the mechanism can
be tested by primordial black hole dark matter and gravitational wave
observations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:47:04 GMT""},{""version"":""v2"",""created"":""Thu, 24 Dec 2020 04:59:38 GMT""},{""version"":""v3"",""created"":""Sat, 10 Apr 2021 04:00:22 GMT""}]","2021-04-13"
"2011.10607","Christopher Dean","Christopher L. Dean, Stephen J. Lee, Jason Pacheco, John W. Fisher III","Lightweight Data Fusion with Conjugate Mappings",,,,,"stat.ML cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present an approach to data fusion that combines the interpretability of
structured probabilistic graphical models with the flexibility of neural
networks. The proposed method, lightweight data fusion (LDF), emphasizes
posterior analysis over latent variables using two types of information:
primary data, which are well-characterized but with limited availability, and
auxiliary data, readily available but lacking a well-characterized statistical
relationship to the latent quantity of interest. The lack of a forward model
for the auxiliary data precludes the use of standard data fusion approaches,
while the inability to acquire latent variable observations severely limits
direct application of most supervised learning methods. LDF addresses these
issues by utilizing neural networks as conjugate mappings of the auxiliary
data: nonlinear transformations into sufficient statistics with respect to the
latent variables. This facilitates efficient inference by preserving the
conjugacy properties of the primary data and leads to compact representations
of the latent variable posterior distributions. We demonstrate the LDF
methodology on two challenging inference problems: (1) learning electrification
rates in Rwanda from satellite imagery, high-level grid infrastructure, and
other sources; and (2) inferring county-level homicide rates in the USA by
integrating socio-economic data using a mixture model of multiple conjugate
mappings.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:47:13 GMT""}]","2020-11-24"
"2011.10608","Rameswar Panda","Ulrich Finkler, Michele Merler, Rameswar Panda, Mayoore S. Jaiswal,
  Hui Wu, Kandan Ramakrishnan, Chun-Fu Chen, Minsik Cho, David Kung, Rogerio
  Feris, and Bishwaranjan Bhattacharjee","Large Scale Neural Architecture Search with Polyharmonic Splines",,,,,"cs.CV","http://creativecommons.org/licenses/by/4.0/","  Neural Architecture Search (NAS) is a powerful tool to automatically design
deep neural networks for many tasks, including image classification. Due to the
significant computational burden of the search phase, most NAS methods have
focused so far on small, balanced datasets. All attempts at conducting NAS at
large scale have employed small proxy sets, and then transferred the learned
architectures to larger datasets by replicating or stacking the searched cells.
We propose a NAS method based on polyharmonic splines that can perform search
directly on large scale, imbalanced target datasets. We demonstrate the
effectiveness of our method on the ImageNet22K benchmark[16], which contains 14
million images distributed in a highly imbalanced manner over 21,841
categories. By exploring the search space of the ResNet [23] and Big-Little Net
ResNext [11] architectures directly on ImageNet22K, our polyharmonic splines
NAS method designed a model which achieved a top-1 accuracy of 40.03% on
ImageNet22K, an absolute improvement of 3.13% over the state of the art with
similar global batch size [15].
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:50:35 GMT""}]","2020-11-24"
"2011.10609","Do\u{g}a G\""urg\""uno\u{g}lu","Do\u{g}a G\""urg\""uno\u{g}lu, Berkan Dulek, Sinan Gezici","Power Adaptation for Vector Parameter Estimation according to Fisher
  Information based Optimality Criteria","Submitted to a journal",,,,"eess.SP","http://creativecommons.org/publicdomain/zero/1.0/","  The optimal power adaptation problem is investigated for vector parameter
estimation according to various Fisher information based optimality criteria.
By considering an observation model that involves a linear transformation of
the parameter vector and an additive noise component with an arbitrary
probability distribution, six different optimal power allocation problems are
formulated based on Fisher information based objective functions. Via
optimization theoretic approaches, various closed-form solutions are derived
for the proposed problems. Also, the results are extended to cases in which
nuisance parameters exist in the system model or certain types of nonlinear
transformations are applied on the parameter vector. Numerical examples are
presented to investigate performance of the proposed power allocation
strategies.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:51:32 GMT""},{""version"":""v2"",""created"":""Sun, 17 Oct 2021 21:04:25 GMT""}]","2021-10-19"
"2011.10610","Adam Thorpe","Adam J. Thorpe, Kendric R. Ortiz, Meeko M. K. Oishi","SReachTools Kernel Module: Data-Driven Stochastic Reachability Using
  Hilbert Space Embeddings of Distributions",,,,,"math.OC cs.SY eess.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present algorithms for performing data-driven stochastic reachability as
an addition to SReachTools, an open-source stochastic reachability toolbox. Our
method leverages a class of machine learning techniques known as kernel
embeddings of distributions to approximate the safety probabilities for a wide
variety of stochastic reachability problems. By representing the probability
distributions of the system state as elements in a reproducing kernel Hilbert
space, we can learn the ""best fit"" distribution via a simple regularized
least-squares problem, and then compute the stochastic reachability safety
probabilities as simple linear operations. This technique admits finite sample
bounds and has known convergence in probability. We implement these methods as
part of SReachTools, and demonstrate their use on a double integrator system,
on a million-dimensional repeated planar quadrotor system, and a cart-pole
system with a black-box neural network controller.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:52:45 GMT""},{""version"":""v2"",""created"":""Mon, 22 Mar 2021 18:20:15 GMT""}]","2021-03-24"
"2011.10611","Mark Robert Baker","Mark Robert Baker, Natalia Kiriushcheva, Sergei Kuzmin","Noether and Hilbert (metric) energy-momentum tensors are not, in
  general, equivalent","35 pages","Nuclear Physics B 962 (2021) 115240","10.1016/j.nuclphysb.2020.115240",,"math-ph gr-qc math.MP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multiple methods for deriving the energy-momentum tensor for a physical
theory exist in the literature. The most common methods are to use Noether's
first theorem with the 4-parameter Poincar\'{e} translation, or to write the
action in a curved spacetime and perform variation with respect to the metric
tensor, then return to a Minkowski spacetime. These are referred to as the
Noether and Hilbert (metric/ curved space/ variational) energy-momentum
tensors, respectively. In electrodynamics and other simple models, the Noether
and Hilbert methods yield the same result. Due to this fact, it is often
asserted that these methods are generally equivalent for any theory considered,
and that this gives physicists a freedom in using either method to derive an
energy-momentum tensor depending on the problem at hand. $\dots$ For spin-2,
the ideal candidate to check this equivalence for a more complicated model,
there exist many energy-momentum tensors in the literature, none of which are
gauge invariant, so it is not clear which expression one hopes to obtain from
the Noether and Hilbert approaches unlike in the case of e.g. electrodynamics.
It has been shown, however, that the linearized Gauss-Bonnet gravity model
(second order derivatives, second rank tensor potential) has an energy-momentum
tensor that is unique, gauge invariant, symmetric, conserved, and trace-free
when derived from Noether's first theorem (all the same properties of the
physical energy-momentum tensor of electrodynamics). This makes it the ideal
candidate to check if the Noether and Hilbert methods coincide for a more
complicated model. It is proven here using this model as a counterexample, by
direct calculation, that the Noether and Hilbert energy-momentum tensors are
not, in general, equivalent.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 19:53:47 GMT""}]","2020-11-24"
"2011.10612","Panagiotis Iosif","Panagiotis Iosif and Nikolaos Stergioulas","Equilibrium sequences of differentially rotating stars with
  post-merger-like rotational profiles","[v1]:14 pages, 10 figures, [v2]: 18 pages, 12 figures (extra results
  added), [v3]: typos corrected, accepted for publication in MNRAS","Mon. Not. R. Astron. Soc. 503(1): 850-866 (2021)","10.1093/mnras/stab392","VIR-1007A-20","gr-qc astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present equilibrium sequences of rotating relativistic stars, constructed
with a new rotation law that was proposed by Uryu et al. (2017). We choose
rotational parameters motivated by simulations of binary neutron star merger
remnants, but otherwise adopt a cold, relativistic N=1 polytropic EOS, in order
to perform a detailed comparison to published equilibrium sequences that used
the Komatsu, Eriguchi and Hachisu (1989) rotation law. We find a small
influence of the choice of rotation law on the mass of the equilibrium models
and a somewhat larger influence on their radius. The versatility of the new
rotation law allows us to construct models that have a similar rotational
profile and axis ratio as observed for merger remnants, while at the same time
being quasi-spherical. More specifically, we construct equilibrium sequence
variations with different degrees of differential rotation and identify type A
and type C solutions, similar to the corresponding types in the classification
of Ansorg, Gondek-Rosinska and Villain (2009). While our models are highly
accurate solutions of the fully general relativistic structure equations, we
demonstrate that for models relevant to merger remnants the IWM-CFC
approximation still maintains an acceptable accuracy.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:04:15 GMT""},{""version"":""v2"",""created"":""Thu, 4 Feb 2021 17:38:37 GMT""},{""version"":""v3"",""created"":""Wed, 24 Mar 2021 14:06:39 GMT""}]","2021-03-25"
"2011.10613","Martin Scharlemann","Martin Scharlemann","Generating the Goeritz group of $S^3$","v2 is a major rewrite and expansion, now meant to include all
  technical details of the argument",,,,"math.GT","http://creativecommons.org/licenses/by/4.0/","  In 1980 J. Powell \cite{Po} proposed that five specific elements sufficed to
generate the Goeritz group for any genus Heegaard splitting of the 3-sphere.
Here we prove that a natural expansion of Powell's proposed generators, to
include all eyeglass twists and all topological conjugates of Powell's
generators does suffice.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:07:30 GMT""},{""version"":""v2"",""created"":""Fri, 22 Jul 2022 15:04:52 GMT""}]","2022-07-25"
"2011.10614","James Stokes","Tianchen Zhao, James Stokes, Oliver Knitter, Brian Chen, Shravan
  Veerapaneni","Meta Variational Monte Carlo","To appear at the Third Workshop on Machine Learning and the Physical
  Sciences (NeurIPS 2020)",,,,"quant-ph cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An identification is found between meta-learning and the problem of
determining the ground state of a randomly generated Hamiltonian drawn from a
known ensemble. A model-agnostic meta-learning approach is proposed to solve
the associated learning problem and a preliminary experimental study of random
Max-Cut problems indicates that the resulting Meta Variational Monte Carlo
accelerates training and improves convergence.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:11:42 GMT""}]","2020-11-24"
"2011.10615","Tom Grimes","Tom Grimes, Eric Church, William Pitts, Lynn Wood, Eva Brayfindley,
  Luke Erikson, Mark Greaves","Adversarial Training for EM Classification Networks","10 pages, 16 figures",,,,"cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a novel variant of Domain Adversarial Networks with impactful
improvements to the loss functions, training paradigm, and hyperparameter
optimization. New loss functions are defined for both forks of the DANN
network, the label predictor and domain classifier, in order to facilitate more
rapid gradient descent, provide more seamless integration into modern neural
networking frameworks, and allow previously unavailable inferences into network
behavior. Using these loss functions, it is possible to extend the concept of
'domain' to include arbitrary user defined labels applicable to subsets of the
training data, the test data, or both. As such, the network can be operated in
either 'On the Fly' mode where features provided by the feature extractor
indicative of differences between 'domain' labels in the training data are
removed or in 'Test Collection Informed' mode where features indicative of
difference between 'domain' labels in the combined training and test data are
removed (without needing to know or provide test activity labels to the
network). This work also draws heavily from previous works on Robust Training
which draws training examples from a L_inf ball around the training data in
order to remove fragile features induced by random fluctuations in the data. On
these networks we explore the process of hyperparameter optimization for both
the domain adversarial and robust hyperparameters. Finally, this network is
applied to the construction of a binary classifier used to identify the
presence of EM signal emitted by a turbopump. For this example, the effect of
the robust and domain adversarial training is to remove features indicative of
the difference in background between instances of operation of the device -
providing highly discriminative features on which to construct the classifier.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:11:58 GMT""}]","2020-11-24"
"2011.10616","Rui Wang","Rui Wang, Danielle Maddix, Christos Faloutsos, Yuyang Wang, Rose Yu","Bridging Physics-based and Data-driven modeling for Learning Dynamical
  Systems",,,,,"cs.LG physics.soc-ph q-bio.PE","http://creativecommons.org/licenses/by/4.0/","  How can we learn a dynamical system to make forecasts, when some variables
are unobserved? For instance, in COVID-19, we want to forecast the number of
infected and death cases but we do not know the count of susceptible and
exposed people. While mechanics compartment models are widely used in epidemic
modeling, data-driven models are emerging for disease forecasting. We first
formalize the learning of physics-based models as AutoODE, which leverages
automatic differentiation to estimate the model parameters. Through a benchmark
study on COVID-19 forecasting, we notice that physics-based mechanistic models
significantly outperform deep learning. Our method obtains a 57.4% reduction in
mean absolute errors for 7-day ahead COVID-19 forecasting compared with the
best deep learning competitor. Such performance differences highlight the
generalization problem in dynamical system learning due to distribution shift.
We identify two scenarios where distribution shift can occur: changes in data
domain and changes in parameter domain (system dynamics). Through systematic
experiments on several dynamical systems, we found that deep learning models
fail to forecast well under both scenarios. While much research on distribution
shift has focused on changes in the data domain, our work calls attention to
rethink generalization for learning dynamical systems.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:16:10 GMT""},{""version"":""v2"",""created"":""Thu, 29 Apr 2021 08:09:56 GMT""}]","2021-04-30"
"2011.10617","Parthapratim Biswas","Parthapratim Biswas and Dil Limbu","$Ab$ $initio$ hydrogen dynamics and the morphology of voids in amorphous
  silicon","10 pages, 13 figures",,"10.1002/pssb.202170047",,"cond-mat.mtrl-sci cond-mat.dis-nn","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents an $ab$ $initio$ study of hydrogen dynamics inside
nanometer-size voids in $a$-Si within the framework of the density-functional
theory for a varying hydrogen load of 10 to 30 H atoms/void at the low and high
temperature of 400 K and 700 K, respectively. Using the local density
approximation and its generalized-gradient counterpart, the dynamics of
hydrogen atoms inside the voids are examined with an emphasis on the diffusion
of H atoms/molecules, and the resulting nanostructural changes of the void
surfaces. The results from simulations suggest that the microstructure of the
hydrogen distribution on the void surfaces and the morphology of the voids are
characterized by the presence of a significant number of monohydride Si-H
bonds, along with a few dihydride Si-H$_2$ configurations. The study also
reveals that a considerable number of (about 10--45 at.%) total H atoms inside
voids can appear as H$_2$ molecules for a hydrogen load of 10--30 H atoms/void.
The approximate shape of the voids is addressed from a knowledge of the
positions of the void-surface atoms using the convex-hull approximation and the
Gaussian broadening of the pseudo-atomic surfaces of Si and H atoms.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:17:44 GMT""}]","2021-10-04"
"2011.10618","Kenji Maillard","Meven Lennon-Bertrand, Kenji Maillard, Nicolas Tabareau, \'Eric Tanter","Gradualizing the Calculus of Inductive Constructions","83 pages (59 + bibliography + appendix), final version to appear in
  TOPLAS",,"10.1145/3495528",,"cs.PL","http://creativecommons.org/licenses/by/4.0/","  We investigate gradual variations on the Calculus of Inductive Construction
(CIC) for swifter prototyping with imprecise types and terms. We observe, with
a no-go theorem, a crucial tradeoff between graduality and the key properties
of normalization and closure of universes under dependent product that CIC
enjoys. Beyond this Fire Triangle of Graduality, we explore the gradualization
of CIC with three different compromises, each relaxing one edge of the Fire
Triangle. We develop a parametrized presentation of Gradual CIC (GCIC) that
encompasses all three variations, and develop their metatheory. We first
present a bidirectional elaboration of GCIC to a dependently-typed cast
calculus, CastCIC, which elucidates the interrelation between typing,
conversion, and the gradual guarantees. We use a syntactic model of CastCIC to
inform the design of a safe, confluent reduction, and establish, when
applicable, normalization. We study the static and dynamic gradual guarantees
as well as the stronger notion of graduality with embedding-projection pairs
formulated by New and Ahmed, using appropriate semantic model constructions.
This work informs and paves the way towards the development of malleable proof
assistants and dependently-typed programming languages.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:18:35 GMT""},{""version"":""v2"",""created"":""Sat, 25 Sep 2021 07:56:01 GMT""},{""version"":""v3"",""created"":""Mon, 8 Nov 2021 21:23:03 GMT""},{""version"":""v4"",""created"":""Wed, 17 Nov 2021 13:22:40 GMT""}]","2021-11-18"
"2011.10619","Dimitris Boskos","Dimitris Boskos and Dimos V. Dimarogonas","Finite Horizon Discrete Models for Multi-Agent Control Systems with
  Coupled Dynamics",,,"10.1016/j.automatica.2020.108838",,"eess.SY cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The goal of this paper is to obtain online abstractions for coupled
multi-agent systems in a decentralized manner. A discrete model which captures
the motion capabilities of each agent is derived over a bounded time-horizon,
by discretizing a corresponding overapproximation of the agent's reachable
states. The individual abstractions' composition provides a correct
representation of the coupled continuous system over the horizon and renders
the approach appropriate for control synthesis under high-level specifications
which are assigned to the agents over this time window. Sufficient conditions
are also provided for the space and time discretization to guarantee the
derivation of deterministic abstractions with tunable transition capabilities.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:23:41 GMT""}]","2020-11-26"
"2011.10620","P. A. Gonzalez","R. D. B. Fontana, P. A. Gonz\'alez, Eleftherios Papantonopoulos, Yerko
  V\'asquez","Anomalous decay rate of quasinormal modes in Reissner-Nordstr\""om black
  holes","Version accepted for publication in PRD","Phys. Rev. D 103, 064005 (2021)","10.1103/PhysRevD.103.064005",,"gr-qc hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The anomalous decay rate of the quasinormal modes occurs when the
longest-lived modes are the ones with higher angular number. Such behaviour has
been recently studied in different static spacetimes, for scalar and fermionic
perturbations, being observed in both cases. In this work, we extend the
existent studies to the charged spacetimes, namely, the Reissner-Nordstr\""om,
the Reissner-Nordstr\""om-de Sitter and the Reissner-Nordstr\""om-Anti-de Sitter
black holes. We show that the anomalous decay rate behaviour of the scalar
field perturbations is present for every charged geometry in the photon sphere
modes, with the existence of a critical scalar field mass whenever $\Lambda
\geq 0$. In general, this critical value of mass increases with the raise of
the black hole charge, thus rendering a minimum in the Schwarzschild limit. We
also study the dominant mode/family for the massless and massive scalar field
in these geometries showing a non-trivial dominance of the spectra that depends
on the black hole mass and charge.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:24:16 GMT""},{""version"":""v2"",""created"":""Wed, 9 Dec 2020 13:03:10 GMT""},{""version"":""v3"",""created"":""Wed, 3 Feb 2021 13:23:15 GMT""}]","2021-03-10"
"2011.10621","Antonio Ricardo Martines","Antonio Ricardo Martines","An exact power series expansion for the Kallen-Sabry vacuum polarization
  potential","12 pages",,,,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this work it is presented an exact power series formula for the
Kallen-Sabry vacuum polarization potential.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:26:00 GMT""}]","2020-11-24"
"2011.10622","Sophie Kriz","Sophie Kriz","Notes on equivariant homology with constant coefficients","Accepted for publication in the Pacific Journal of Mathematics",,,,"math.AT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, for a finite group, we discuss a method for calculating
equivariant homology with constant coefficients. We apply it to completely
calculate the geometric fixed points of the equivariant spectrum representing
equivariant (co)homology with constant coefficients. We also treat a more
complicated example of inverting the standard representation in the equivariant
homology of split extraspecial groups at the prime 2.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:26:31 GMT""}]","2020-11-24"
"2011.10623","Glenn Hurlbert","Glenn Hurlbert and Essak Seddiq","On the Target Pebbling Conjecture","16 pages, 4 figures. Replaces ""The Target Pebbling Conjecture""
  (2011.10623.v2), with improved exposition",,,,"math.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Graph pebbling is a network optimization model for satisfying vertex demands
with vertex supplies (called pebbles), with partial loss of pebbles in transit.
The pebbling number of a demand in a graph is the smallest number for which
every placement of that many supply pebbles satisfies the demand. The Target
Conjecture (Herscovici-Hester-Hurlbert, 2009) posits that the largest pebbling
number of a demand of fixed size $t$ occurs when the demand is entirely stacked
on one vertex. This truth of this conjecture could be useful for attacking many
open problems in graph pebbling, including the famous conjecture of Graham
(1989) involving graph products. It has been verified for complete graphs,
cycles, cubes, and trees. In this paper we prove the conjecture for 2-paths and
Kneser graphs over pairs.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:29:53 GMT""},{""version"":""v2"",""created"":""Mon, 10 May 2021 16:08:42 GMT""},{""version"":""v3"",""created"":""Mon, 20 Dec 2021 22:49:34 GMT""}]","2021-12-22"
"2011.10624","Wayesh Qarony","E. Ponizovskaya Devine, Wayesh Qarony, Ahasan Ahamed, Ahmed S Mayet,
  Soroush Ghandiparsi, Cesar Bartolo-Perez, Aly F Elrefaie, Toshishige Yamada,
  Shih-Yuan Wang, M. Saif Islam","Single Microhole per Pixel in CMOS Image Sensor with Enhanced Optical
  Sensitivity in Near-Infrared","13 pages, 4 figures, and 2 tables",,,,"physics.optics eess.IV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Silicon photodiode based CMOS sensors with backside-illumination for 300 to
1000 nm wavelength range were studied. We showed that a single hole in the
photodiode increases the optical efficiency of the pixel. In near-infrared
wavelengths, the enhancement allows 70% absorption in a 3 microns thick Si. It
is 4x better than for the flat pixel. We compared different shapes and sizes of
single holes and holes arrays. We have shown that a certain size and shape in
single holes pronounce better optical efficiency enhancement. The crosstalk was
successfully reduced with trenches between pixels. We optimized the trenches to
achieve minimal pixel separation for 1.12 microns pixel.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:33:15 GMT""}]","2020-11-24"
"2011.10625","Zhentian Qian","Zhentian Qian, Kartik Patath, Jie Fu, Jing Xiao","Semantic SLAM with Autonomous Object-Level Data Association",,,,,"cs.RO cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It is often desirable to capture and map semantic information of an
environment during simultaneous localization and mapping (SLAM). Such semantic
information can enable a robot to better distinguish places with similar
low-level geometric and visual features and perform high-level tasks that use
semantic information about objects to be manipulated and environments to be
navigated. While semantic SLAM has gained increasing attention, there is little
research on semanticlevel data association based on semantic objects, i.e.,
object-level data association. In this paper, we propose a novel object-level
data association algorithm based on bag of words algorithm, formulated as a
maximum weighted bipartite matching problem. With object-level data association
solved, we develop a quadratic-programming-based semantic object initialization
scheme using dual quadric and introduce additional constraints to improve the
success rate of object initialization. The integrated semantic-level SLAM
system can achieve high-accuracy object-level data association and real-time
semantic mapping as demonstrated in the experiments. The online semantic map
building and semantic-level localization capabilities facilitate semantic-level
mapping and task planning in a priori unknown environment.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:33:39 GMT""}]","2020-11-24"
"2011.10626","Joshua Lothringer","Joshua D. Lothringer, Zafar Rustamkulov, David K. Sing, Neale P.
  Gibson, Jamie Wilson, Kevin C. Schlaufman","A New Window into Planet Formation and Migration: Refractory-to-Volatile
  Elemental Ratios in Ultra-hot Jupiters","20 pages, 9 Figures. Accepted for publication in ApJ",,"10.3847/1538-4357/abf8a9",,"astro-ph.EP","http://creativecommons.org/licenses/by/4.0/","  A primary goal of exoplanet characterization is to use a planet's current
composition to understand how that planet formed. For example, the C/O ratio
has long been recognized as carrying important information on the chemistry of
volatile species. Refractory elements, like Fe, Mg, and Si, are usually not
considered in this conversation because they condense into solids like Fe(s) or
MgSiO$_3$ and would be removed from the observable, gaseous atmosphere in
exoplanets cooler than about 2000~K. However, planets hotter than about 2000~K,
called ultra-hot Jupiters (UHJs), are warm enough to largely avoid the
condensation of refractory species. In this paper, we explore the insight that
the measurement of refractory abundances can provide into a planet's origins.
Through refractory-to-volatile elemental abundance ratios, we can estimate a
planet's atmospheric rock-to-ice fraction and constrain planet formation and
migration scenarios. We first relate a planet's present-day
refractory-to-volatile ratio to its rock-to-ice ratio from formation using
various compositional models for the rocky and icy components of the
protoplanetary disk. We discuss potential confounding factors like the
sequestration of heavy metals in the core and condensation. We then show such a
measurement using atmospheric retrievals of the low-resolution UV-IR
transmission spectrum of WASP-121b with PETRA, from which we estimate a
refractory-to-volatile ratio of 5.0$^{+6.0}_{-2.7}\times$ solar and a
rock-to-ice ratio greater than 2/3. This result is consistent with significant
atmospheric enrichment by rocky planetismals. Lastly, we discuss the rich
future potential for measuring refractory-to-volatile ratios in ultra-hot
Jupiters with the arrival of JWST and by combining observations at low- and
high-resolution.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:34:30 GMT""},{""version"":""v2"",""created"":""Thu, 15 Apr 2021 16:29:17 GMT""}]","2021-06-23"
"2011.10627","Quinn Minor","Quinn E. Minor, Sophia Gad-Nasr, Manoj Kaplinghat, Simona Vegetti","An unexpected high concentration for the dark substructure in the
  gravitational lens SDSSJ0946+1006","24 pages, 18 figures, 3 appendices",,"10.1093/mnras/stab2247",,"astro-ph.GA astro-ph.CO","http://creativecommons.org/licenses/by/4.0/","  The presence of an invisible substructure has previously been detected in the
gravitational lens galaxy SDSSJ0946+1006 through its perturbation of the lensed
images. Using flexible models for the main halo and the subhalo perturbation to
fit the lensed images, we demonstrate that the subhalo has an extraordinarily
high central density and steep density slope. The inferred concentration for
the subhalo is well above the expected scatter in concentrations for
$\Lambda$CDM halos of similar mass. We robustly infer the subhalo's projected
mass within 1 kpc to be $\sim 2$-$3.7\times 10^9$M$_\odot$ at $>$95% CL for all
our lens models, while the average slope of the subhalo's projected density
profile over the radial range 0.75-1.25 kpc is constrained to be steeper than
isothermal ($\gamma_{2D} \lesssim -1$). By modeling the subhalo light directly,
we infer a conservative upper bound on its luminosity $L_V < 1.2\times
10^8L_\odot$ at 95% CL, which shows that the perturber is dark matter
dominated. To compare to $\Lambda$CDM expectations, we analyze subhalos within
analogues of lensing galaxies in the Illustris TNG100-1 simulation over many
lines of sight, and find hundreds of subhalos that achieve a projected mass
within 1 kpc of $\gtrsim 2\times10^9M_\odot$. However, less than 1% of the mock
observations yield a log-slope steep enough to be consistent with our lensing
models, and they $all$ have stellar masses in excess of that allowed by
observations by about an order of magnitude or more. We conclude that the
presence of such a dark, highly concentrated subhalo is unexpected in a
$\Lambda$CDM universe. Finally, we show that this tension with CDM is not
significantly reduced if the perturber is assumed to be a line-of-sight
structure, rather than a subhalo.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:37:11 GMT""}]","2021-08-18"
"2011.10628","Alessandro Carotenuto","Alessandro Carotenuto, Fedele Lizzi, Mattia Manfredonia, Flavio
  Mercati","The Weyl-Mellin quantization map for $\kappa$-Minkowski Noncommutative
  Spacetime",,,,,"hep-th math-ph math.MP math.QA","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We present a quantization of the functions of spacetime, i.e.\ a map, analog
to Weyl map, which reproduces the $\kappa$-Minkowski commutation relations, and
it has the desirable properties of mapping square integrable funcions into
Hilbert-Schmidt operators, as well as real functions into self-adjoint
operators. The map is based on Mellin transform on radial and time coordinates.
The map also define a deformed $*$ product which we discuss with examples.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:37:15 GMT""},{""version"":""v2"",""created"":""Fri, 10 Dec 2021 09:41:37 GMT""}]","2021-12-13"
"2011.10629","Quinn Minor","Quinn E. Minor, Manoj Kaplinghat, Tony H. Chan, Emily Simon","Inferring the concentration of dark matter subhalos perturbing strongly
  lensed images","14 pages, 11 figures",,"10.1093/mnras/stab2209",,"astro-ph.CO astro-ph.GA","http://creativecommons.org/licenses/by/4.0/","  We demonstrate that the perturbations of strongly lensed images by low-mass
dark matter subhalos are significantly impacted by the concentration of the
perturbing subhalo. For subhalo concentrations expected in $\Lambda$CDM,
significant constraints on the concentration can be obtained at HST resolution
for subhalos with masses larger than about $10^{10}M_\odot$. Constraints are
also possible for lower mass subhalos, if their concentrations are higher than
the expected scatter in CDM. We also find that the concentration of lower mass
perturbers down to $\sim 10^8M_\odot$ can be well-constrained with a resolution
of $\sim 0.01''$, which is achievable with long-baseline interferometry.
Subhalo concentration also plays a critical role in the detectability of a
perturbation, such that only high concentration perturbers with mass $\lesssim
10^9M_\odot$ are likely to be detected at HST resolution. If scatter in the
$\Lambda$CDM mass-concentration relation is not accounted for during lens
modeling, the inferred subhalo mass can be biased by up to a factor of 3(6) for
subhalos of mass $10^9 M_\odot$($10^{10} M_\odot$); this bias can be eliminated
if one varies both mass and concentration during lens fitting. Alternatively,
one may robustly infer the projected mass within the subhalo's perturbation
radius, defined by its distance to the critical curve of the lens being
perturbed. With a sufficient number of detections, these strategies will make
it possible to constrain the halo mass-concentration relation at low masses in
addition to the mass function, offering a probe of dark matter physics as well
as the small-scale primordial power spectrum.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:37:38 GMT""}]","2021-08-11"
"2011.10630","David \v{S}i\v{s}ka","Marc Sabate-Vidales and David \v{S}i\v{s}ka and Lukasz Szpruch","Solving path dependent PDEs with LSTM networks and path signatures",,,,,"q-fin.CP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Using a combination of recurrent neural networks and signature methods from
the rough paths theory we design efficient algorithms for solving parametric
families of path dependent partial differential equations (PPDEs) that arise in
pricing and hedging of path-dependent derivatives or from use of non-Markovian
model, such as rough volatility models in Jacquier and Oumgari, 2019. The
solutions of PPDEs are functions of time, a continuous path (the asset price
history) and model parameters. As the domain of the solution is infinite
dimensional many recently developed deep learning techniques for solving PDEs
do not apply. Similarly as in Vidales et al. 2018, we identify the objective
function used to learn the PPDE by using martingale representation theorem. As
a result we can de-bias and provide confidence intervals for then neural
network-based algorithm. We validate our algorithm using classical models for
pricing lookback and auto-callable options and report errors for approximating
both prices and hedging strategies.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:42:13 GMT""}]","2020-11-24"
"2011.10631","Ahmed Elkamshishy","Ahmed A.Elkamshishy and Chris H. Greene","Counterexample to the Bohigas Conjecture for Transmission Through
  aOne-Dimensional Lattice",,"Phys. Rev. E 103, 062211 (2021)","10.1103/PhysRevE.103.062211",,"nlin.CD cond-mat.dis-nn","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Resonances in particle transmission through a 1D finite lattice are studied
in the presence of a finite number of impurities. Although this is a
one-dimensional system that is classically integrable and has no chaos,
studying the statistical properties of the spectrum such as the level spacing
distribution and the spectral rigidity shows quantum chaos signatures. Using a
dimensionless parameter that reflects the degree of state localization, we
demonstrate how the transition from regularity to chaos is affected by state
localization. The resonance positions are calculated using both the
Wigner-Smithtime-delay and a Siegert state method, which are in good agreement.
Our results give evidence for the existence of quantum chaos in one dimension
which is a counter-example to the Bohigas-Giannoni-Schmit conjecture.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:46:34 GMT""}]","2021-06-16"
"2011.10632","Arne Ludwig","N. Bart, C. Dangel, P. Zajac, N. Spitzer, J. Ritzmann, M. Schmidt, H.
  G. Babin, R. Schott, S. R. Valentin, S. Scholz, Y. Wang, R. Uppu, D. Najer,
  M. C. L\""obl, N. Tomm, A. Javadi, N. O. Antoniadis, L. Midolo, K. M\""uller,
  R. J. Warburton, P. Lodahl, A. D. Wieck, J.J. Finley, and A. Ludwig","Wafer-Scale Epitaxial Modulation of Quantum Dot Density",,,,,"cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph","http://creativecommons.org/licenses/by/4.0/","  Precise control of the properties of semiconductor quantum dots (QDs) is
vital for creating novel devices for quantum photonics and advanced
opto-electronics. Suitable low QD-density for single QD devices and experiments
are challenging to control during epitaxy and are typically found only in
limited regions of the wafer. Here, we demonstrate how conventional molecular
beam epitaxy (MBE) can be used to modulate the density of optically active QDs
in one- and two- dimensional patterns, while still retaining excellent quality.
We find that material thickness gradients during layer-by-layer growth result
in surface roughness modulations across the whole wafer. Growth on such
templates strongly influences the QD nucleation probability. We obtain density
modulations between 1 and 10 QDs/${\mu}m^{2}$ and periods ranging from several
millimeters down to at least a few hundred microns. This novel method is
universal and expected to be applicable to a wide variety of different
semiconductor material systems. We apply the method to enable growth of
ultra-low noise QDs across an entire 3-inch semiconductor wafer.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:51:45 GMT""},{""version"":""v2"",""created"":""Thu, 9 Dec 2021 12:26:53 GMT""}]","2021-12-10"
"2011.10633","Ann-Katrin Michel","Ann-Katrin U. Michel, Felix Donat, Aurelia Siegfried, Olesya Yarema,
  Hanbing Fang, Maksym Yarema, Vanessa Wood, Christoph R. M\""uller, and David
  J. Norris","Phase Transitions in Germanium Telluride Nanoparticle Phase-Change
  Materials Studied by Time-Resolved X-Ray Diffraction",,,"10.1063/5.0032624",,"cond-mat.mtrl-sci cond-mat.mes-hall","http://creativecommons.org/licenses/by/4.0/","  Germanium telluride (GeTe), a phase-change material, is known to exhibit four
different structural phases: three at room temperature (one amorphous and two
crystalline, $\alpha$ and $\gamma$) and one at high temperature (crystalline
$\beta$). Because transitions between the amorphous and crystalline phases lead
to significant changes in material properties (e.g., refractive index and
resistivity), GeTe has been investigated as a phase-change material for
photonics, thermoelectrics, ferroelectrics, and spintronics. Consequently, the
temperature-dependent phase transitions in GeTe have been studied for bulk and
thin-film GeTe, both fabricated by sputtering. Colloidal synthesis of
nanoparticles offers a more flexible fabrication approach for amorphous and
crystalline GeTe. These nanoparticles are known to exhibit size-dependent
properties, such as an increased crystallization temperature for the
amorphous-to-$\alpha$ transition in sub-10\,nm GeTe particles. The
$\alpha$-to-$\beta$ phase transition is also expected to vary with size, but
this effect has not yet been investigated for GeTe. Here, we report
time-resolved X-ray diffraction of GeTe nanoparticles with different diameters
and from different synthetic protocols. We observe a non-volatile
amorphous-to-$\alpha$ transition between 210$^{\circ}$C and 240$^{\circ}$C and
a volatile $\alpha$-to-$\beta$ transition between 370$^{\circ}$C and
420$^{\circ}$C. The latter transition was reversible and repeatable. While the
transition temperatures are shifted relative to the values known for bulk GeTe,
the nanoparticle-based samples still exhibit the same structural phases
reported for sputtered GeTe. Thus, colloidal GeTe maintains the same general
phase behavior as bulk GeTe while allowing for more flexible and accessible
fabrication. Therefore, nanoparticle-based GeTe films show great potential for
applications, such as in active photonics.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:54:43 GMT""}]","2021-03-17"
"2011.10634","Junbo Zhao","Manyun Huang, Junbo Zhao, Zhinong Wei, Marco Pau, and Guoqiang Sun","Distributed Robust State Estimation for Hybrid AC/DC Distribution
  Systems using Multi-Source Data","8 pages, 12 figures",,,,"eess.SY cs.SY","http://creativecommons.org/licenses/by-sa/4.0/","  Hybrid AC/DC distribution systems are becoming a popular means to accommodate
the increasing penetration of distributed energy resources and flexible loads.
This paper proposes a distributed and robust state estimation (DRSE) method for
hybrid AC/DC distribution systems using multiple sources of data. In the
proposed distributed implementation framework, a unified robust linear state
estimation model is derived for each AC and DC regions, where the regions are
connected via AC/DC converters and only limited information exchange is needed.
To enhance the estimation accuracy of the areas with low measurement coverage,
a deep neural network (DNN) is used to extract hidden system statistical
information and allow deriving nodal power injections that keep up with the
real-time measurement update rate. This provides the way of integrating smart
meter data, SCADA measurements and zero injections together for state
estimation. Simulations on two hybrid AC/DC distribution systems show that the
proposed DRSE has only slight accuracy loss by the linearization formulation
but offers robustness of suppressing bad data automatically, as well as
benefits of improving computational efficiency.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:58:02 GMT""}]","2020-11-24"
"2011.10635","Hashim A. Hashim","Hashim A. Hashim and Abdelrahman E. E. Eltoukhy","Landmark and IMU Data Fusion: Systematic Convergence Geometric Nonlinear
  Observer for SLAM and Velocity Bias",,,"10.1109/TITS.2020.3035550",,"eess.SY cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Navigation solutions suitable for cases when both autonomous robot's pose
(\textit{i.e}., attitude and position) and its environment are unknown are in
great demand. Simultaneous Localization and Mapping (SLAM) fulfills this need
by concurrently mapping the environment and observing robot's pose with respect
to the map. This work proposes a nonlinear observer for SLAM posed on the
manifold of the Lie group of $\mathbb{SLAM}_{n}\left(3\right)$, characterized
by systematic convergence, and designed to mimic the nonlinear motion dynamics
of the true SLAM problem. The system error is constrained to start within a
known large set and decay systematically to settle within a known small set.
The proposed estimator is guaranteed to achieve predefined transient and
steady-state performance and eliminate the unknown bias inevitably present in
velocity measurements by directly using measurements of angular and
translational velocity, landmarks, and information collected by an inertial
measurement unit (IMU). Experimental results obtained by testing the proposed
solution on a real-world dataset collected by a quadrotor demonstrate the
observer's ability to estimate the six-degrees-of-freedom (6 DoF) robot pose
and to position unknown landmarks in three-dimensional (3D) space. Keywords:
Simultaneous Localization and Mapping, Nonlinear filter for SLAM, Nonlinear
filter for SLAM on Matrix Lie group, pose, asymptotic stability, prescribed
performance, adaptive estimate, feature, inertial measurement unit, inertial
vision unit, IMU, SE(3), SO(3), noise.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:58:11 GMT""},{""version"":""v2"",""created"":""Sat, 28 Nov 2020 18:43:15 GMT""},{""version"":""v3"",""created"":""Thu, 31 Mar 2022 19:24:49 GMT""}]","2022-04-04"
"2011.10636","Felipe Lepe","Erwin Hern\'andez, Felipe Lepe, Jesus Vellojin","A mixed parameter formulation with applications to linear
  viscoelasticity",,,,,"math.NA cs.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this work we propose and analyze an abstract parameter dependent model
written as a mixed variational formulation based on Volterra integrals of
second kind. For the analysis, we consider a suitable adaptation to the classic
mixed theory in the Volterra equations setting, and prove the well posedness of
the resulting mixed viscoelastic formulation. Error estimates are derived,
using the available results for Volterra equations, where all the estimates are
independent of the perturbation parameter. We consider an application of the
developed theory in a viscoelastic Timoshenko beam, and report numerical
experiments in order to assess the independence of the perturbation parameter.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:00:19 GMT""}]","2020-11-24"
"2011.10637","Remy Dubertrand","Dominik Hahn, Juan-Diego Urbina, Klaus Richter, Remy Dubertrand, S. L.
  Sondhi","Ergodic and non-ergodic many-body dynamics in strongly nonlinear
  lattices","16 pages, 12 figures","Phys. Rev. E 103, 052213 (2021)","10.1103/PhysRevE.103.052213",,"nlin.CD cond-mat.stat-mech","http://creativecommons.org/licenses/by/4.0/","  The study of non-linear oscillator chains in classical many-body dynamics has
a storied history going back to the seminal work of Fermi, Pasta, Ulam and
Tsingou (FPUT). We introduce a new family of such systems which consist of
chains of $N$ harmonically coupled particles with the non-linearity introduced
by confining the motion of each individual particle to a box/stadium with hard
walls. The stadia are arranged on a one dimensional lattice but they
individually do not have to be one dimensional thus permitting the introduction
of chaos already at the lattice scale. For the most part we study the case
where the motion is entirely one dimensional. We find that the system exhibits
a mixed phase space for any finite value of $N$. Computations of Lyapunov
spectra at randomly picked phase space locations and a direct comparison
between Hamiltonian evolution and phase space averages indicate that the
regular regions of phase space are not significant at large system sizes. While
the continuum limit of our model is itself a singular limit of the integrable
sinh-Gordon theory, we do not see any evidence for the kind of non-ergodicity
famously seen in the FPUT work. Finally, we examine the chain with particles
confined to two dimensional stadia where the individual stadium is already
chaotic, and find a much more chaotic phase space at small system sizes.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:03:14 GMT""},{""version"":""v2"",""created"":""Tue, 18 May 2021 10:53:57 GMT""}]","2021-05-19"
"2011.10638","Paolo Leonetti","Marek Balcerzak and Paolo Leonetti","Convergent subseries of divergent series","6 pp; comments are welcome",,,,"math.CA math.FA math.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Let $\mathscr{X}$ be the set of positive real sequences $x=(x_n)$ such that
the series $\sum_n x_n$ is divergent. For each $x \in \mathscr{X}$, let
$\mathcal{I}_x$ be the collection of all $A\subseteq \mathbf{N}$ such that the
subseries $\sum_{n \in A}x_n$ is convergent. Moreover, let $\mathscr{A}$ be the
set of sequences $x \in \mathscr{X}$ such that $\lim_n x_n=0$ and
$\mathcal{I}_x\neq \mathcal{I}_y$ for all sequences $y=(y_n) \in \mathscr{X}$
with $\liminf_n y_{n+1}/y_n>0$. We show that $\mathscr{A}$ is comeager and that
contains uncountably many sequences $x$ which generate pairwise nonisomorphic
ideals $\mathcal{I}_x$. This answers, in particular, an open question recently
posed by M. Filipczak and G. Horbaczewska.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:07:58 GMT""}]","2020-11-24"
"2011.10639","Paulo Brand\~ao","Paulo A. Brand\~ao, Jo\~ao P. Mendon\c{c}a and S. B. Cavalcanti","Low coherence-induced resonance in double-layer structures having
  parity-time symmetry",,,"10.1364/OL.415663",,"physics.optics","http://creativecommons.org/licenses/by/4.0/","  We derive simple formulae for the transmittance $T$ and reflectance $R$ of
Gaussian-Schell beams incident upon any stratified dielectric structure by
using second-order classical coherence theory in the space-frequency picture.
The formalism is applied to a particular structure consisting of a
double-layer, with balanced gain and loss, satisfying the parity-time symmetry
conditions. It is shown that sources with a low degree of spatial coherence, on
the order of the wavelength, can induce large resonant peaks in the transmitted
and reflected amplitudes. The resonance peaks vanish as the spatial coherence
increases.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:11:02 GMT""},{""version"":""v2"",""created"":""Mon, 11 Jan 2021 13:28:08 GMT""}]","2021-02-24"
"2011.10640","Michael Gr. Voskoglou Prof. Dr.","Michael Voskoglou","Assessment and Linear Programming under Fuzzy Conditions","19 pages, 3 figures","Journal of Fuzzy Extension and Applications, 1(3), 198-216, 2020","10.22105/jfea.2020.253436.1024",,"cs.AI","http://creativecommons.org/licenses/by/4.0/","  A new fuzzy method is developed using triangular/trapezoidal fuzzy numbers
for evaluating a group's mean performance, when qualitative grades instead of
numerical scores are used for assessing its members' individual performance.
Also, a new technique is developed for solving Linear Programming problems with
fuzzy coefficients and everyday life applications are presented to illustrate
our results.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:13:36 GMT""}]","2020-11-24"
"2011.10641","Ben Cameron","Maimoonah Ahmed and Ben Cameron","The node cop-win reliability of unicyclic and bicyclic graphs",,,,,"math.CO","http://creativecommons.org/licenses/by/4.0/","  Various models to quantify the reliability of a network have been studied
where certain components of the graph may fail at random and the probability
that the remaining graph is connected is the proxy for reliability. In this
work we introduce a strengthening of one of these models by considering the
probability that the remaining graph is not just connected but also cop-win. A
graph is cop-win if one cop can guarantee capture of a fleeing robber in the
well-studied pursuit-evasion game of Cops and Robber. More precisely, for a
graph $G$ with nodes that are operational independently with probability $p$
and edges that are operational if and only if both of their endpoints are
operational, the node cop-win reliability of $G$, denoted $\text{NCRel}(G,p)$,
is the probability that the operational nodes induce a cop-win subgraph of $G$.
It is then of interest to find graphs $G$ with $n$ nodes and $m$ edges such
that $\text{NCRel}(G,p)\ge\text{NCRel}(H,p)$ for all $p\in[0,1]$ and all graphs
$H$ with $n$ nodes and $m$ edges. Such a graph is called uniformly most
reliable. We show that uniformly most reliable graphs exist for unicyclic and
bicyclic graphs, respectively. This is in contrast to the fact that there are
no known sparse graphs maximizing the corresponding notion of node reliability.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:13:49 GMT""}]","2020-11-24"
"2011.10642","Daniel Beauchamp","Daniel Beauchamp, Keith M. Chugg","Linearization for High-Speed Current-Steering DACs Using Neural Networks",,,,,"eess.SP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper proposes a novel foreground linearization scheme for a high-speed
CS-DAC. The technique leverages neural networks (NNs) to derive a LUT that maps
the inverse of the DAC transfer characteristic onto the input codes. The
algorithm is shown to improve conventional methods by at least 6dB in terms of
intermodulation (IM) performance for frequencies up to 9GHz on a
state-of-the-art 10-bit CS-DAC operating at 40.96GS/s (gigasamples-per-second)
in 14nm CMOS.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:14:12 GMT""}]","2020-11-24"
"2011.10643","Abolfazl Hashemi","Abolfazl Hashemi, Anish Acharya, Rudrajit Das, Haris Vikalo, Sujay
  Sanghavi, Inderjit Dhillon","On the Benefits of Multiple Gossip Steps in Communication-Constrained
  Decentralized Optimization",,,,,"cs.LG cs.DC math.OC stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In decentralized optimization, it is common algorithmic practice to have
nodes interleave (local) gradient descent iterations with gossip (i.e.
averaging over the network) steps. Motivated by the training of large-scale
machine learning models, it is also increasingly common to require that
messages be {\em lossy compressed} versions of the local parameters. In this
paper, we show that, in such compressed decentralized optimization settings,
there are benefits to having {\em multiple} gossip steps between subsequent
gradient iterations, even when the cost of doing so is appropriately accounted
for e.g. by means of reducing the precision of compressed information. In
particular, we show that having $O(\log\frac{1}{\epsilon})$ gradient iterations
{with constant step size} - and $O(\log\frac{1}{\epsilon})$ gossip steps
between every pair of these iterations - enables convergence to within
$\epsilon$ of the optimal value for smooth non-convex objectives satisfying
Polyak-\L{}ojasiewicz condition. This result also holds for smooth strongly
convex objectives. To our knowledge, this is the first work that derives
convergence results for nonconvex optimization under arbitrary communication
compression.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:17:32 GMT""}]","2020-11-24"
"2011.10644","Gerhard M\""uller","Benaoumeur Bakhti and Gerhard M\""uller","Interacting hard-sphere fluids in an external field","13 pages, 8 figures","Phys. Rev. E 103, 032604 (2021)","10.1103/PhysRevE.103.032604",,"cond-mat.soft","http://creativecommons.org/licenses/by/4.0/","  We present a new method for studying equilibrium properties of interacting
fluids in an arbitrary external field. The fluid is composed of monodisperse
spherical particles with hard-core repulsion and additional interactions of
arbitrary shape and limited range. Our method of analysis is exact in one
dimension and provides demonstrably good approximations in higher dimensions.
It can cope with homogeneous and heterogeneous environments. We derive an
equation for the pair distribution function. The solution, to be evaluated
numerically, in general, or analytically for special cases, enters expressions
for the entropy and free energy functionals. For some one-dimensional systems,
our approach yields analytic solutions, reproducing available exact results
from different approaches.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:17:34 GMT""},{""version"":""v2"",""created"":""Sat, 6 Mar 2021 15:01:44 GMT""}]","2021-03-09"
"2011.10645","Yoji Yamato","Yoji Yamato","Study of Resource Amount Configuration for Automatic Application
  Offloading","6 pages, 1 figure, in Japanese",,,"IEICE Technical Report, SWIM2020-11, Nov. 2020","cs.DC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, utilization of heterogeneous hardware other than small core
CPU such as GPU, FPGA or many core CPU is increasing. However, when using
heterogeneous hardware, barriers of technical skills such as OpenMP, CUDA and
OpenCL are high. Based on that, I have proposed environment-adaptive software
that enables automatic conversion, configuration, and high performance
operation of once written code, according to the hardware to be placed.
However, although the conversion of the code according to the migration
destination environment has been studied so far, there has been no research to
properly set the resource amount. In this paper, as a new element of
environment adaptive software, in order to operate the application with high
cost performance, I study a method to optimize the resource amount of CPUs and
offload devices.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:20:03 GMT""}]","2020-11-24"
"2011.10646","Jamie Scott","Jamie Scott","On the Topological Complexity of Maps",,,,,"math.AT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We define and develop a homotopy invariant notion for the topological
complexity of a map $f:X \to Y$, denoted TC($f$), that interacts with TC($X$)
and TC($Y$) in the same way cat($f$) interacts with cat($X$) and cat($Y$).
Furthermore, TC($f$) and cat($f$) satisfy the same inequalities as TC($X$) and
cat($X$). We compare it to other invariants defined in the papers
[15,16,17,18,20]. We apply TC($f$) to studying group homomorphisms $f:H\to G$.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:20:42 GMT""}]","2020-11-24"
"2011.10647","Krunal Shah","Krunal Shah, Nitish Gupta, Dan Roth","What do we expect from Multiple-choice QA Systems?","Findings of EMNLP 2020","Findings of the Association for Computational Linguistics: EMNLP
  2020 pg. 3547-3553",,,"cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recent success of machine learning systems on various QA datasets could
be interpreted as a significant improvement in models' language understanding
abilities. However, using various perturbations, multiple recent works have
shown that good performance on a dataset might not indicate performance that
correlates well with human's expectations from models that ""understand""
language. In this work we consider a top performing model on several Multiple
Choice Question Answering (MCQA) datasets, and evaluate it against a set of
expectations one might have from such a model, using a series of
zero-information perturbations of the model's inputs. Our results show that the
model clearly falls short of our expectations, and motivates a modified
training approach that forces the model to better attend to the inputs. We show
that the new training paradigm leads to a model that performs on par with the
original model while better satisfying our expectations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:27:10 GMT""}]","2020-11-24"
"2011.10648","Youngsoo Choi","Youngkyu Kim, Karen May Wang, Youngsoo Choi","Efficient space-time reduced order model for linear dynamical systems in
  Python using less than 120 lines of code","24 pages, 18 figures",,,,"math.NA cs.MS cs.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A classical reduced order model (ROM) for dynamical problems typically
involves only the spatial reduction of a given problem. Recently, a novel
space-time ROM for linear dynamical problems has been developed, which further
reduces the problem size by introducing a temporal reduction in addition to a
spatial reduction without much loss in accuracy. The authors show an order of a
thousand speed-up with a relative error of less than 0.00001 for a large-scale
Boltzmann transport problem. In this work, we present for the first time the
derivation of the space-time Petrov-Galerkin projection for linear dynamical
systems and its corresponding block structures. Utilizing these block
structures, we demonstrate the ease of construction of the space-time ROM
method with two model problems: 2D diffusion and 2D convection diffusion, with
and without a linear source term. For each problem, we demonstrate the entire
process of generating the full order model (FOM) data, constructing the
space-time ROM, and predicting the reduced-order solutions, all in less than
120 lines of Python code. We compare our Petrov-Galerkin method with the
traditional Galerkin method and show that the space-time ROMs can achieve
O(100) speed-ups with O(0.001) to O(0.0001) relative errors for these problems.
Finally, we present an error analysis for the space-time Petrov-Galerkin
projection and derive an error bound, which shows an improvement compared to
traditional spatial Galerkin ROM methods.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:31:58 GMT""}]","2020-12-21"
"2011.10649","Rodrigo Bufalo","R. Bufalo, M. Ghasemkhani, and A. Soto","Adler-Bell-Jackiw anomaly in VSR electrodynamics","V1: 13 pages, 1 figure; V2: 13 pages, 1 figure, revised version with
  improved discussions",,,,"hep-th hep-ph","http://creativecommons.org/licenses/by/4.0/","  In this paper, we examine the problem of anomalies of the fermionic currents
in the context of the very special relativity (VSR). We consider the VSR
contributions to the triangle amplitude $\left\langle J_{5}^{\lambda} J^{\mu}
J^{\nu} \right\rangle $, which allows the evaluation of the vector and axial
Ward identities. Actually, we observe that the VSR nonlocal effects respect the
vector Ward identity, and it also contributes in a very interesting and unique
way for the Adler-Bell-Jackiw anomaly.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:33:24 GMT""},{""version"":""v2"",""created"":""Wed, 7 Apr 2021 23:14:43 GMT""}]","2021-04-09"
"2011.10650","Rewon Child","Rewon Child","Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them
  on Images","17 pages, 14 figures",,,,"cs.LG cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a hierarchical VAE that, for the first time, generates samples
quickly while outperforming the PixelCNN in log-likelihood on all natural image
benchmarks. We begin by observing that, in theory, VAEs can actually represent
autoregressive models, as well as faster, better models if they exist, when
made sufficiently deep. Despite this, autoregressive models have historically
outperformed VAEs in log-likelihood. We test if insufficient depth explains why
by scaling a VAE to greater stochastic depth than previously explored and
evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,
these very deep VAEs achieve higher likelihoods, use fewer parameters, generate
samples thousands of times faster, and are more easily applied to
high-resolution images. Qualitative studies suggest this is because the VAE
learns efficient hierarchical visual representations. We release our source
code and models at https://github.com/openai/vdvae.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:35:31 GMT""},{""version"":""v2"",""created"":""Tue, 16 Mar 2021 18:33:19 GMT""}]","2021-03-18"
"2011.10651","Marcin Bownik","Marcin Bownik, Li-An Daniel Wang","A PDE Characterization of Anisotropic Hardy Spaces",,,,,"math.CA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We obtain a differential characterization for the anisotropic Hardy space
$H_A^p$ by identifying it with a parabolic Hardy space associated with a
general continuous group. This allows $H_A^p$ to be defined using a parabolic
differential equation of Calderon and Torchinsky. We also provide a
classification of dilations corresponding to equivalent anisotropic Hardy
spaces with respect to linear transformations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:37:21 GMT""}]","2020-11-24"
"2011.10652","Aparna Khare","Aparna Khare, Srinivas Parthasarathy, Shiva Sundaram","Self-Supervised learning with cross-modal transformers for emotion
  recognition","To appear in SLT2020",,"10.1109/SLT48900.2021.9383618",,"cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Emotion recognition is a challenging task due to limited availability of
in-the-wild labeled datasets. Self-supervised learning has shown improvements
on tasks with limited labeled datasets in domains like speech and natural
language. Models such as BERT learn to incorporate context in word embeddings,
which translates to improved performance in downstream tasks like question
answering. In this work, we extend self-supervised training to multi-modal
applications. We learn multi-modal representations using a transformer trained
on the masked language modeling task with audio, visual and text features. This
model is fine-tuned on the downstream task of emotion recognition. Our results
on the CMU-MOSEI dataset show that this pre-training technique can improve the
emotion recognition performance by up to 3% compared to the baseline.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:38:34 GMT""}]","2021-04-08"
"2011.10653","Abe Leite","Abe Leite and Sa\'ul A. Blanco","Effects of Human vs. Automatic Feedback on Students' Understanding of AI
  Concepts and Programming Style","Published in SIGCSE '20: Proceedings of the 51st ACM Technical
  Symposium on Computer Science Education","SIGCSE '20: Proceedings of the 51st ACM Technical Symposium on
  Computer Science Education (Feb 2020) 44-50","10.1145/3328778.3366921",,"cs.HC cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The use of automatic grading tools has become nearly ubiquitous in large
undergraduate programming courses, and recent work has focused on improving the
quality of automatically generated feedback. However, there is a relative lack
of data directly comparing student outcomes when receiving computer-generated
feedback and human-written feedback. This paper addresses this gap by splitting
one 90-student class into two feedback groups and analyzing differences in the
two cohorts' performance. The class is an intro to AI with programming HW
assignments. One group of students received detailed computer-generated
feedback on their programming assignments describing which parts of the
algorithms' logic was missing; the other group additionally received
human-written feedback describing how their programs' syntax relates to issues
with their logic, and qualitative (style) recommendations for improving their
code. Results on quizzes and exam questions suggest that human feedback helps
students obtain a better conceptual understanding, but analyses found no
difference between the groups' ability to collaborate on the final project. The
course grade distribution revealed that students who received human-written
feedback performed better overall; this effect was the most pronounced in the
middle two quartiles of each group. These results suggest that feedback about
the syntax-logic relation may be a primary mechanism by which human feedback
improves student outcomes.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:40:32 GMT""}]","2020-11-24"
"2011.10654","Soumick Chatterjee","Dhanunjaya Mitta, Soumick Chatterjee, Oliver Speck and Andreas
  N\""urnberger","Upgraded W-Net with Attention Gates and its Application in Unsupervised
  3D Liver Segmentation",,"Proceedings of the 10th International Conference on Pattern
  Recognition Applications and Methods 2021 - Volume 1","10.5220/0010221504880494","ICPRAM, ISBN 978-989-758-486-2, pages 488-494","eess.IV cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Segmentation of biomedical images can assist radiologists to make a better
diagnosis and take decisions faster by helping in the detection of
abnormalities, such as tumors. Manual or semi-automated segmentation, however,
can be a time-consuming task. Most deep learning based automated segmentation
methods are supervised and rely on manually segmented ground-truth. A possible
solution for the problem would be an unsupervised deep learning based approach
for automated segmentation, which this research work tries to address. We use a
W-Net architecture and modified it, such that it can be applied to 3D volumes.
In addition, to suppress noise in the segmentation we added attention gates to
the skip connections. The loss for the segmentation output was calculated using
soft N-Cuts and for the reconstruction output using SSIM. Conditional Random
Fields were used as a post-processing step to fine-tune the results. The
proposed method has shown promising results, with a dice coefficient of 0.88
for the liver segmentation compared against manual segmentation.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:45:28 GMT""}]","2021-02-11"
"2011.10665","Shuocheng Guo","Shuocheng Guo, Xinwu Qian, Jun Liu","Charging-as-a-Service: On-demand battery delivery for light-duty
  electric vehicles for mobility service","28 pages",,,,"math.OC physics.soc-ph","http://creativecommons.org/licenses/by/4.0/","  This study presents an innovative solution for powering electric vehicles,
named Charging-as-a-Service (CaaS), that concerns the potential large-scale
adoption of light-duty electric vehicles (LDEV) in the Mobility-as-a-Service
(MaaS) industry. Analogous to the MaaS, the core idea of the CaaS is to
dispatch service vehicles (SVs) that carry modular battery units (MBUs) to
provide LDEVs for mobility service with on-demand battery delivery. The CaaS
system is expected to tackle major bottlenecks of a large-scale LDEV adoption
in the MaaS industry due to the lack of charging infrastructure and excess
waiting and charging time. A hybrid agent-based simulation model (HABM) is
developed to model the dynamics of the CaaS system with SV agents, and a
trip-based stationary charging probability distribution is introduced to
simulate the generation of charging demand for LDEVs. Two dispatching
algorithms are further developed to support the optimal operation of the CaaS.
The model is validated by assuming electrifying all 13,000 yellow taxis in New
York City (NYC) that follow the same daily trip patterns. Multiple scenarios
are analyzed under various SV fleet sizes and dispatching strategies. The
results suggest that optimal deployment of 250 SVs may serve the LDEV fleet in
NYC with an average waiting time of 5 minutes, save the travel distance at over
50 miles per minute, and gain considerable profits of up to $50 per minute.
This study offers significant insights into the feasibility, service
efficiency, and financial sustainability for deploying city-wide CaaS systems
to power the electric MaaS industry.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:06:05 GMT""}]","2020-11-25"
"2011.10666","Lily Xu","Rachel Guo, Lily Xu, Drew Cronin, Francis Okeke, Andrew Plumptre,
  Milind Tambe","Enhancing Poaching Predictions for Under-Resourced Wildlife Conservation
  Parks Using Remote Sensing Imagery","Presented at NeurIPS 2020 Workshop on Machine Learning for the
  Developing World. 4 pages, 1 page references. 4 figures, 1 table",,,,"cs.LG cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Illegal wildlife poaching is driving the loss of biodiversity. To combat
poaching, rangers patrol expansive protected areas for illegal poaching
activity. However, rangers often cannot comprehensively search such large
parks. Thus, the Protection Assistant for Wildlife Security (PAWS) was
introduced as a machine learning approach to help identify the areas with
highest poaching risk. As PAWS is deployed to parks around the world, we
recognized that many parks have limited resources for data collection and
therefore have scarce feature sets. To ensure under-resourced parks have access
to meaningful poaching predictions, we introduce the use of publicly available
remote sensing data to extract features for parks. By employing this data from
Google Earth Engine, we also incorporate previously unavailable dynamic data to
enrich predictions with seasonal trends. We automate the entire
data-to-deployment pipeline and find that, with only using publicly available
data, we recuperate prediction performance comparable to predictions made using
features manually computed by park specialists. We conclude that the inclusion
of satellite imagery creates a robust system through which parks of any
resource level can benefit from poaching risks for years to come.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:06:57 GMT""}]","2020-11-24"
"2011.10667","Francois Fillion-Gourdeau","F. Fillion-Gourdeau, E. Lorin and S. MacLean","Numerical quasi-conformal transformations for electron dynamics on
  strained graphene surfaces","19 pages, 8 figures","Phys. Rev. E 103, 013312 (2021)","10.1103/PhysRevE.103.013312",,"physics.comp-ph cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The dynamics of low energy electrons in general static strained graphene
surface is modelled mathematically by the Dirac equation in curved space-time.
In Cartesian coordinates, a parametrization of the surface can be
straightforwardly obtained, but the resulting Dirac equation is intricate for
general surface deformations. Two different strategies are introduced to
simplify this problem: the diagonal metric approximation and the change of
variables to isothermal coordinates. These coordinates are obtained from
quasi-conformal transformations characterized by the Beltrami equation, whose
solution gives the mapping between both coordinate systems. To implement this
second strategy, a least square finite-element numerical scheme is introduced
to solve the Beltrami equation. The Dirac equation is then solved via an
accurate pseudo-spectral numerical method in the pseudo-Hermitian
representation that is endowed with explicit unitary evolution and conservation
of the norm. The two approaches are compared and applied to the scattering of
electrons on Gaussian shaped graphene surface deformations. It is demonstrated
that electron wave packets can be focused by these local strained regions.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:15:19 GMT""}]","2021-01-27"
"2011.10668","Hotae Lee","Hotae Lee, Monimoy Bujarbaruah, and Francesco Borrelli","Learning How to Solve Bubble Ball","Accepted to L4DC 2021",,,,"eess.SY cs.LG cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  ""Bubble Ball"" is a game built on a 2D physics engine, where a finite set of
objects can modify the motion of a bubble-like ball. The objective is to choose
the set and the initial configuration of the objects, in order to get the ball
to reach a target flag. The presence of obstacles, friction, contact forces and
combinatorial object choices make the game hard to solve. In this paper, we
propose a hierarchical predictive framework which solves Bubble Ball.
Geometric, kinematic and dynamic models are used at different levels of the
hierarchy. At each level of the game, data collected during failed iterations
are used to update models at all hierarchical level and converge to a feasible
solution to the game. The proposed approach successfully solves a large set of
Bubble Ball levels within reasonable number of trials. This proposed framework
can also be used to solve other physics-based games, especially with limited
training data from human demonstrations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:16:00 GMT""},{""version"":""v2"",""created"":""Thu, 29 Apr 2021 00:33:29 GMT""}]","2021-04-30"
"2011.10669","James Hare","James Z. Hare, Cesar A. Uribe, Lance Kaplan, Ali Jadbabaie","A General Framework for Distributed Inference with Uncertain Models",,,,,"cs.AI cs.MA cs.SI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper studies the problem of distributed classification with a network
of heterogeneous agents. The agents seek to jointly identify the underlying
target class that best describes a sequence of observations. The problem is
first abstracted to a hypothesis-testing framework, where we assume that the
agents seek to agree on the hypothesis (target class) that best matches the
distribution of observations. Non-Bayesian social learning theory provides a
framework that solves this problem in an efficient manner by allowing the
agents to sequentially communicate and update their beliefs for each hypothesis
over the network. Most existing approaches assume that agents have access to
exact statistical models for each hypothesis. However, in many practical
applications, agents learn the likelihood models based on limited data, which
induces uncertainty in the likelihood function parameters. In this work, we
build upon the concept of uncertain models to incorporate the agents'
uncertainty in the likelihoods by identifying a broad set of parametric
distribution that allows the agents' beliefs to converge to the same result as
a centralized approach. Furthermore, we empirically explore extensions to
non-parametric models to provide a generalized framework of uncertain models in
non-Bayesian social learning.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:17:12 GMT""}]","2020-11-24"
"2011.10670","Junwei Liang","Junwei Liang","From Recognition to Prediction: Analysis of Human Action and Trajectory
  Prediction in Video","Ph.D. Thesis. Version 2: Defense. See here:
  https://junweiliang.github.io/thesis/",,,,"cs.CV","http://creativecommons.org/licenses/by/4.0/","  With the advancement in computer vision deep learning, systems now are able
to analyze an unprecedented amount of rich visual information from videos to
enable applications such as autonomous driving, socially-aware robot assistant
and public safety monitoring. Deciphering human behaviors to predict their
future paths/trajectories and what they would do from videos is important in
these applications. However, human trajectory prediction still remains a
challenging task, as scene semantics and human intent are difficult to model.
Many systems do not provide high-level semantic attributes to reason about
pedestrian future. This design hinders prediction performance in video data
from diverse domains and unseen scenarios. To enable optimal future human
behavioral forecasting, it is crucial for the system to be able to detect and
analyze human activities as well as scene semantics, passing informative
features to the subsequent prediction module for context understanding.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:23:34 GMT""},{""version"":""v2"",""created"":""Thu, 1 Jul 2021 16:22:59 GMT""},{""version"":""v3"",""created"":""Fri, 16 Jul 2021 13:45:43 GMT""}]","2021-07-19"
"2011.10671","Di Feng","Di Feng, Ali Harakeh, Steven Waslander, Klaus Dietmayer","A Review and Comparative Study on Probabilistic Object Detection in
  Autonomous Driving","Accepted in the IEEE Transactions on Intelligent Transportation
  Systems",,"10.1109/TITS.2021.3096854",,"cs.CV cs.RO","http://creativecommons.org/licenses/by/4.0/","  Capturing uncertainty in object detection is indispensable for safe
autonomous driving. In recent years, deep learning has become the de-facto
approach for object detection, and many probabilistic object detectors have
been proposed. However, there is no summary on uncertainty estimation in deep
object detection, and existing methods are not only built with different
network architectures and uncertainty estimation methods, but also evaluated on
different datasets with a wide range of evaluation metrics. As a result, a
comparison among methods remains challenging, as does the selection of a model
that best suits a particular application. This paper aims to alleviate this
problem by providing a review and comparative study on existing probabilistic
object detection methods for autonomous driving applications. First, we provide
an overview of generic uncertainty estimation in deep learning, and then
systematically survey existing methods and evaluation metrics for probabilistic
object detection. Next, we present a strict comparative study for probabilistic
object detection based on an image detector and three public autonomous driving
datasets. Finally, we present a discussion of the remaining challenges and
future works. Code has been made available at
https://github.com/asharakeh/pod_compare.git
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:30:36 GMT""},{""version"":""v2"",""created"":""Sun, 11 Jul 2021 07:04:10 GMT""}]","2021-07-13"
"2011.10672","Johannes Schneider","Johannes Schneider and Rene Abraham and Christian Meske and Jan vom
  Brocke","AI Governance for Businesses",,"Information Systems Management, 2022","10.1080/10580530.2022.2085825",,"cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Artificial Intelligence (AI) governance regulates the exercise of authority
and control over the management of AI. It aims at leveraging AI through
effective use of data and minimization of AI-related cost and risk. While
topics such as AI governance and AI ethics are thoroughly discussed on a
theoretical, philosophical, societal and regulatory level, there is limited
work on AI governance targeted to companies and corporations. This work views
AI products as systems, where key functionality is delivered by machine
learning (ML) models leveraging (training) data. We derive a conceptual
framework by synthesizing literature on AI and related fields such as ML. Our
framework decomposes AI governance into governance of data, (ML) models and
(AI) systems along four dimensions. It relates to existing IT and data
governance frameworks and practices. It can be adopted by practitioners and
academics alike. For practitioners the synthesis of mainly research papers, but
also practitioner publications and publications of regulatory bodies provides a
valuable starting point to implement AI governance, while for academics the
paper highlights a number of areas of AI governance that deserve more
attention.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:31:37 GMT""},{""version"":""v2"",""created"":""Sun, 26 Jun 2022 20:52:22 GMT""}]","2022-06-28"
"2011.10673","Diego Dominici","Diego Dominici","Orthogonality of the Dickson polynomials of the (k+1)-th kind",,,,,"math.CA math.AC","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We study the Dickson polynomials of the (k+1)-th kind over the field of
complex numbers. We show that they are a family of co-recursive orthogonal
polynomials with respect to a quasi-definite moment functional L_{k}. We find
an integral representation for L_{k} and compute explicit expressions for all
of its moments.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:43:28 GMT""}]","2020-11-24"
"2011.10674","Anton Xue","Anton Xue and Nikolai Matni","Data-Driven System Level Synthesis",,,,,"math.OC cs.LG cs.SY eess.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We establish data-driven versions of the System Level Synthesis (SLS)
parameterization of achievable closed-loop system responses for a
linear-time-invariant system over a finite-horizon. Inspired by recent work in
data-driven control that leverages tools from behavioral theory, we show that
optimization problems over system-responses can be posed using only libraries
of past system trajectories, without explicitly identifying a system model. We
first consider the idealized setting of noise free trajectories, and show an
exact equivalence between traditional and data-driven SLS. We then show that in
the case of a system driven by process noise, tools from robust SLS can be used
to characterize the effects of noise on closed-loop performance, and further
draw on tools from matrix concentration to show that a simple trajectory
averaging technique can be used to mitigate these effects. We end with
numerical experiments showing the soundness of our methods.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:52:29 GMT""},{""version"":""v2"",""created"":""Tue, 2 Mar 2021 08:05:39 GMT""},{""version"":""v3"",""created"":""Sat, 6 Mar 2021 19:42:49 GMT""}]","2021-03-09"
"2011.10675","Cristina Vasconcelos","Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin, Nicolas Le
  Roux, Ross Goroshin","An Effective Anti-Aliasing Approach for Residual Networks",,,,,"cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Image pre-processing in the frequency domain has traditionally played a vital
role in computer vision and was even part of the standard pipeline in the early
days of deep learning. However, with the advent of large datasets, many
practitioners concluded that this was unnecessary due to the belief that these
priors can be learned from the data itself. Frequency aliasing is a phenomenon
that may occur when sub-sampling any signal, such as an image or feature map,
causing distortion in the sub-sampled output. We show that we can mitigate this
effect by placing non-trainable blur filters and using smooth activation
functions at key locations, particularly where networks lack the capacity to
learn them. These simple architectural changes lead to substantial improvements
in out-of-distribution generalization on both image classification under
natural corruptions on ImageNet-C [10] and few-shot learning on Meta-Dataset
[17], without introducing additional trainable parameters and using the default
hyper-parameters of open source codebases.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:55:57 GMT""}]","2020-11-24"
"2011.10676","Jean-Claude Ndogmo","J. C. Ndogmo","Lie group classification and conservation laws of a class of hyperbolic
  equations","25 pages, 2 tables",,,,"math.AP","http://creativecommons.org/licenses/by-nc-nd/4.0/","  A new method for the Lie group classification of differential equations is
proposed. It is based of the determination of all possible cases of linear
dependence of certain indeterminate appearing in the determining equations of
symmetries of the equation. The method is simple and systematic and applied to
a family of hyperbolic equations. Moreover, as the said family contains several
known equations with important physical applications, low-order conservation
laws of some relevant equations from the family are computed, and the results
obtained are discussed with regard to the symmetry integrability of a
particular class from the underlying family of hyperbolic equations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 23:01:29 GMT""}]","2020-11-24"
"2011.10677","David Doty","David Haley and David Doty","Computing properties of thermodynamic binding networks: An integer
  programming approach",,,,,"cs.ET","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The thermodynamic binding networks (TBN) model is a tool for studying
engineered molecular systems. The TBN model allows one to reason about their
behavior through a simplified abstraction that ignores details about molecular
composition, focusing on two key determinants of a system's energetics common
to any chemical substrate: how many molecular bonds are formed, and how many
separate complexes exist in the system. We formulate as an integer program the
NP-hard problem of computing stable (a.k.a., minimum energy) configurations of
a TBN: those configurations that maximize the number of bonds and complexes. We
provide open-source software solving this integer program. We give empirical
evidence that this approach enables dramatically faster computation of TBN
stable configurations than previous approaches based on SAT solvers.
Furthermore, unlike SAT-based approaches, our integer programming formulation
can reason about TBNs in which some molecules have unbounded counts. These
improvements in turn allow us to efficiently automate verification of desired
properties of practical TBNs. Finally, we show that the TBN has a natural
representation with a unique Hilbert basis describing the ""fundamental
components"" out of which locally minimal energy configurations are composed.
This characterization helps verify correctness of not only stable
configurations, but entire ""kinetic pathways"" in a TBN.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 23:01:40 GMT""},{""version"":""v2"",""created"":""Wed, 12 May 2021 01:42:44 GMT""}]","2021-05-13"
"2011.10678","Alireza Zareian","Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, Shih-Fu Chang","Open-Vocabulary Object Detection Using Captions","To be presented at CVPR 2021 (oral paper)",,,,"cs.CV cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Despite the remarkable accuracy of deep neural networks in object detection,
they are costly to train and scale due to supervision requirements.
Particularly, learning more object categories typically requires proportionally
more bounding box annotations. Weakly supervised and zero-shot learning
techniques have been explored to scale object detectors to more categories with
less supervision, but they have not been as successful and widely adopted as
supervised models. In this paper, we put forth a novel formulation of the
object detection problem, namely open-vocabulary object detection, which is
more general, more practical, and more effective than weakly supervised and
zero-shot approaches. We propose a new method to train object detectors using
bounding box annotations for a limited set of object categories, as well as
image-caption pairs that cover a larger variety of objects at a significantly
lower cost. We show that the proposed method can detect and localize objects
for which no bounding box annotation is provided during training, at a
significantly higher accuracy than zero-shot approaches. Meanwhile, objects
with bounding box annotation can be detected almost as accurately as supervised
methods, which is significantly better than weakly supervised baselines.
Accordingly, we establish a new state of the art for scalable object detection.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 23:05:46 GMT""},{""version"":""v2"",""created"":""Sun, 14 Mar 2021 18:45:04 GMT""}]","2021-03-16"
"2011.10679","Chang Liu","Godwin Enemali, Rui Zhang, Hugh McCann, Chang Liu","Cost-Effective Quasi-Parallel Sensing Instrumentation for Industrial
  Chemical Species Tomography","Submitted to IEEE Transactions on Industrial Electronics",,,,"eess.SY cs.SY eess.SP physics.ins-det","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Chemical Species Tomography (CST) has been widely applied for imaging of
critical gas-phase parameters in industrial processes. To acquire high-fidelity
images, CST is typically implemented by line-of-sight Wavelength Modulation
Spectroscopy (WMS) measurements from multiple laser beams. The modulated
transmission signal on each laser beam needs to be a) digitised by a high-speed
analogue-to-digital converter (ADC); b) demodulated by a digital lock-in (DLI)
module; and c) transferred to high-level processor for image reconstruction.
Although a fully parallel data acquisition (DAQ) and signal processing system
can achieve these functionalities with maximised temporal response, it leads to
a highly complex, expensive and power-consuming instrumentation system with
high potential for inconsistency between the sampled beams due to the
electronics alone. In addition, the huge amount of spectral data sampled in
parallel significantly burdens the communication process in industrial
applications where in situ signal digitisation is distanced from the high-level
data processing. To address these issues, a quasi-parallel sensing technique
and electronic circuits were developed for industrial CST, in which the
digitisation and demodulation of the multi-beam transmission signals are
multiplexed over the high-frequency modulation within a wavelength scan. Our
development not only maintains the temporal response of the fully parallel
sensing scheme, but also facilitates the cost-effective implementation of
industrial CST with very low complexity and reduced load on data transfer. The
proposed technique is analytically proven, numerically examined by
noise-contaminated CST simulations, and experimentally validated using a
lab-scale CST system with 32 laser beams.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 23:06:35 GMT""}]","2020-11-24"
"2011.10680","Amir Gholami","Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric
  Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W. Mahoney, Kurt Keutzer","HAWQV3: Dyadic Neural Network Quantization",,"ICML 2021",,,"cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Current low-precision quantization algorithms often have the hidden cost of
conversion back and forth from floating point to quantized integer values. This
hidden cost limits the latency improvement realized by quantizing Neural
Networks. To address this, we present HAWQV3, a novel mixed-precision
integer-only quantization framework. The contributions of HAWQV3 are the
following: (i) An integer-only inference where the entire computational graph
is performed only with integer multiplication, addition, and bit shifting,
without any floating point operations or even integer division; (ii) A novel
hardware-aware mixed-precision quantization method where the bit-precision is
calculated by solving an integer linear programming problem that balances the
trade-off between model perturbation and other constraints, e.g., memory
footprint and latency; (iii) Direct hardware deployment and open source
contribution for 4-bit uniform/mixed-precision quantization in TVM, achieving
an average speed up of $1.45\times$ for uniform 4-bit, as compared to uniform
8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed
methods on ResNet18/50 and InceptionV3, for various model compression levels
with/without mixed precision. For ResNet50, our INT8 quantization achieves an
accuracy of $77.58\%$, which is $2.68\%$ higher than prior integer-only work,
and our mixed-precision INT4/8 quantization can reduce INT8 latency by $23\%$
and still achieve $76.73\%$ accuracy. Our framework and the TVM implementation
have been open sourced.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 23:51:43 GMT""},{""version"":""v2"",""created"":""Mon, 15 Feb 2021 06:43:10 GMT""},{""version"":""v3"",""created"":""Wed, 23 Jun 2021 07:49:12 GMT""}]","2021-06-24"
"2011.10681","Xiaochu Wang","Xiaochu Wang and Wenyuan Tang","Analysis and Evaluation of Baseline Manipulation in Demand Response
  Programs",,,,,"eess.SY cs.SY","http://creativecommons.org/licenses/by/4.0/","  The customer baseline is required to assign rebates to participants in
baseline-based demand response (DR) programs. The average baseline method has
been widely accepted in practice due to its simplicity and reliability.
However, the customer's baseline manipulation is little-known in the
literature. We start from a customer's perspective and establish a Markov
decision process to model the customer's payoff-maximizing problem. The
behavior of a rational customer's underconsumption on DR days and
overconsumption on non-DR days are revealed. Furthermore, we propose an
approximated baseline method and show how the consumption distribution and
program parameters affect the results. Due to the curse of dimensionality, a
linear policy-based rollout algorithm is introduced to obtain a practical
approximate solution. Finally, a case study is carried out to illustrate the
baseline manipulation, where the simulation results confirm the effectiveness
of the proposed methods and shed light on how to properly design baseline
methods.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 23:55:06 GMT""}]","2020-11-24"
"2011.10682","Bolin Gao","Bolin Gao, Lacra Pavel","Continuous-Time Convergence Rates in Potential and Monotone Games","20 pages, 5 figures, manuscript submitted to SIAM Journal on Control
  and Optimization (SICON) for possible publication",,,,"math.OC cs.GT cs.MA cs.SY eess.SY math.DS","http://creativecommons.org/licenses/by/4.0/","  In this paper, we provide exponential rates of convergence to the interior
Nash equilibrium for continuous-time dual-space game dynamics such as mirror
descent (MD) and actor-critic (AC). We perform our analysis in $N$-player
continuous concave games that satisfy certain monotonicity assumptions while
possibly also admitting potential functions. In the first part of this paper,
we provide a novel relative characterization of monotone games and show that MD
and its discounted version converge with $\mathcal{O}(e^{-\beta t})$ in
relatively strongly and relatively hypo-monotone games, respectively. In the
second part of this paper, we specialize our results to games that admit a
relatively strongly concave potential and show AC converges with
$\mathcal{O}(e^{-\beta t})$. These rates extend their known convergence
conditions. Simulations are performed which empirically back up our results.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 00:00:55 GMT""},{""version"":""v2"",""created"":""Wed, 28 Jul 2021 00:15:23 GMT""},{""version"":""v3"",""created"":""Thu, 3 Feb 2022 03:02:07 GMT""}]","2022-02-04"
"2011.10683","Vrindavan Harrison","Vrindavan Harrison, Juraj Juraska, Wen Cui, Lena Reed, Kevin K.
  Bowden, Jiaqi Wu, Brian Schwarzmann, Abteen Ebrahimi, Rishi Rajasekaran,
  Nikhil Varghese, Max Wechsler-Azen, Steve Whittaker, Jeffrey Flanigan, and
  Marilyn Walker","Athena: Constructing Dialogues Dynamically with Discourse Constraints","3rd Proceedings of Alexa Prize (Alexa Prize 2019)",,,,"cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This report describes Athena, a dialogue system for spoken conversation on
popular topics and current events. We develop a flexible topic-agnostic
approach to dialogue management that dynamically configures dialogue based on
general principles of entity and topic coherence. Athena's dialogue manager
uses a contract-based method where discourse constraints are dispatched to
clusters of response generators. This allows Athena to procure responses from
dynamic sources, such as knowledge graph traversals and feature-based
on-the-fly response retrieval methods. After describing the dialogue system
architecture, we perform an analysis of conversations that Athena participated
in during the 2019 Alexa Prize Competition. We conclude with a report on
several user studies we carried out to better understand how individual user
characteristics affect system ratings.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 00:28:34 GMT""}]","2020-11-24"
"2011.10684","Haozhe Feng","Hao-Zhe Feng, Kezhi Kong, Minghao Chen, Tianye Zhang, Minfeng Zhu, Wei
  Chen","SHOT-VAE: Semi-supervised Deep Generative Models With Label-aware ELBO
  Approximations","12 pages, 6 figures, Accepted for presentation at AAAI2021",,,,"cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Semi-supervised variational autoencoders (VAEs) have obtained strong results,
but have also encountered the challenge that good ELBO values do not always
imply accurate inference results. In this paper, we investigate and propose two
causes of this problem: (1) The ELBO objective cannot utilize the label
information directly. (2) A bottleneck value exists and continuing to optimize
ELBO after this value will not improve inference accuracy. On the basis of the
experiment results, we propose SHOT-VAE to address these problems without
introducing additional prior knowledge. The SHOT-VAE offers two contributions:
(1) A new ELBO approximation named smooth-ELBO that integrates the label
predictive loss into ELBO. (2) An approximation based on optimal interpolation
that breaks the ELBO value bottleneck by reducing the margin between ELBO and
the data likelihood. The SHOT-VAE achieves good performance with a 25.30% error
rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11% on
CIFAR-10 with 4k labels.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 00:38:31 GMT""},{""version"":""v2"",""created"":""Wed, 2 Dec 2020 09:27:54 GMT""},{""version"":""v3"",""created"":""Thu, 3 Dec 2020 07:44:59 GMT""},{""version"":""v4"",""created"":""Tue, 8 Dec 2020 07:04:44 GMT""}]","2020-12-09"
"2011.10685","Ross Glandon Jr.","Ross Glandon, Mahesh Narayanamurthi, Adrian Sandu","Linearly Implicit Multistep Methods for Time Integration","36 pages, 5 figures, submitted to SISC in May 2020",,,"CSL-TR-19-13","math.NA cs.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Time integration methods for solving initial value problems are an important
component of many scientific and engineering simulations. Implicit time
integrators are desirable for their stability properties, significantly
relaxing restrictions on timestep size. However, implicit methods require
solutions to one or more systems of nonlinear equations at each timestep, which
for large simulations can be prohibitively expensive. This paper introduces a
new family of linearly implicit multistep methods (LIMM), which only requires
the solution of one linear system per timestep. Order conditions and stability
theory for these methods are presented, as well as design and implementation
considerations. Practical methods of order up to five are developed that have
similar error coefficients, but improved stability regions, when compared to
the widely used BDF methods. Numerical testing of a self-starting variable
stepsize and variable order implementation of the new LIMM methods shows
measurable performance improvement over a similar BDF implementation.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 00:52:27 GMT""}]","2020-11-24"
"2011.10686","Kazuya Shinjo","Kazuya Shinjo, Shigetoshi Sota, and Takami Tohyama","Effect of phase string on single-hole dynamics in the two-leg Hubbard
  ladder","14 pages, 9 figures","Phys. Rev. B 103, 035141 (2021)","10.1103/PhysRevB.103.035141",,"cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Optical measurements in doped Mott insulators have discovered the emergence
of spectral weights at mid-infrared (MIR) upon chemical doping and photodoping.
MIR weights may have a relation to string-type excitation of spins, which is
induced by a doped hole generating misarranged spins with respect to their
sublattice. There are two types of string effects: one is an $S^z$ string that
is repairable by quantum spin flips and the other is a phase string irreparable
by the spin flips. We investigate the effect of $S^{z}$ and phase strings on
MIR weights. Calculating the optical conductivity of the single-hole Hubbard
model in the strong-coupling regime and the $t$-$J$ model on two-leg ladders by
using time-dependent Lanczos and density-matrix renormalization group, we find
that phase strings make a crucial effect on the emergence of MIR weights as
compared with $S^{z}$ strings. Our findings indicate that a mutual Chern-Simons
gauge field acting between spin and charge degrees of freedom, which is the
origin of phase strings, is significant for obtaining MIR weights. Conversely,
if we remove this gauge field, no phase is picked up by a doped hole. As a
result, a spin-polaron accompanied by a local spin distortion emerges and a
quasiparticle with a cosine-like energy dispersion is formed in single-particle
spectral function. Furthermore, we suggest a Floquet engineering to examine the
phase-string effect in cold atoms.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:01:25 GMT""},{""version"":""v2"",""created"":""Tue, 26 Jan 2021 11:50:06 GMT""}]","2021-01-27"
"2011.10687","Gowri Somanath","Gowri Somanath and Daniel Kurz","HDR Environment Map Estimation for Real-Time Augmented Reality","Supplementary video at
  https://docs-assets.developer.apple.com/ml-research/papers/hdr-environment-map.mp4
  Code at https://github.com/apple/ml-envmapnet Accepted to CVPR 2021",,,,"cs.CV cs.AI cs.GR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:01:53 GMT""},{""version"":""v2"",""created"":""Fri, 26 Mar 2021 19:32:32 GMT""},{""version"":""v3"",""created"":""Fri, 14 May 2021 21:32:49 GMT""},{""version"":""v4"",""created"":""Tue, 22 Jun 2021 22:15:31 GMT""},{""version"":""v5"",""created"":""Tue, 27 Jul 2021 20:48:22 GMT""}]","2021-07-29"
"2011.10688","Xinwei Yao","Xinwei Yao, Ohad Fried, Kayvon Fatahalian, Maneesh Agrawala","Iterative Text-based Editing of Talking-heads Using Neural Retargeting","Project Website is https://davidyao.me/projects/text2vid",,,,"cs.CV cs.GR cs.MM","http://creativecommons.org/licenses/by/4.0/","  We present a text-based tool for editing talking-head video that enables an
iterative editing workflow. On each iteration users can edit the wording of the
speech, further refine mouth motions if necessary to reduce artifacts and
manipulate non-verbal aspects of the performance by inserting mouth gestures
(e.g. a smile) or changing the overall performance style (e.g. energetic,
mumble). Our tool requires only 2-3 minutes of the target actor video and it
synthesizes the video for each iteration in about 40 seconds, allowing users to
quickly explore many editing possibilities as they iterate. Our approach is
based on two key ideas. (1) We develop a fast phoneme search algorithm that can
quickly identify phoneme-level subsequences of the source repository video that
best match a desired edit. This enables our fast iteration loop. (2) We
leverage a large repository of video of a source actor and develop a new
self-supervised neural retargeting technique for transferring the mouth motions
of the source actor to the target actor. This allows us to work with relatively
short target actor videos, making our approach applicable in many real-world
editing scenarios. Finally, our refinement and performance controls give users
the ability to further fine-tune the synthesized results.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:05:55 GMT""}]","2020-11-24"
"2011.10689","Fumihiko Nakamura","Fumihiko Nakamura, Michael C. Mackey","Asymptotic (statistical) periodicity in two-dimensional maps","17 pages, 6 figures",,,,"math.DS","http://creativecommons.org/licenses/by/4.0/","  In this paper we give a new sufficient condition for asymptotic periodicity
of Frobenius-Perron operator corresponding to two--dimensional maps. The result
of the asymptotic periodicity for strictly expanding systems, that is, all
eigenvalues of the system are greater than one, in a high-dimensional dynamical
systems was already known. Our new theorem enables to apply for the system
having an eigenvalue smaller than one. The key idea for the proof is a function
of bounded variation defined by line integration. Finally, we introduce a new
two-dimensional dynamical system exhibiting the asymptotic periodicity with
different periods depending on parameter values, and discuss to apply our
theorem to the model.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:08:46 GMT""},{""version"":""v2"",""created"":""Thu, 10 Jun 2021 03:35:25 GMT""},{""version"":""v3"",""created"":""Tue, 3 Aug 2021 05:24:56 GMT""}]","2021-08-04"
"2011.10690","Parshan Pakiman","Boxiao Chen, Selvaprabu Nadarajah, Parshan Pakiman, Stefanus Jasin","Self-adapting Robustness in Demand Learning",,,,,"cs.LG cs.IR","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We study dynamic pricing over a finite number of periods in the presence of
demand model ambiguity. Departing from the typical no-regret learning
environment, where price changes are allowed at any time, pricing decisions are
made at pre-specified points in time and each price can be applied to a large
number of arrivals. In this environment, which arises in retailing, a pricing
decision based on an incorrect demand model can significantly impact cumulative
revenue. We develop an adaptively-robust-learning (ARL) pricing policy that
learns the true model parameters from the data while actively managing demand
model ambiguity. It optimizes an objective that is robust with respect to a
self-adapting set of demand models, where a given model is included in this set
only if the sales data revealed from prior pricing decisions makes it
""probable"". As a result, it gracefully transitions from being robust when
demand model ambiguity is high to minimizing regret when this ambiguity
diminishes upon receiving more data. We characterize the stochastic behavior of
ARL's self-adapting ambiguity sets and derive a regret bound that highlights
the link between the scale of revenue loss and the customer arrival pattern. We
also show that ARL, by being conscious of both model ambiguity and revenue,
bridges the gap between a distributionally robust policy and a
follow-the-leader policy, which focus on model ambiguity and revenue,
respectively. We numerically find that the ARL policy, or its extension
thereof, exhibits superior performance compared to distributionally robust,
follow-the-leader, and upper-confidence-bound policies in terms of expected
revenue and/or value at risk.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:15:54 GMT""}]","2020-11-24"
"2011.10691","Atsushi Nakayashiki","Atsushi Nakayashiki","Tau Functions of (n,1) curves and Soliton Solutions on Non-Zero Constant
  Backgrounds","27 pages, no figure",,"10.1007/s11005-021-01411-3",,"nlin.SI math-ph math.AG math.MP math.QA","http://creativecommons.org/licenses/by/4.0/","  We study the tau function of the KP-hierarchy associated with an (n,1) curve
$y^n=x-\alpha$. If $\alpha=0$ the corresponding tau function is 1. On the other
hand if $\alpha\neq 0$ the tau function becomes the exponential of a quadratic
function of the time variables. By applying vertex opertaors to the latter we
obtain soliton solutions on non-zero constant backgrounds.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:23:11 GMT""}]","2021-07-14"
"2011.10692","Ade Irma Suriajaya Ph.D.","Athanasios Sourmelidis, J\""orn Steuding, and Ade Irma Suriajaya","Riemann-Type Functional Equations -- Julia Line and Counting Formulae --","28 pages, a part of the original version uploaded last year","Indag. Math. (2022)","10.1016/j.indag.2022.08.002","RIKEN-iTHEMS-Report-21","math.NT math.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study Riemann-type functional equations with respect to value-distribution
theory and derive implications for their solutions. In particular, for a fixed
complex number $a\neq0$ and a function from the Selberg class $\mathcal{L}$, we
prove a Riemann-von Mangoldt formula for the number of a-points of the
$\Delta$-factor of the functional equation of $\mathcal{L}$ and an analog of
Landau's formula over these points. From the last formula we derive that the
ordinates of these $a$-points are uniformly distributed modulo one. Lastly, we
show the existence of the mean-value of the values of $\mathcal{L}(s)$ taken at
these points.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:27:43 GMT""},{""version"":""v2"",""created"":""Wed, 13 Oct 2021 02:11:26 GMT""}]","2022-10-07"
"2011.10693","Jordan Broussard","Jordan Broussard","Templates, Arrays, and Overlays",,,,,"math.CO","http://creativecommons.org/licenses/by/4.0/","  We will use overlays and templates derived from two-dimensional recurrence
relations to build the arrays, and we will study the structure of the overlays,
including initial conditions and basis arrays.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:29:42 GMT""}]","2020-11-24"
"2011.10694","Cesar Lema","Cesar Lema and Anna Choromanska","Approximating Ground State Energies and Wave Functions of Physical
  Systems with Neural Networks",,,,,"quant-ph cond-mat.dis-nn","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Quantum theory has been remarkably successful in providing an understanding
of physical systems at foundational scales. Solving the Schr\""odinger equation
provides full knowledge of all dynamical quantities of the physical system.
However closed form solutions to this equation are only available for a few
systems and approximation methods are typically used to find solutions. In this
paper we address the problem of solving the time independent Schr\""odinger
equation for the ground state solution of physical systems. We propose using
end-to-end deep learning approach in a variational optimization scheme for
approximating the ground state energies and wave functions of these systems. A
neural network realizes a universal trial wave function and is trained in an
unsupervised learning framework by optimizing the expectation value of the
Hamiltonian of a physical system. The proposed approach is evaluated on
physical systems consisting of a particle in a box with and without a
perturbation. We demonstrate that our approach obtains approximations of ground
state energies and wave functions that are highly accurate, which makes it a
potentially plausible candidate for solving more complex physical systems for
which analytical solutions are beyond reach.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:30:52 GMT""}]","2020-11-24"
"2011.10695","Micha{\l} Derezi\'nski","Micha{\l} Derezi\'nski, Zhenyu Liao, Edgar Dobriban and Michael W.
  Mahoney","Sparse sketches with small inversion bias",,,,,"cs.DS cs.LG stat.ML","http://creativecommons.org/licenses/by/4.0/","  For a tall $n\times d$ matrix $A$ and a random $m\times n$ sketching matrix
$S$, the sketched estimate of the inverse covariance matrix $(A^\top A)^{-1}$
is typically biased: $E[(\tilde A^\top\tilde A)^{-1}]\ne(A^\top A)^{-1}$, where
$\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in
statistics and distributed optimization, when averaging multiple independently
constructed estimates of quantities that depend on the inverse covariance. We
develop a framework for analyzing inversion bias, based on our proposed concept
of an $(\epsilon,\delta)$-unbiased estimator for random matrices. We show that
when the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries,
then after simple rescaling, the estimator $(\frac m{m-d}\tilde A^\top\tilde
A)^{-1}$ is $(\epsilon,\delta)$-unbiased for $(A^\top A)^{-1}$ with a sketch of
size $m=O(d+\sqrt d/\epsilon)$. This implies that for $m=O(d)$, the inversion
bias of this estimator is $O(1/\sqrt d)$, which is much smaller than the
$\Theta(1)$ approximation error obtained as a consequence of the subspace
embedding guarantee for sub-gaussian sketches. We then propose a new sketching
technique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas
from both data-oblivious sparse embeddings as well as data-aware leverage-based
row sampling methods, to get $\epsilon$ inversion bias for sketch size
$m=O(d\log d+\sqrt d/\epsilon)$ in time $O(\text{nnz}(A)\log n+md^2)$, where
nnz is the number of non-zeros. The key techniques enabling our analysis
include an extension of a classical inequality of Bai and Silverstein for
random quadratic forms, which we call the Restricted Bai-Silverstein
inequality; and anti-concentration of the Binomial distribution via the
Paley-Zygmund inequality, which we use to prove a lower bound showing that
leverage score sampling sketches generally do not achieve small inversion bias.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:33:15 GMT""},{""version"":""v2"",""created"":""Sat, 10 Jul 2021 01:24:51 GMT""}]","2021-07-13"
"2011.10696","Sebastien Vievard","Sebastien Vievard, Aurelie Bonnefois, Frederic Cassaing, Joseph
  Montri, Laurent Mugnier","Cophasing multiple aperture telescopes with Linearized Analytic Phase
  Diversity (LAPD)","Accepted for publication in JATIS",,,,"astro-ph.IM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Focal plane wavefront sensing is an appealing technique to cophase multiple
aperture telescopes. Phase diversity, operable with any aperture configuration
or source extension, generally suffers from high computing load. In this
Letter, we introduce, characterize and experimentally validate the LAPD
algorithm, based on a fast linearized phase diversity algorithm \rev{with a
capture range comparable to classic phase diversity.} We demonstrate that a
typical performance of lambda/75 RMS wavefront error can be reached.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:33:41 GMT""},{""version"":""v2"",""created"":""Fri, 4 Dec 2020 14:43:45 GMT""}]","2020-12-07"
"2011.10697","Mahdi Elhousni","Elhousni Mahdi, Zhang Ziming and Huang Xinming","Aerial Height Prediction and Refinement Neural Networks with Semantic
  and Geometric Guidance",,,,,"cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep learning provides a powerful new approach to many computer vision tasks.
Height prediction from aerial images is one of those tasks that benefited
greatly from the deployment of deep learning which replaced old multi-view
geometry techniques. This letter proposes a two-stage approach, where first a
multi-task neural network is used to predict the height map resulting from a
single RGB aerial input image. We also include a second refinement step, where
a denoising autoencoder is used to produce higher quality height maps.
Experiments on two publicly available datasets show that our method is capable
of producing state-of-the-art results. Code is available at
https://github.com/melhousni/DSMNet.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:39:37 GMT""},{""version"":""v2"",""created"":""Mon, 15 Feb 2021 19:00:15 GMT""},{""version"":""v3"",""created"":""Mon, 12 Apr 2021 16:58:27 GMT""},{""version"":""v4"",""created"":""Fri, 12 Nov 2021 16:54:23 GMT""}]","2021-11-15"
"2011.10698","Shihong Fang","Shihong Fang, Anna Choromanska","Backdoor Attacks on the DNN Interpretation System","Published at the 2022 AAAI Conference on Artificial Intelligence
  (AAAI), 2022",,,,"cs.CR cs.AI cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Interpretability is crucial to understand the inner workings of deep neural
networks (DNNs) and many interpretation methods generate saliency maps that
highlight parts of the input image that contribute the most to the prediction
made by the DNN. In this paper we design a backdoor attack that alters the
saliency map produced by the network for an input image only with injected
trigger that is invisible to the naked eye while maintaining the prediction
accuracy. The attack relies on injecting poisoned data with a trigger into the
training data set. The saliency maps are incorporated in the penalty term of
the objective function that is used to train a deep model and its influence on
model training is conditioned upon the presence of a trigger. We design two
types of attacks: targeted attack that enforces a specific modification of the
saliency map and untargeted attack when the importance scores of the top pixels
from the original saliency map are significantly reduced. We perform empirical
evaluation of the proposed backdoor attacks on gradient-based and gradient-free
interpretation methods for a variety of deep learning architectures. We show
that our attacks constitute a serious security threat when deploying deep
learning models developed by untrusty sources. Finally, in the Supplement we
demonstrate that the proposed methodology can be used in an inverted setting,
where the correct saliency map can be obtained only in the presence of a
trigger (key), effectively making the interpretation system available only to
selected users.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:54:45 GMT""},{""version"":""v2"",""created"":""Thu, 3 Dec 2020 01:49:42 GMT""},{""version"":""v3"",""created"":""Tue, 19 Jul 2022 21:42:22 GMT""}]","2022-07-21"
"2011.10699","Jose Gomez-Tames","Akimasa Hirata, Sachiko Kodera, Kensuke Sasaki, Jose Gomez-Tames,
  Ilkka Laakso, Andrew Wood, Soichi Watanabe, Kenneth R. Foster","Human Exposure to Radiofrequency Energy above 6 GHz: Review of
  Computational Dosimetry Studies","38 pages, 3 figures",,"10.1088/1361-6560/abf1b7",,"physics.med-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  International guidelines/standards for human protection from electromagnetic
fields have been revised recently, especially for frequencies above 6 GHz where
new wireless communication systems have been deployed. Above this frequency a
new physical quantity ""absorbed/epithelia power density"" has been adopted as a
dose metric. Then, the permissible level of external field strength/power
density is derived for practical assessment. In addition, a new physical
quantity, fluence or absorbed energy density, is introduced for protection from
brief pulses (especially for shorter than 10 sec). These limits were explicitly
designed to avoid excessive increases in tissue temperature, based on
electromagnetic and thermal modeling studies but supported by experimental data
where available. This paper reviews the studies on the computational
modeling/dosimetry which are related to the revision of the
guidelines/standards. The comparisons with experimental data as well as an
analytic solution are also been presented. Future research needs and additional
comments on the revision will also be mentioned.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:55:26 GMT""}]","2021-05-26"
"2011.10700","Alexandros Haridis (Charidis)","Alexandros Haridis","Some Open Problems Regarding the Number of Lines and Slopes in
  Arrangements that Determine Shapes","21 Pages. 20 Figures. Expository article",,,,"math.GM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A set $L$ of straight lines and a set $P$ of points in the Euclidean plane
define an arrangement $\mathcal{A}$ = ($L$, $P$) of construction lines and
registration marks, if and only if: (1) any point in $P$ is a point of
intersection of at least two lines in $L$, and (2) any two nonparallel lines in
$L$ have a unique point of intersection in $P$. This paper discusses the
following open problems regarding such arrangements. Suppose $k \geq 0$ number
of points are given in the plane. How many construction lines $k$ points must
determine? How many distinct slopes, or directions, are defined by construction
lines that $k$ points determine? How many distinct sets of construction lines
partition the plane, such that the lines meet at exactly $k$ points? Empirical
evidence is reported for small numbers of $k$, offering partial answers to the
three problems. A conjecture is also stated for the first problem, on the
number of construction lines, after examining a related problem about finite
linear spaces from incidence geometry.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 01:58:56 GMT""}]","2020-11-24"
"2011.10701","Hiroyasu  Koizumi","Hiroyasu Koizumi","London moment, London's superpotential, Nambu-Goldstone mode, and Berry
  connection from many-body wave functions",,"J Supercond Nov Magn (2021)","10.1007/s10948-021-05827-9",,"cond-mat.supr-con","http://creativecommons.org/licenses/by/4.0/","  Although the standard theory of superconductivity based on the BCS theory is
a successful one, there are several experimental results that indicate the
necessity for fundamental revisions. One of them is the mass in the London
moment. Experiments indicate the mass in the London moment is the free electron
mass although the BCS theory and its extension predict it to be an effective
mass.
  We show that this discrepancy is lifted if we install the London's
superpotential in the theory, and identify it as the Berry phase arising from
the many-body wave functions. Then, the induced current by the applied magnetic
field becomes a stable current calculated using the free energy in contrast to
the linear response current assumed in the standard theory which yields the
Nambu-Goldstone mode. The Nambu-Goldstone mode arising from the breakdown of
the global $U(1)$ gauge invariance in the standard theory is replaced by the
collective mode arising from the Berry connection. Then, the free electron mass
appears in the London moment.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:11:10 GMT""}]","2021-04-14"
"2011.10702","Alexander Wong","James Ren Hou Lee, Maya Pavlova, Mahmoud Famouri, and Alexander Wong","CancerNet-SCa: Tailored Deep Neural Network Designs for Detection of
  Skin Cancer from Dermoscopy Images","8 pages",,,,"cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Skin cancer continues to be the most frequently diagnosed form of cancer in
the U.S., with not only significant effects on health and well-being but also
significant economic costs associated with treatment. A crucial step to the
treatment and management of skin cancer is effective skin cancer detection due
to strong prognosis when treated at an early stage, with one of the key
screening approaches being dermoscopy examination. Motivated by the advances of
deep learning and inspired by the open source initiatives in the research
community, in this study we introduce CancerNet-SCa, a suite of deep neural
network designs tailored for the detection of skin cancer from dermoscopy
images that is open source and available to the general public as part of the
Cancer-Net initiative. To the best of the authors' knowledge, CancerNet-SCa
comprises of the first machine-designed deep neural network architecture
designs tailored specifically for skin cancer detection, one of which
possessing a self-attention architecture design with attention condensers.
Furthermore, we investigate and audit the behaviour of CancerNet-SCa in a
responsible and transparent manner via explainability-driven model auditing.
While CancerNet-SCa is not a production-ready screening solution, the hope is
that the release of CancerNet-SCa in open source, open access form will
encourage researchers, clinicians, and citizen data scientists alike to
leverage and build upon them.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:17:59 GMT""}]","2020-11-24"
"2011.10703","David Urbanik","David Urbanik","Absolute Hodge and $\ell$-adic Monodromy","Comments welcome!",,"10.1112/S0010437X2200745X",,"math.AG math.NT","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Let $\mathbb{V}$ be a motivic variation of Hodge structure on a $K$-variety
$S$, let $\mathcal{H}$ be the associated $K$-algebraic Hodge bundle, and let
$\sigma \in \textrm{Aut}(\mathbb{C}/K)$ be an automorphism. The absolute Hodge
conjecture predicts that given a Hodge vector $v \in \mathcal{H}_{\mathbb{C},
s}$ above $s \in S(\mathbb{C})$ which lies inside $\mathbb{V}_{s}$, the
conjugate vector $v_{\sigma} \in \mathcal{H}_{\mathbb{C}, s_{\sigma}}$ is Hodge
and lies inside $\mathbb{V}_{s_{\sigma}}$. We study this problem in the
situation where we have an algebraic subvariety $Z \subset S_{\mathbb{C}}$
containing $s$ whose algebraic monodromy group $\mathbf{H}_Z$ fixes $v$. Using
relationships between $\mathbf{H}_Z$ and $\mathbf{H}_{Z_{\sigma}}$ coming from
the theories of complex and $\ell$-adic local systems, we establish a criterion
that implies the absolute Hodge conjecture for $v$ subject to a group-theoretic
condition on $\mathbf{H}_{Z}$. We then use our criterion to establish new cases
of the absolute Hodge conjecture.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:20:10 GMT""}]","2022-06-24"
"2011.10704","Weixin Liang","Weixin Liang, James Zou","Neural Group Testing to Accelerate Deep Learning","ISIT 2021. Code & data available at
  https://github.com/Weixin-Liang/NeuralGroupTesting",,,,"cs.LG cs.AI cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advances in deep learning have made the use of large, deep neural
networks with tens of millions of parameters. The sheer size of these networks
imposes a challenging computational burden during inference. Existing work
focuses primarily on accelerating each forward pass of a neural network.
Inspired by the group testing strategy for efficient disease testing, we
propose neural group testing, which accelerates by testing a group of samples
in one forward pass. Groups of samples that test negative are ruled out. If a
group tests positive, samples in that group are then retested adaptively. A key
challenge of neural group testing is to modify a deep neural network so that it
could test multiple samples in one forward pass. We propose three designs to
achieve this without introducing any new parameters and evaluate their
performances. We applied neural group testing in an image moderation task to
detect rare but inappropriate images. We found that neural group testing can
group up to 16 images in one forward pass and reduce the overall computation
cost by over 73% while improving detection performance.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:23:54 GMT""},{""version"":""v2"",""created"":""Sun, 9 May 2021 23:03:47 GMT""}]","2021-05-11"
"2011.10705","Suhas Suresh Jain","Suhas S. Jain and Ali Mani","Modeling transport of scalars in two-phase flows with a
  diffuse-interface method","38 pages, 24 figures, submitted to the journal of computational
  physics",,,,"physics.flu-dyn physics.comp-ph","http://creativecommons.org/licenses/by/4.0/","  In this article, we propose a novel scalar-transport model for the simulation
of scalar quantities in two-phase flows with a phase-field method
(diffuse-interface method). In a two-phase flow, the scalar quantities
typically have disparate properties in two phases, which results in effective
confinement of the scalar quantities in one of the phases, in the time scales
of interest. This confinement of the scalars lead to the formation of sharp
gradients of the scalar concentration values at the interface, presenting a
serious challenge for its numerical simulations.
  To overcome this challenge, we propose a model for the transport of scalars.
The model is discretized using a central-difference scheme, which leads to a
non-dissipative implementation that is crucial for the simulation of turbulent
flows. Furthermore, the provable strengths of the proposed model are: (a) the
model maintains the positivity property of the scalar concentration field, a
physical realizability requirement for the simulation of scalars, when the
proposed criterion is satisfied, (b) the proposed model is such that the
transport of the scalar concentration field is consistent with the transport of
the volume fraction field, which results in the enforcement of the effective
zero-flux boundary condition for the scalar at the interface; and therefore,
prevents the artificial numerical diffusion of the scalar across the interface.
  Finally, we present numerical simulations using the proposed model in a wide
range of two-phase flow regimes, spanning laminar to turbulent flows; and
assess: the accuracy and robustness of the model, the validity of the
positivity property of the scalar concentration field, and the enforcement of
the zero-flux boundary condition for the scalar at the interface.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:30:00 GMT""}]","2020-11-24"
"2011.10706","Mark Saddler","Mark R. Saddler, Andrew Francl, Jenelle Feather, Kaizhi Qian, Yang
  Zhang, Josh H. McDermott","Speech Denoising with Auditory Models","First two authors contributed equally, 5 pages, 3 PDF figures",,,,"eess.AS cs.SD","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Contemporary speech enhancement predominantly relies on audio transforms that
are trained to reconstruct a clean speech waveform. The development of
high-performing neural network sound recognition systems has raised the
possibility of using deep feature representations as 'perceptual' losses with
which to train denoising systems. We explored their utility by first training
deep neural networks to classify either spoken words or environmental sounds
from audio. We then trained an audio transform to map noisy speech to an audio
waveform that minimized the difference in the deep feature representations
between the output audio and the corresponding clean audio. The resulting
transforms removed noise substantially better than baseline methods trained to
reconstruct clean waveforms, and also outperformed previous methods using deep
feature losses. However, a similar benefit was obtained simply by using losses
derived from the filter bank inputs to the deep networks. The results show that
deep features can guide speech enhancement, but suggest that they do not yet
outperform simple alternatives that do not involve learned features.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:36:58 GMT""},{""version"":""v2"",""created"":""Fri, 11 Dec 2020 02:16:29 GMT""},{""version"":""v3"",""created"":""Fri, 13 Aug 2021 01:20:55 GMT""}]","2021-08-16"
"2011.10707","Sarath Sreedharan","Sarath Sreedharan, Tathagata Chakraborti, Yara Rizk and Yasaman
  Khazaeni","Explainable Composition of Aggregated Assistants",,,,,"cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A new design of an AI assistant that has become increasingly popular is that
of an ""aggregated assistant"" -- realized as an orchestrated composition of
several individual skills or agents that can each perform atomic tasks. In this
paper, we will talk about the role of planning in the automated composition of
such assistants and explore how concepts in automated planning can help to
establish transparency of the inner workings of the assistant to the end-user.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:39:27 GMT""}]","2020-11-24"
"2011.10708","Blake Zimmerman","Blake E. Zimmerman, Sara L. Johnson, Henrik A. Od\'een, Jill E. Shea,
  Rachel E. Factor, Sarang C. Joshi, and Allison H. Payne","Histology to 3D In Vivo MR Registration for Volumetric Evaluation of
  MRgFUS Treatment Assessment Biomarkers","12 pages, 5 figures, 2 tables",,,,"eess.IV physics.med-ph","http://creativecommons.org/licenses/by/4.0/","  Advances in imaging and early cancer detection have increased interest in
magnetic resonance (MR) guided focused ultrasound (MRgFUS) technologies for
cancer treatment. MRgFUS ablation treatments could reduce surgical risks,
preserve organ tissue/function, and improve patient quality of life. However,
surgical resection and histological analysis remain the gold standard to assess
cancer treatment response. For non-invasive ablation therapies such as MRgFUS,
the treatment response must be determined through MR imaging biomarkers.
However, current MR biomarkers are inconclusive and have not been rigorously
evaluated against histology via accurate registration. Existing registration
methods rely on anatomical features to directly register in vivo MR and
histology. For MRgFUS applications in anatomies such as liver, kidney, or
breast, anatomical features independent from treatment features are often
insufficient to perform direct registration. We present a novel MR to histology
registration workflow that utilizes intermediate imaging and does not rely on
these independent features. The presented workflow yields an overall
registration accuracy of 1.00 +/- 0.13 mm. The developed registration pipeline
is used to evaluate a common MRgFUS treatment assessment biomarker against
histology. Evaluating MR biomarkers against histology using this registration
pipeline will facilitate validating novel MRgFUS biomarkers to improve
treatment assessment without surgical intervention.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:40:50 GMT""}]","2020-11-24"
"2011.10709","Kareem M. Attiah","Kareem M. Attiah, Foad Sohrabi, and Wei Yu","Deep Learning for Channel Sensing and Hybrid Precoding in TDD Massive
  MIMO OFDM Systems","15 Pages, 16 figures, Accepted at IEEE Transactions in Wireless
  Communications",,"10.1109/TWC.2022.3187790",,"cs.IT eess.SP math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper proposes a deep learning approach to channel sensing and downlink
hybrid beamforming for massive multiple-input multiple-output systems operating
in the time division duplex mode and employing either single-carrier or
multicarrier transmission. The conventional precoding design involves a
two-step process of first estimating the high-dimensional channel, then
designing the precoders based on such estimate. This two-step process is,
however, not necessarily optimal. This paper shows that by using a learning
approach to design the analog sensing and the hybrid downlink precoders
directly from the received pilots without the intermediate high-dimensional
channel estimation, the overall system performance can be significantly
improved. Training a neural network to design the analog and digital precoders
simultaneously is, however, difficult. Further, such an approach is not
generalizable to systems with different number of users. In this paper, we
develop a simplified and generalizable approach that learns the uplink sensing
matrix and downlink analog precoder using a deep neural network that decomposes
on a per-user basis, then designs the digital precoder based on the estimated
low-dimensional equivalent channel. Numerical comparisons show that the
proposed methodology results in significantly less training overhead and leads
to an architecture that generalizes to various system settings.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:46:35 GMT""},{""version"":""v2"",""created"":""Sat, 2 Oct 2021 01:10:06 GMT""},{""version"":""v3"",""created"":""Wed, 29 Jun 2022 17:27:30 GMT""}]","2022-06-30"
"2011.10710","Xiaoyi Qin","Xiaoyi Qin and Yaogen Yang and Lin Yang and Xuyang Wang and Junjie
  Wang and Ming Li","Exploring Voice Conversion based Data Augmentation in Text-Dependent
  Speaker Verification","Submitted to ICASSP2021",,,,"cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we focus on improving the performance of the text-dependent
speaker verification system in the scenario of limited training data. The
speaker verification system deep learning based text-dependent generally needs
a large scale text-dependent training data set which could be labor and cost
expensive, especially for customized new wake-up words. In recent studies,
voice conversion systems that can generate high quality synthesized speech of
seen and unseen speakers have been proposed. Inspired by those works, we adopt
two different voice conversion methods as well as the very simple re-sampling
approach to generate new text-dependent speech samples for data augmentation
purposes. Experimental results show that the proposed method significantly
improves the Equal Error Rare performance from 6.51% to 4.51% in the scenario
of limited training data.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:55:47 GMT""}]","2020-11-24"
"2011.10711","Yanguang Chen","Yanguang Chen, Yuqing Long","Spatial Signal Analysis based on Wave-Spectral Fractal Scaling: A Case
  of Urban Street Networks","22 pages, 7 figures, 4 tables","Applied Sciences, 2021, 11(1): 87","10.3390/app11010087",,"physics.soc-ph","http://creativecommons.org/licenses/by/4.0/","  For a long time, many methods are developed to make temporal signal analyses
based on time series. However, for geographical systems, spatial signal
analyses are as important as temporal signal analyses. Nonstationary spatial
and temporal processes are associated with nonlinearity, and cannot be
effectively analyzed by conventional analytical approaches. Fractal theory
provides a powerful tool for exploring complexity and is helpful for
spatio-temporal signal analysis. This paper is devoted to researching spatial
signals of geographical systems by means of wave-spectrum scaling. The traffic
networks of 10 Chinese cities are taken as cases for positive studies. Fast
Fourier transform and least squares regression analysis are employed to
calculate spectral exponents. The results show that the wave-spectral density
distribution of all these urban traffic networks follows scaling law, and the
spectral scaling exponents can be converted to fractal dimension values. Using
the fractal parameters, we can make spatial analyses for the geographical
signals. The analytical process can be generalized to temporal signal analyses.
The wave-spectrum scaling methods can be applied to both self-similar fractal
signals and self-affine fractal signals in the geographical world.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:56:50 GMT""}]","2020-12-29"
"2011.10712","Lintao Ye","Lintao Ye, Aritra Mitra and Shreyas Sundaram","Near-Optimal Data Source Selection for Bayesian Learning","Accepted to L4DC 2021",,,,"cs.LG cs.CC cs.IT math.IT math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study a fundamental problem in Bayesian learning, where the goal is to
select a set of data sources with minimum cost while achieving a certain
learning performance based on the data streams provided by the selected data
sources. First, we show that the data source selection problem for Bayesian
learning is NP-hard. We then show that the data source selection problem can be
transformed into an instance of the submodular set covering problem studied in
the literature, and provide a standard greedy algorithm to solve the data
source selection problem with provable performance guarantees. Next, we propose
a fast greedy algorithm that improves the running times of the standard greedy
algorithm, while achieving performance guarantees that are comparable to those
of the standard greedy algorithm. The fast greedy algorithm can also be applied
to solve the general submodular set covering problem with performance
guarantees. Finally, we validate the theoretical results using numerical
examples, and show that the greedy algorithms work well in practice.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 03:12:38 GMT""},{""version"":""v2"",""created"":""Sun, 2 May 2021 15:46:30 GMT""}]","2021-05-04"
"2011.10713","Hussein Sibai","Hussein Sibai and Yangge Li and Sayan Mitra","SceneChecker: Boosting Scenario Verification using Symmetry Abstractions",,,,,"eess.SY cs.FL cs.LG cs.RO cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We presentSceneChecker, a tool for verifying scenarios involving vehicles
executing complex plans in large cluttered workspaces. SceneChecker converts
the scenario verification problem to a standard hybrid system verification
problem, and solves it effectively by exploiting structural properties in the
plan and the vehicle dynamics. SceneChecker uses symmetry abstractions, a novel
refinement algorithm, and importantly, is built to boost the performance of any
existing reachability analysis tool as a plug-in subroutine. We evaluated
SceneChecker on several scenarios involving ground and aerial vehicles with
nonlinear dynamics and neural network controllers, employing different kinds of
symmetries, using different reachability subroutines, and following plans with
hundreds of way-points in complex workspaces. Compared to two leading tools,
DryVR and Flow*, SceneChecker shows 20x speedup in verification time, even
while using those very tools as reachability subroutines.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 03:18:55 GMT""},{""version"":""v2"",""created"":""Wed, 3 Mar 2021 01:39:28 GMT""}]","2021-03-04"
"2011.10714","Elahe Aghapour","Elahe Aghapour, Nora Ayanian","Double Meta-Learning for Data Efficient Policy Optimization in
  Non-Stationary Environments","8 pages, 4 figures",,,,"cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We are interested in learning models of non-stationary environments, which
can be framed as a multi-task learning problem. Model-free reinforcement
learning algorithms can achieve good asymptotic performance in multi-task
learning at a cost of extensive sampling, due to their approach, which requires
learning from scratch. While model-based approaches are among the most data
efficient learning algorithms, they still struggle with complex tasks and model
uncertainties. Meta-reinforcement learning addresses the efficiency and
generalization challenges on multi task learning by quickly leveraging the
meta-prior policy for a new task. In this paper, we propose a
meta-reinforcement learning approach to learn the dynamic model of a
non-stationary environment to be used for meta-policy optimization later. Due
to the sample efficiency of model-based learning methods, we are able to
simultaneously train both the meta-model of the non-stationary environment and
the meta-policy until dynamic model convergence. Then, the meta-learned dynamic
model of the environment will generate simulated data for meta-policy
optimization. Our experiment demonstrates that our proposed method can
meta-learn the policy in a non-stationary environment with the data efficiency
of model-based learning approaches while achieving the high asymptotic
performance of model-free meta-reinforcement learning.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 03:19:35 GMT""}]","2020-11-24"
"2011.10715","Kyungmin Kim","Seungjae Jung, Kyung-Min Kim, Hanock Kwak and Young-Jin Park","A Worrying Analysis of Probabilistic Time-series Models for Sales
  Forecasting","NeurIPS 2020 workshop (I Can't Believe It's Not Better,
  ICBINB@NeurIPS 2020). All authors contributed equally to this research",,,,"cs.LG cs.AI stat.CO stat.ME","http://creativecommons.org/licenses/by/4.0/","  Probabilistic time-series models become popular in the forecasting field as
they help to make optimal decisions under uncertainty. Despite the growing
interest, a lack of thorough analysis hinders choosing what is worth applying
for the desired task. In this paper, we analyze the performance of three
prominent probabilistic time-series models for sales forecasting. To remove the
role of random chance in architecture's performance, we make two experimental
principles; 1) Large-scale dataset with various cross-validation sets. 2) A
standardized training and hyperparameter selection. The experimental results
show that a simple Multi-layer Perceptron and Linear Regression outperform the
probabilistic models on RMSE without any feature engineering. Overall, the
probabilistic models fail to achieve better performance on point estimation,
such as RMSE and MAPE, than comparably simple baselines. We analyze and discuss
the performances of probabilistic time-series models.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 03:31:23 GMT""}]","2020-11-24"
"2011.10716","Ghurumuruhan Ganesan","Ghurumuruhan Ganesan","Euclidean traveling salesman problem with location dependent and power
  weighted edges","Accepted for publication in Journal of Theoretical Probability",,,,"math.PR","http://creativecommons.org/licenses/by/4.0/","  Consider~\(n\) nodes~\(\{X_i\}_{1 \leq i \leq n}\) independently distributed
in the unit square~\(S,\) each according to a distribution~\(f\) and
let~\(K_n\) be the complete graph formed by joining each pair of nodes by a
straight line segment. For every edge~\(e\) in~\(K_n\) we associate a
weight~\(w(e)\) that may depend on the \emph{individual locations} of the
endvertices of~\(e\) and is not necessarily a power of the Euclidean length
of~\(e.\) Denoting~\(TSP_n\) to be the minimum weight of a spanning cycle
of~\(K_n\) corresponding to the travelling salesman problem (TSP) and assuming
an equivalence condition on the weight function~\(w(.),\) we prove
that~\(TSP_n\) appropriately scaled and centred converges to zero a.s.\ and in
mean as~\(n \rightarrow \infty.\) We also obtain upper and lower bound
deviation estimates for~\(TSP_n.\)
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 03:50:37 GMT""}]","2020-11-24"
"2011.10717","Antoine Jacquey","Antoine B. Jacquey, Hadrien Rattez, Manolis Veveakis","Strain localization regularization and patterns formation in
  rate-dependent plastic materials with multiphysics coupling",,,"10.1016/j.jmps.2021.104422",,"physics.geo-ph cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Strain localization is an instability phenomenon occurring in deformable
solid materials which undergo dissipative deformation mechanisms. Such
instability is characterized by the localization of the displacement or
velocity fields in a zone of finite thickness and is generally associated with
the failure of materials. In several fields of material engineering and natural
sciences, estimating the thickness of localized deformation is required to make
accurate predictions of the evolution of the physical properties within
localized strain regions and of the material strength. In this context,
scientists and engineers often rely on numerical modeling techniques to study
strain localization in solid materials. However, classical continuum theory for
elasto-plastic materials fails at estimating strain localization thicknesses
due to the lack of an internal length in the model constitutive laws. In this
study, we investigate at which conditions multiphysics coupling enables to
regularize the problem of strain localization using rate-dependent plasticity.
We show that coupling the constitutive laws for deformation to a single generic
diffusion-reaction equation representing a dissipative state variable can be
sufficient to regularize the ill-posed problem under some conditions on the
softening parameters in the plastic potential. We demonstrate in these cases
how rate-dependent plasticity and multiphysics coupling can lead to material
instabilities depicting one or several internal length scales controlled by the
physical parameters resulting in the formation of regular or erratic patterns.
As we consider a general form of the equations, the results presented in this
study can be applied to a large panel of examples in the material engineering
and geosciences communities.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 04:05:37 GMT""}]","2021-04-21"
"2011.10718","Mohammad Javad Khojasteh","Anshuka Rangi, Mohammad Javad Khojasteh and Massimo Franceschetti","Learning-based attacks in Cyber-Physical Systems: Exploration,
  Detection, and Control Cost trade-offs","To appear in L4DC 2021. First two authors contributed equally","Learning for Dynamics and Control 2021, PMLR",,,"eess.SY cs.CR cs.LG cs.MA cs.SY stat.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the problem of learning-based attacks in linear systems, where the
communication channel between the controller and the plant can be hijacked by a
malicious attacker. We assume the attacker learns the dynamics of the system
from observations, then overrides the controller's actuation signal, while
mimicking legitimate operation by providing fictitious sensor readings to the
controller. On the other hand, the controller is on a lookout to detect the
presence of the attacker and tries to enhance the detection performance by
carefully crafting its control signals. We study the trade-offs between the
information acquired by the attacker from observations, the detection
capabilities of the controller, and the control cost. Specifically, we provide
tight upper and lower bounds on the expected $\epsilon$-deception time, namely
the time required by the controller to make a decision regarding the presence
of an attacker with confidence at least $(1-\epsilon\log(1/\epsilon))$. We then
show a probabilistic lower bound on the time that must be spent by the attacker
learning the system, in order for the controller to have a given expected
$\epsilon$-deception time. We show that this bound is also order optimal, in
the sense that if the attacker satisfies it, then there exists a learning
algorithm with the given order expected deception time. Finally, we show a
lower bound on the expected energy expenditure required to guarantee detection
with confidence at least $1-\epsilon \log(1/\epsilon)$.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 04:08:16 GMT""},{""version"":""v2"",""created"":""Thu, 20 May 2021 02:11:55 GMT""}]","2021-05-21"
"2011.10719","Erica Sturm","Erica J. Sturm, Matthew R. Carbone, Deyu Lu, Andreas Weichselbaum,
  Robert M. Konik","Predicting impurity spectral functions using machine learning","31 pages, 27 figures, 5 tables","Phys. Rev. B 103, 245118 (2021)","10.1103/PhysRevB.103.245118",,"cond-mat.str-el quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Anderson Impurity Model (AIM) is a canonical model of quantum many-body
physics. Here we investigate whether machine learning models, both neural
networks (NN) and kernel ridge regression (KRR), can accurately predict the AIM
spectral function in all of its regimes, from empty orbital, to mixed valence,
to Kondo. To tackle this question, we construct two large spectral databases
containing approximately 410k and 600k spectral functions of the single-channel
impurity problem. We show that the NN models can accurately predict the AIM
spectral function in all of its regimes, with point-wise mean absolute errors
down to 0.003 in normalized units. We find that the trained NN models
outperform models based on KRR and enjoy a speedup on the order of $10^5$ over
traditional AIM solvers. The required size of the training set of our model can
be significantly reduced using furthest point sampling in the AIM parameter
space, which is important for generalizing our method to more complicated
multi-channel impurity problems of relevance to predicting the properties of
real materials.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 04:28:29 GMT""}]","2021-06-23"
"2011.10720","Roland Matsouaka","Roland A. Matsouaka and Adrian Coles","Robust statistical inference for the matched net benefit and the matched
  win ratio using prioritized composite endpoints","27 pages, 7 figures",,,,"stat.ME","http://creativecommons.org/publicdomain/zero/1.0/","  As alternatives to the time-to-first-event analysis of composite endpoints,
the {\it net benefit} (NB) and the {\it win ratio} (WR) -- which assess
treatment effects using prioritized component outcomes based on clinical
importance -- have been proposed. However, statistical inference of NB and WR
relies on a large-sample assumptions, which can lead to an invalid test
statistic and inadequate, unsatisfactory confidence intervals, especially when
the sample size is small or the proportion of wins is near 0 or 1.
  In this paper, we develop a systematic approach to address these limitations
in a paired-sample design. We first introduce a new test statistic under the
null hypothesis of no treatment difference. Then, we present the formula to
calculate the sample size. Finally, we develop the confidence interval
estimations of these two estimators. To estimate the confidence intervals, we
use the {\it method of variance estimates recovery} (MOVER), that combines two
separate individual-proportion confidence intervals into a hybrid interval for
the estimand of interest. We assess the performance of the proposed test
statistic and MOVER confidence interval estimations through simulation studies.
  We demonstrate that the MOVER confidence intervals are as good as the
large-sample confidence intervals when the sample is large and when the
proportions of wins is bounded away from 0 and 1. Moreover, the MOVER intervals
outperform their competitors when the sample is small or the proportions are at
or near the boundaries 0 and 1. We illustrate the method (and its competitors)
using three examples from randomized clinical studies.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 04:54:38 GMT""}]","2020-11-24"
"2011.10721","Chuanzheng Wang","Chuanzheng Wang, Yinan Li, Yiming Meng, Stephen L. Smith, Jun Liu","Learning Control Barrier Functions with High Relative Degree for
  Safety-Critical Control",,,,,"eess.SY cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Control barrier functions have shown great success in addressing control
problems with safety guarantees. These methods usually find the next safe
control input by solving an online quadratic programming problem. However,
model uncertainty is a big challenge in synthesizing controllers. This may lead
to the generation of unsafe control actions, resulting in severe consequences.
In this paper, we develop a learning framework to deal with system uncertainty.
Our method mainly focuses on learning the dynamics of the control barrier
function, especially for high relative degree with respect to a system. We show
that for each order, the time derivative of the control barrier function can be
separated into the time derivative of the nominal control barrier function and
a remainder. This implies that we can use a neural network to learn the
remainder so that we can approximate the dynamics of the real control barrier
function. We show by simulation that our method can generate safe trajectories
under parametric uncertainty using a differential drive robot model.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 05:01:12 GMT""}]","2020-11-24"
"2011.10722","Michael Coons","Michael Coons and James Evans","A sequential view of self--similar measures, or, What the ghosts of
  Mahler and Cantor can teach us about dimension","8 pages",,,,"math.NT","http://creativecommons.org/licenses/by/4.0/","  We show that missing $q$-ary digit sets $F\subseteq[0,1]$ have corresponding
naturally associated countable binary $q$-automatic sequence $f$. Using this
correspondence, we show that the Hausdorff dimension of $F$ is equal to the
base-$q$ logarithm of the Mahler eigenvalue of $f$. In addition, we demonstrate
that the standard mass distribution $\nu_F$ supported on $F$ is equal to the
ghost measure $\mu_f$ of $f$.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 05:05:45 GMT""}]","2020-11-24"
"2011.10723","Xing Wu","Xing Wu and Jie Cao","Non-uniform continuous dependence on initial data for a two_component
  Novikov system in Besov space","This paper has been submitted",,,,"math.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we show that the solution map of the two-component Novikov
system is not uniformly continuous on the initial data in Besov spaces $B_{p,
r}^{s-1}(\mathbb{R})\times B_{p, r}^s(\mathbb{R})$ with $s>\max\{1+\frac{1}{p},
\frac{3}{2}\}$, $1\leq p< \infty$, $1\leq r<\infty$. Our result covers and
extends the previous non-uniform continuity in Sobolev spaces
$H^{s-1}(\mathbb{R})\times H^s(\mathbb{R})$ for $s>\frac{5}{2}$ (J. Math.
Phys., 2017) to Besov spaces.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 05:13:56 GMT""}]","2020-11-24"
"2011.10724","Gopal Goel","Gopal Goel and Andrew Yao","A Quantized Analogue of the Markov-Krein Correspondence","Comments and suggestions welcome! (version 3 fixed a major error,
  which resulted in the deletion of an appendix)",,,,"math.PR math-ph math.CO math.MP math.RT math.SP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study a family of measures originating from the signatures of the
irreducible components of representations of the unitary group, as the size of
the group goes to infinity. Given a random signature $\lambda$ of length $N$
with counting measure $\mathbf{m}$, we obtain a random signature $\mu$ of
length $N-1$ through projection onto a unitary group of lower dimension. The
signature $\mu$ interlaces with the signature $\lambda$, and we record the data
of $\mu,\lambda$ in a random rectangular Young diagram $w$. We show that under
a certain set of conditions on $\lambda$, both $\mathbf{m}$ and $w$ converge as
$N\to\infty$. We provide an explicit moment generating function relationship
between the limiting objects. We further show that the moment generating
function relationship induces a bijection between bounded measures and certain
continual Young diagrams, which can be viewed as a quantized analogue of the
Markov-Krein correspondence.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 05:14:59 GMT""},{""version"":""v2"",""created"":""Tue, 24 Nov 2020 21:32:41 GMT""},{""version"":""v3"",""created"":""Thu, 15 Jul 2021 20:55:13 GMT""}]","2021-07-19"
"2011.10725","Hau-Tieng Wu","Xiucai Ding and Hau-Tieng Wu","Impact of signal-to-noise ratio and bandwidth on graph Laplacian
  spectrum from high-dimensional noisy point cloud",,,,,"math.ST cs.LG math.SP stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We systematically study the spectrum of kernel-based graph Laplacian (GL)
constructed from high-dimensional and noisy random point cloud in the nonnull
setup. The problem is motived by studying the model when the clean signal is
sampled from a manifold that is embedded in a low-dimensional Euclidean
subspace, and corrupted by high-dimensional noise. We quantify how the signal
and noise interact over different regions of signal-to-noise ratio (SNR), and
report the resulting peculiar spectral behavior of GL. In addition, we explore
the impact of chosen kernel bandwidth on the spectrum of GL over different
regions of SNR, which lead to an adaptive choice of kernel bandwidth that
coincides with the common practice in real data. This result paves the way to a
theoretical understanding of how practitioners apply GL when the dataset is
noisy.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 05:19:04 GMT""},{""version"":""v2"",""created"":""Tue, 19 Oct 2021 22:54:28 GMT""},{""version"":""v3"",""created"":""Sun, 17 Jul 2022 12:58:45 GMT""},{""version"":""v4"",""created"":""Thu, 20 Oct 2022 02:20:57 GMT""}]","2022-10-21"
"2011.10726","Michael Danielczuk","Michael Danielczuk, Arsalan Mousavian, Clemens Eppner, Dieter Fox","Object Rearrangement Using Learned Implicit Collision Functions","First two authors contributed equally. 2021 IEEE International
  Conference on Robotics and Automation. 8 pages, 4 figures, 3 tables",,,,"cs.RO cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Robotic object rearrangement combines the skills of picking and placing
objects. When object models are unavailable, typical collision-checking models
may be unable to predict collisions in partial point clouds with occlusions,
making generation of collision-free grasping or placement trajectories
challenging. We propose a learned collision model that accepts scene and query
object point clouds and predicts collisions for 6DOF object poses within the
scene. We train the model on a synthetic set of 1 million scene/object point
cloud pairs and 2 billion collision queries. We leverage the learned collision
model as part of a model predictive path integral (MPPI) policy in a tabletop
rearrangement task and show that the policy can plan collision-free grasps and
placements for objects unseen in training in both simulated and physical
cluttered scenes with a Franka Panda robot. The learned model outperforms both
traditional pipelines and learned ablations by 9.8% in accuracy on a dataset of
simulated collision queries and is 75x faster than the best-performing
baseline. Videos and supplementary material are available at
https://research.nvidia.com/publication/2021-03_Object-Rearrangement-Using.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 05:36:06 GMT""},{""version"":""v2"",""created"":""Fri, 26 Mar 2021 07:38:35 GMT""}]","2021-03-29"
"2011.10727","Ravindra Yadav","Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde","Stochastic Talking Face Generation Using Latent Distribution Matching","InterSpeech 2020",,,,"cs.CV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  The ability to envisage the visual of a talking face based just on hearing a
voice is a unique human capability. There have been a number of works that have
solved for this ability recently. We differ from these approaches by enabling a
variety of talking face generations based on single audio input. Indeed, just
having the ability to generate a single talking face would make a system almost
robotic in nature. In contrast, our unsupervised stochastic audio-to-video
generation model allows for diverse generations from a single audio input.
Particularly, we present an unsupervised stochastic audio-to-video generation
model that can capture multiple modes of the video distribution. We ensure that
all the diverse generations are plausible. We do so through a principled
multi-modal variational autoencoder framework. We demonstrate its efficacy on
the challenging LRW and GRID datasets and demonstrate performance better than
the baseline, while having the ability to generate multiple diverse lip
synchronized videos.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 06:05:24 GMT""}]","2020-11-24"
"2011.10728","Changjian Fu","Wei Dai and Changjian Fu","A reduction approach to silting objects for derived categories of
  hereditary categories","12 pages",,,,"math.RA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Let $\mathcal{H}$ be a hereditary abelian category over a field $k$ with
finite dimensional $\operatorname{Hom}$ and $\operatorname{Ext}$ spaces. It is
proved that the bounded derived category $\mathcal{D}^b(\mathcal{H})$ has a
silting object iff $\mathcal{H}$ has a tilting object iff
$\mathcal{D}^b(\mathcal{H})$ has a simple-minded collection with acyclic
$\operatorname{Ext}$-quiver. Along the way, we obtain a new proof for the fact
that every presilting object of $\mathcal{D}^b(\mathcal{H})$ is a partial
silting object. We also consider the question of complements for
pre-simple-minded collections. In contrast to presilting objects, a
pre-simple-minded collection $\mathcal{R}$ of $\mathcal{D}^b(\mathcal{H})$ can
be completed into a simple-minded collection iff the
$\operatorname{Ext}$-quiver of $\mathcal{R}$ is acyclic.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 06:06:10 GMT""}]","2020-11-24"
"2011.10729","Aleksey Korobenko","Aleksey Korobenko, Soham Saha, Alan T. K. Godfrey, Marina Gertsvolf,
  Andrei Yu. Naumov, David M. Villeneuve, Alexandra Boltasseva, Vladimir M.
  Shalaev, Paul B. Corkum","High-harmonic generation in metallic titanium nitride",,,"10.1038/s41467-021-25224-z",,"physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  High-harmonic generation is the cornerstone of nonlinear optics. It has been
demonstrated in a wide range of crystalline systems including dielectrics,
semiconductors, and semi-metals, as well as in gases, leaving metals out due to
their low damage threshold. Here, we report on the high-harmonic generation in
metallic titanium nitride (TiN) films. TiN is a refractory plasmonic metal,
known for its high melting temperature and laser damage threshold, with optical
properties similar to those of gold. We show that TiN can withstand laser
pulses with peak intensities as high as 13 TW/cm$^2$, one order of magnitude
higher than gold, enabling the emission of intraband harmonics up to photon
energies of 11 eV. These harmonics can pave the way for compact and efficient
plasmonic devices producing vacuum ultraviolet (VUV) frequency combs. Through
numerical calculations and experimental studies, we show that the intensity
scaling and angular anisotropy of the emitted VUV radiation stem from the
anisotropic conduction band structure of TiN, thus confirming its intraband
origin.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 06:10:25 GMT""}]","2021-09-01"
"2011.10730","Andrew Taylor","Andrew J. Taylor, Victor D. Dorobantu, Sarah Dean, Benjamin Recht,
  Yisong Yue, Aaron D. Ames","Towards Robust Data-Driven Control Synthesis for Nonlinear Systems with
  Actuation Uncertainty","8 pages, 2 figures, submitted to Conference on Decision & Control
  (CDC) 2021",,,,"eess.SY cs.RO cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Modern nonlinear control theory seeks to endow systems with properties such
as stability and safety, and has been deployed successfully across various
domains. Despite this success, model uncertainty remains a significant
challenge in ensuring that model-based controllers transfer to real world
systems. This paper develops a data-driven approach to robust control synthesis
in the presence of model uncertainty using Control Certificate Functions
(CCFs), resulting in a convex optimization based controller for achieving
properties like stability and safety. An important benefit of our framework is
nuanced data-dependent guarantees, which in principle can yield
sample-efficient data collection approaches that need not fully determine the
input-to-state relationship. This work serves as a starting point for
addressing important questions at the intersection of nonlinear control theory
and non-parametric learning, both theoretical and in application. We validate
the proposed method in simulation with an inverted pendulum in multiple
experimental configurations.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 06:11:43 GMT""},{""version"":""v2"",""created"":""Wed, 31 Mar 2021 21:14:24 GMT""}]","2021-04-02"
"2011.10731","Weixin Liang","Weixin Liang, Feiyang Niu, Aishwarya Reganti, Govind Thattai, Gokhan
  Tur","LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular
  Supervision for Visual Question Answering","NeurIPS KR2ML 2020",,,,"cs.CL cs.AI cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The predominant approach to visual question answering (VQA) relies on
encoding the image and question with a ""black-box"" neural encoder and decoding
a single token as the answer like ""yes"" or ""no"". Despite this approach's strong
quantitative results, it struggles to come up with intuitive, human-readable
forms of justification for the prediction process. To address this
insufficiency, we reformulate VQA as a full answer generation task, which
requires the model to justify its predictions in natural language. We propose
LRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning
framework for visual question answering that solves the problem step-by-step
like humans and provides human-readable form of justification at each step.
Specifically, LRTA learns to first convert an image into a scene graph and
parse a question into multiple reasoning instructions. It then executes the
reasoning instructions one at a time by traversing the scene graph using a
recurrent neural-symbolic execution module. Finally, it generates a full answer
to the given question with natural language justifications. Our experiments on
GQA dataset show that LRTA outperforms the state-of-the-art model by a large
margin (43.1% v.s. 28.0%) on the full answer generation task. We also create a
perturbed GQA test set by removing linguistic cues (attributes and relations)
in the questions for analyzing whether a model is having a smart guess with
superficial data correlations. We show that LRTA makes a step towards truly
understanding the question while the state-of-the-art model tends to learn
superficial correlations from the training data.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 06:39:42 GMT""}]","2020-11-24"
"2011.10732","Andrea Gabrio","Andrea Gabrio","A Bayesian framework for patient-level partitioned survival cost-utility
  analysis",,,,,"stat.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Patient-level health economic data collected alongside clinical trials are an
important component of the process of technology appraisal, with a view to
informing resource allocation decisions. For end of life treatments, such as
cancer treatments, modelling of cost-effectiveness/utility data may involve
some form of partitioned survival analysis, where measures of health-related
quality of life and survival time for both pre- and post-progression periods
are combined to generate some aggregate measure of clinical benefits (e.g.
quality-adjusted survival). In addition, resource use data are often collected
from health records on different services from which different cost components
are obtained (e.g. treatment, hospital or adverse events costs). A critical
problem in these analyses is that both effectiveness and cost data present some
complexities, including non-normality, spikes, and missingness, that should be
addressed using appropriate methods. Bayesian modelling provides a powerful
tool which has become more and more popular in the recent health economics and
statistical literature to jointly handle these issues in a relatively easy way.
This paper presents a general Bayesian framework that takes into account the
complex relationships of trial-based partitioned survival cost-utility data,
potentially providing a more adequate evidence for policymakers to inform the
decision-making process. Our approach is motivated by, and applied to, a
working example based on data from a trial assessing the cost-effectiveness of
a new treatment for patients with advanced non-small-cell lung cancer.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 06:43:23 GMT""}]","2020-11-24"
"2011.10733","Ayse Borat","Tane Vergili, Ayse Borat","Topological spaces induced by homotopic distance",,,,,"math.AT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Homotopic distance $\D$ as introduced in \cite{MVML} can be realized as a
pseudometric on $\mathrm{Map}(X,Y)$. In this paper, we study the topology
induced by the pseudometric $\D$. In particular, we consider the space
$\mathrm{Map}(S^1,S^1)$ and show that homotopic distance between any two maps
in this space is 1. Moreover, while a general proof of the non-compactness of
the space $\mathrm{Map}(X,Y)$ is still an open problem, it can be shown that
$\mathrm{Map}(S^1,S^1)$ is not compact.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:03:22 GMT""}]","2020-11-24"
"2011.10734","Jon Goiri","Jon Gabriel Goiri and Anton Van der Ven","MultiShifter: software to generate structural models of extended
  two-dimensional defects in 3D and 2D crystals",,,,,"cond-mat.mtrl-sci","http://creativecommons.org/licenses/by/4.0/","  Extended defects in crystals, such as dislocations, stacking faults and grain
boundaries, play a crucial role in determining a wide variety of materials
properties. Extended defects can also lead to novel electronic properties in
two-dimensional materials, as demonstrated by recent discoveries of emergent
electronic phenomena in twisted graphene bilayers. This paper describes several
approaches to construct crystallographic models of two-dimensional extended
defects in crystals for first-principles electronic structure calculations,
including (i) crystallographic models to parameterize generalized cohesive zone
models for fracture studies and meso-scale models of dislocations and (ii)
crystallographic models of twisted bilayers. The approaches are implemented in
an open source software package called MultiShifter.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:04:33 GMT""},{""version"":""v2"",""created"":""Tue, 24 Nov 2020 03:29:54 GMT""},{""version"":""v3"",""created"":""Sat, 23 Jan 2021 04:59:44 GMT""}]","2021-01-26"
"2011.10735","Pingyuan Wei","Ying Chao, Pingyuan Wei and Jinqiao Duan","Lyapunov Exponents for Hamiltonian Systems under Small L\'evy
  Perturbations",,,"10.1063/5.0058716",,"math.DS math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work is to investigate the (top) Lyapunov exponent for a class of
Hamiltonian systems under small non-Gaussian L\'evy noise. In a suitable moving
frame, the linearisation of such a system can be regarded as a small
perturbation of a nilpotent linear system. The Lyapunov exponent is then
estimated by taking a Pinsky-Wihstutz transformation and applying the
Khas'minskii formula, under appropriate assumptions on smoothness, ergodicity
and integrability. Finally, two examples are present to illustrate our results.
The results characterize the growth or decay rates of a class of dynamical
systems under the interaction between Hamiltonian structures and non-Gaussian
uncertainties.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:07:26 GMT""},{""version"":""v2"",""created"":""Thu, 28 Jan 2021 01:37:44 GMT""},{""version"":""v3"",""created"":""Wed, 2 Jun 2021 02:27:23 GMT""}]","2021-08-25"
"2011.10736","Sonja Verena Zeissner","Johannes Erdmann, Olaf Nackenhorst and Sonja Verena Zei{\ss}ner","Maximum performance of strange-jet tagging at hadron colliders","v2 includes the correction of a bug impacting the performance of the
  tracking detector scenario","JINST 16 (2021) 08, P08039","10.1088/1748-0221/16/08/P08039",,"hep-ex physics.ins-det","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The maximum achievable performance of strange-jet tagging at hadron colliders
and the loss in performance in different detector designs is estimated based on
simulated truth jets from strange-quark and down-quark hadronisation. Both jet
types are classified with a recurrent neural network using long short-term
memory units, at first using all available truth particles and then applying
selections to study the impacts of ideal tracking detectors, Cherenkov
detectors, and calorimeters. Additionally, a manual reconstruction of strange
hadron decays such as $K_S\rightarrow \pi^+ \pi^-$ from charged tracks is
considered.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:11:30 GMT""},{""version"":""v2"",""created"":""Thu, 10 Jun 2021 07:03:05 GMT""}]","2021-09-01"
"2011.10737","Jun Ma","Zilong Cheng, Yulin Li, Kai Chen, Jun Ma, Tong Heng Lee","Neural-iLQR: A Learning-Aided Shooting Method for Trajectory
  Optimization","7 pages, 7 figures",,,,"cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Iterative linear quadratic regulator (iLQR) has gained wide popularity in
addressing trajectory optimization problems with nonlinear system models.
However, as a model-based shooting method, it relies heavily on an accurate
system model to update the optimal control actions and the trajectory
determined with forward integration, thus becoming vulnerable to inevitable
model inaccuracies. Recently, substantial research efforts in learning-based
methods for optimal control problems have been progressing significantly in
addressing unknown system models, particularly when the system has complex
interactions with the environment. Yet a deep neural network is normally
required to fit substantial scale of sampling data. In this work, we present
Neural-iLQR, a learning-aided shooting method over the unconstrained control
space, in which a neural network with a simple structure is used to represent
the local system model. In this framework, the trajectory optimization task is
achieved with simultaneous refinement of the optimal policy and the neural
network iteratively, without relying on the prior knowledge of the system
model. Through comprehensive evaluations on two illustrative control tasks, the
proposed method is shown to outperform the conventional iLQR significantly in
the presence of inaccuracies in system models.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:17:28 GMT""},{""version"":""v2"",""created"":""Thu, 26 Aug 2021 07:57:42 GMT""},{""version"":""v3"",""created"":""Thu, 15 Sep 2022 14:10:20 GMT""}]","2022-09-16"
"2011.10738","Shweta Dahale","Shweta Dahale, Balasubramaniam Natarajan","Multi Time-scale Imputation aided State Estimation in Distribution
  System","5 pages, 6 figures, Preprint submitted to IEEE PES GM 2021",,,,"eess.SY cs.SY","http://creativecommons.org/licenses/by/4.0/","  With the transition to a smart grid, we are witnessing a significant growth
in sensor deployments and smart metering infrastructure in the distribution
system. However, information from these sensors and meters are typically
unevenly sampled at different time-scales and are incomplete. It is critical to
effectively aggregate these information sources for situational awareness. In
order to reconcile the heterogeneous multi-scale time-series data, we present a
multi-task Gaussian process framework. This framework exploits the
spatio-temporal correlation across the time-series data to impute data at any
desired time-scale while providing confidence bounds on the imputations. The
value of the imputed data for distribution system operation is illustrated via
a matrix completion based state estimation strategy. Results on the IEEE 37 bus
distribution system reveals the superior performance of the proposed approach
relative to linear interpolation approaches.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:26:05 GMT""}]","2020-11-24"
"2011.10739","Stefano Pinton Mr","Fabrizio Colombo, Jonathan Gantner, Stefano Pinton","An introduction to hyperholomorphic spectral theories and fractional
  powers of vector operators","28 pages",,,,"math.SP","http://creativecommons.org/publicdomain/zero/1.0/","  The aim of this paper is to give an overview of the spectral theories
associated with the notions of holomorphicity in dimension greater than one. A
first natural extension is the theory of several complex variables whose Cauchy
formula is used to define the holomorphic functional calculus for $n$-tuples of
operators $(A_1,...,A_n)$. A second way is to consider hyperholomorphic
functions of quaternionic or paravector variables. In this case, by the
Fueter-Sce-Qian mapping theorem, we have two different notions of
hyperholomorphic functions that are called slice hyperholomorphic functions and
monogenic functions. Slice hyperholomorphic functions generate the spectral
theory based on the $S$-spectrum while monogenic functions induce the spectral
theory based on the monogenic spectrum. There is also an interesting relation
between the two hyperholomorphic spectral theories via the $F$-functional
calculus. The two hyperholomorphic spectral theories have different and
complementary applications. Here we also discuss how to define the fractional
Fourier's law for nonhomogeneous materials, such definition is based on the
spectral theory on the $S$-spectrum.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:33:41 GMT""}]","2020-11-24"
"2011.10740","Deokkeun An","Deokkeun An, Timothy C. Beers","A Blueprint for the Milky Way's Stellar Populations. II. Improved
  Isochrone Calibration in the SDSS and Pan-STARRS Photometric Systems","19 pages, 11 figures, 3 tables, accepted for publication in the
  Astrophysical Journal",,"10.3847/1538-4357/abccd2",,"astro-ph.GA astro-ph.SR","http://creativecommons.org/licenses/by/4.0/","  We improve the identification and isolation of individual stellar populations
in the Galactic halo based on an updated set of empirically calibrated stellar
isochrones in the Sloan Digital Sky Survey (SDSS) and Pan-STARRS 1 (PS1)
photometric systems. Along the Galactic prime meridian ($l=0^{\circ}$ and
$180^{\circ}$), where proper motions and parallaxes from Gaia DR2 can be used
to compute rotational velocities of stars in the rest frame of the Milky Way,
we use the observed double color-magnitude sequences of stars having large
transverse motions, which are attributed to groups of stars in the metal-poor
halo and the thick disk with halo-like kinematics, respectively. The Gaia
sequences directly constrain color-magnitude relations of model colors, and
help to improve our previous calibration using Galactic star clusters. Based on
these updated sets of stellar isochrones, we confirm earlier results on the
presence of distinct groups of stars in the metallicity versus
rotational-velocity plane, and find that the distribution of the most
metal-poor ([Fe/H] $<-2$) stars in our sample can be modeled using two separate
groups on prograde and retrograde orbits, respectively. At $4$-$6$ kpc from the
Galactic plane, we find approximately equal proportions of the Splashed Disk,
and the metal-rich ($\langle {\rm [Fe/H]} \rangle\sim-1.6$) and metal-poor
($\langle {\rm [Fe/H]} \rangle\sim-2.5$) halos on prograde orbits. The
Gaia-Sausage-Enceladus, the metal-weak thick disk, and the retrograde halo
structure(s) ($\langle {\rm [Fe/H]} \rangle\sim-2.2$) constitute approximately
$10\%$ of the rest of the stellar populations at these distances.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:42:36 GMT""}]","2021-02-10"
"2011.10741","Kaixin Gao","Kai-Xin Gao, Xiao-Lei Liu, Zheng-Hai Huang, Min Wang, Zidong Wang,
  Dachuan Xu, Fan Yu","A Trace-restricted Kronecker-Factored Approximation to Natural Gradient",,,,,"cs.LG math.OC stat.ML","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Second-order optimization methods have the ability to accelerate convergence
by modifying the gradient through the curvature matrix. There have been many
attempts to use second-order optimization methods for training deep neural
networks. Inspired by diagonal approximations and factored approximations such
as Kronecker-Factored Approximate Curvature (KFAC), we propose a new
approximation to the Fisher information matrix (FIM) called Trace-restricted
Kronecker-factored Approximate Curvature (TKFAC) in this work, which can hold
the certain trace relationship between the exact and the approximate FIM. In
TKFAC, we decompose each block of the approximate FIM as a Kronecker product of
two smaller matrices and scaled by a coefficient related to trace. We
theoretically analyze TKFAC's approximation error and give an upper bound of
it. We also propose a new damping technique for TKFAC on convolutional neural
networks to maintain the superiority of second-order optimization methods
during training. Experiments show that our method has better performance
compared with several state-of-the-art algorithms on some deep network
architectures.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 07:47:14 GMT""}]","2020-11-24"
"2011.10742","Eva Arianna Aurelia Pogna","Eva A.A. Pogna, Mahdi Asgari, Valentina Zannier, Lucia Sorba, Leonardo
  Viti and Miriam S. Vitiello","Unveiling the detection dynamics of semiconductor nanowire
  photodetectors by terahertz near-field nanoscopy",,,,,"cond-mat.mtrl-sci physics.ins-det physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Semiconductor nanowire field-effect transistors represent a promising
platform for the development of room-temperature (RT) terahertz (THz) frequency
light detectors due to the strong nonlinearity of their transfer
characteristics and their remarkable combination of low noise-equivalent powers
(< 1 nW/Hz$^{1/2}$) and high responsivities (> 100 V/W). Nano-engineering a NW
photodetector combining high sensitivity with high speed (sub-ns) in the THz
regime at RT is highly desirable for many frontier applications in quantum
optics and nanophotonics, but this requires a clear understanding of the origin
of the photo-response. Conventional electrical and optical measurements,
however, cannot unambiguously determine the dominant detection mechanism due to
inherent device asymmetry that allows different processes to be simultaneously
activated. Here, we innovatively capture snapshots of the photo-response of
individual InAs nanowires via high spatial resolution (35 nm) THz photocurrent
nanoscopy. By coupling a THz quantum cascade laser to scattering-type scanning
near-field optical microscopy (s-SNOM) and monitoring both electrical and
optical readouts, we simultaneously measure transport and scattering
properties. The spatially resolved electric response provides unambiguous
signatures of photo-thermoelectric or bolometric currents whose interplay is
discussed as a function of photon density and material doping, therefore
providing a route to engineer photo-responses by design.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 08:01:57 GMT""}]","2020-11-24"
"2011.10743","Max Jwo Lem Lee","Max Jwo Lem Lee, Li-Ta Hsu, Hoi-Fung Ng, Shang Lee","Semantic-Based VPS for Smartphone Localization in Challenging Urban
  Environments","12 pages, 6 figures",,,,"cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Accurate smartphone-based outdoor localization system in deep urban canyons
are increasingly needed for various IoT applications such as augmented reality,
intelligent transportation, etc. The recently developed feature-based visual
positioning system (VPS) by Google detects edges from smartphone images to
match with pre-surveyed edges in their map database. As smart cities develop,
the building information modeling (BIM) becomes widely available, which
provides an opportunity for a new semantic-based VPS. This article proposes a
novel 3D city model and semantic-based VPS for accurate and robust pose
estimation in urban canyons where global navigation satellite system (GNSS)
tends to fail. In the offline stage, a material segmented city model is used to
generate segmented images. In the online stage, an image is taken with a
smartphone camera that provides textual information about the surrounding
environment. The approach utilizes computer vision algorithms to rectify and
hand segment between the different types of material identified in the
smartphone image. A semantic-based VPS method is then proposed to match the
segmented generated images with the segmented smartphone image. Each generated
image holds a pose that contains the latitude, longitude, altitude, yaw, pitch,
and roll. The candidate with the maximum likelihood is regarded as the precise
pose of the user. The positioning results achieves 2.0m level accuracy in
common high rise along street, 5.5m in foliage dense environment and 15.7m in
alleyway. A 45% positioning improvement to current state-of-the-art method. The
estimation of yaw achieves 2.3{\deg} level accuracy, 8 times the improvement to
smartphone IMU.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 08:18:43 GMT""}]","2020-11-24"
"2011.10744","Hiroyasu Ando","Hiroyasu Ando, T. Okamoto, H. Chang, T. Noguchi, and Shinji Nakaoka","Computation harvesting in road traffic dynamics",,,,,"cs.LG cs.ET","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Owing to recent advances in artificial intelligence and internet of things
(IoT) technologies, collected big data facilitates high computational
performance, while its computational resources and energy cost are large.
Moreover, data are often collected but not used. To solve these problems, we
propose a framework for a computational model that follows a natural
computational system, such as the human brain, and does not rely heavily on
electronic computers. In particular, we propose a methodology based on the
concept of `computation harvesting', which uses IoT data collected from rich
sensors and leaves most of the computational processes to real-world phenomena
as collected data. This aspect assumes that large-scale computations can be
fast and resilient. Herein, we perform prediction tasks using real-world road
traffic data to show the feasibility of computation harvesting. First, we show
that the substantial computation in traffic flow is resilient against sensor
failure and real-time traffic changes due to several combinations of harvesting
from spatiotemporal dynamics to synthesize specific patterns. Next, we show the
practicality of this method as a real-time prediction because of its low
computational cost. Finally, we show that, compared to conventional methods,
our method requires lower resources while providing a comparable performance.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 08:22:19 GMT""}]","2020-11-24"
"2011.10745","Giulio Vignoli","Anne-Sophie H{\o}yer, Giulio Vignoli, Thomas Mejer Hansen, Le Thanh
  Vu, Donald A. Keefer, and Flemming J{\o}rgensen","Multiple-point statistical simulation for hydrogeological models: 3-D
  training image development and conditioning strategies","20 pages","Hydrol.Earth Syst.Sci. 21 (2017)","10.5194/hess-21-6069-2017",,"physics.geo-ph","http://creativecommons.org/licenses/by/4.0/","  Most studies on the application of geostatistical simulations based on
multiple-point statistics (MPS) to hydrogeological modelling focus on
relatively fine-scale models and on the estimation of facies-level structural
uncertainty. Less attention is paid to the input data and the construction of
Training Images (TIs). E.g. even though the TI should capture a set of spatial
geological characteristics, the majority of the research still relies on 2D or
quasi-3D training images. Here, we demonstrate a novel strategy for 3D MPS
modelling characterized by (i) realistic 3D TIs and (ii) an effective workflow
for incorporating a diverse group of geological and geophysical data sets. The
study covers 2810 km^2 in southern Denmark. MPS simulations are performed on a
subset of the geological succession (the lower to middle Miocene sediments)
which is characterized by relatively uniform structures and dominated by sand
and clay. The simulated domain is large and each of the geostatistical
realizations contains approximately 45 x 10^6 voxels with size 100 m x 100 m x
5 m. Data used for the modelling include water well logs, seismic data, and a
previously published 3D geological model. We apply a series of different
strategies for the simulations based on data quality and develop a novel method
to effectively create observed spatial trends. The TI is constructed as a
relatively small 3D voxel model covering an area of 90 km^2. We use an
iterative training image development strategy and find that even slight
modifications in the TI create significant changes in simulations. Thus, this
study shows how to include both the geological environment and the type and
quality of input information in order to achieve optimal results from MPS
modelling. We present a practical workflow to build the TI and effectively
handle different types of input information to perform large-scale
geostatistical modelling
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 08:23:19 GMT""}]","2020-11-24"
"2011.10746","Slavko Simic Dr","Slavko Simic","Some Generalizations of Jensen's Inequality",,,,,"math.CA","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In this article we give some improvements and generalizations of the famous
Jensen's and Jensen-Mercer inequalities for twice differentiable functions,
where convexity property of the target function is not assumed in advance. They
represents a refinement of these inequalities in the case of convex/concave
functions with numerous applications in Theory of Means and Probability and
Statistics.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 08:32:36 GMT""}]","2020-11-24"
"2011.10747","Mengjin Zhao","Mengjin Zhao and Guangyan Jia","Continuous-Time Risk Contribution of the Terminal Variance and its
  Related Risk Budgeting Problem",,,,,"q-fin.MF","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  To achieve robustness of risk across different assets, risk parity investing
rules, a particular state of risk contributions, have grown in popularity over
the previous few decades. To generalize the concept of risk contribution from
the simple covariance matrix case to the continuous-time case in which the
terminal variance of wealth is used as the risk measure, we characterize risk
contributions and marginal risk contributions on various assets as predictable
processes using the Gateaux differential and Doleans measure. Meanwhile, the
risk contributions we extend here have the aggregation property, namely that
total risk can be represented as the aggregation of those among different
assets and $(t,\omega)$. Subsequently, as an inverse target -- allocating risk,
the risk budgeting problem of how to obtain policies whose risk contributions
coincide with pre-given risk budgets in the continuous-time case is also
explored in this paper. These policies are solutions to stochastic convex
optimizations parametrized by the pre-given risk budgets. Moreover,
single-period risk budgeting policies are explained as the projection of risk
budgeting policies in continuous-time cases. On the application side,
volatility-managed portfolios in [Moreira and Muir,2017] can be obtained by
risk budgeting optimization; similarly to previous findings, continuous-time
mean-variance allocation in [Zhou and Li, 2000] appears to be concentrated in
terms of risk contribution.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 08:41:17 GMT""},{""version"":""v2"",""created"":""Wed, 10 Mar 2021 00:54:12 GMT""},{""version"":""v3"",""created"":""Tue, 31 Aug 2021 03:31:54 GMT""},{""version"":""v4"",""created"":""Sun, 20 Feb 2022 15:02:44 GMT""}]","2022-02-22"
"2011.10748","Ernesto Berr\'ios-Caro","Ernesto Berr\'ios-Caro and Tobias Galla","Beyond the adiabatic limit in systems with fast environments: a
  $\tau$-leaping algorithm","22 pages, 6 figures","Phys. Rev. E 104, 014122 (2021)","10.1103/PhysRevE.104.014122",,"cond-mat.stat-mech q-bio.PE","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We propose a $\tau$-leaping simulation algorithm for stochastic systems
subject to fast environmental changes. Similar to conventional $\tau$-leaping
the algorithm proceeds in discrete time steps, but as a principal addition it
captures environmental noise beyond the adiabatic limit. The key idea is to
treat the input rates for the $\tau$-leaping as (clipped) Gaussian random
variables with first and second moments constructed from the environmental
process. In this way, each step of the algorithm retains environmental
stochasticity to sub-leading order in the time scale separation between system
and environment. We test the algorithm on several toy examples with discrete
and continuous environmental states, and find good performance in the regime of
fast environmental dynamics. At the same time, the algorithm requires
significantly less computing time than full simulations of the combined system
and environment. In this context we also discuss several methods for the
simulation of stochastic population dynamics in time-varying environments with
continuous states.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 08:48:54 GMT""}]","2021-07-28"
"2011.10749","Dongkwan Kim","Dongkwan Kim, Eunsoo Kim, Sang Kil Cha, Sooel Son, and Yongdae Kim","Revisiting Binary Code Similarity Analysis using Interpretable Feature
  Engineering and Lessons Learned","23 pages, accepted to IEEE Transactions on Software Engineering (June
  2022)",,"10.1109/TSE.2022.3187689",,"cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Binary code similarity analysis (BCSA) is widely used for diverse security
applications, including plagiarism detection, software license violation
detection, and vulnerability discovery. Despite the surging research interest
in BCSA, it is significantly challenging to perform new research in this field
for several reasons. First, most existing approaches focus only on the end
results, namely, increasing the success rate of BCSA, by adopting
uninterpretable machine learning. Moreover, they utilize their own benchmark,
sharing neither the source code nor the entire dataset. Finally, researchers
often use different terminologies or even use the same technique without citing
the previous literature properly, which makes it difficult to reproduce or
extend previous work. To address these problems, we take a step back from the
mainstream and contemplate fundamental research questions for BCSA. Why does a
certain technique or a certain feature show better results than the others?
Specifically, we conduct the first systematic study on the basic features used
in BCSA by leveraging interpretable feature engineering on a large-scale
benchmark. Our study reveals various useful insights on BCSA. For example, we
show that a simple interpretable model with a few basic features can achieve a
comparable result to that of recent deep learning-based approaches.
Furthermore, we show that the way we compile binaries or the correctness of
underlying binary analysis tools can significantly affect the performance of
BCSA. Lastly, we make all our source code and benchmark public and suggest
future directions in this field to help further research.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 08:54:48 GMT""},{""version"":""v2"",""created"":""Tue, 20 Jul 2021 08:17:14 GMT""},{""version"":""v3"",""created"":""Tue, 22 Feb 2022 11:15:06 GMT""},{""version"":""v4"",""created"":""Thu, 7 Jul 2022 01:45:58 GMT""}]","2022-07-08"
"2011.10750","Chen Xing","C. Xing, X. Cheng, and M. D. Ding","Evolution of the Toroidal Flux of CME Flux Ropes during Eruption","21 pages, 5 figures, 1 table; accepted for publication in The
  Innovation",,"10.1016/j.xinn.2020.100059",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Coronal mass ejections (CMEs) are large-scale explosions of the coronal
magnetic field. It is believed that magnetic reconnection significantly builds
up the core structure of CMEs, a magnetic flux rope, during the eruption.
However, the quantitative evolution of the flux rope, particularly its toroidal
flux, is still unclear. In this paper, we study the evolution of the toroidal
flux of the CME flux rope for four events. The toroidal flux is estimated as
the magnetic flux in the footpoint region of the flux rope, which is identified
by a method that simultaneously takes the coronal dimming and the hook of the
flare ribbon into account. We find that the toroidal flux of the CME flux rope
for all four events shows a two-phase evolution: a rapid increasing phase
followed by a decreasing phase. We further compare the evolution of the
toroidal flux with that of the Geostationary Operational Environmental
Satellites soft X-ray flux and find that they are basically synchronous in
time, except that the peak of the former is somewhat delayed. The results
suggest that the toroidal flux of the CME flux rope may be first quickly built
up by the reconnection mainly taking place in the sheared overlying field and
then reduced by the reconnection among the twisted field lines within the flux
rope, as enlightened by a recent 3D magnetohydrodynamic simulation of CMEs.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:00:34 GMT""}]","2020-11-24"
"2011.10751","Marco Zoli","Marco Zoli","First-passage probability: a test for DNA Hamiltonian parameters","This is a Pre-print. Accepted manuscript at the DOI","Phys. Chem. Chem. Phys. vol. 22, 26901 (2020)","10.1039/D0CP04046K",,"cond-mat.soft cond-mat.stat-mech physics.bio-ph q-bio.BM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A method is proposed to select the suitable sets of potential parameters for
a one-dimensional mesoscopic Hamiltonian model, first introduced to describe
the DNA melting transition and later extended to investigate thermodynamic and
dynamical properties of nucleic acids. The DNA base pair fluctuations are
considered as time dependent trajectories whose initial condition sets the no
crossing constraint enforced in the path integral for the first-passage
probability. Performing the path integration at room temperature, relations are
established between the cutoff on the amplitude of the base pair fluctuations
and the model parameters. In particular, it is shown that the non-linear
stacking parameter should be $\sim 1$. The formalism here developed may be
applied to compute the lifetime of open base pairs in three-dimensional helical
models for DNA molecules.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:30:32 GMT""}]","2020-12-09"
"2011.10752","Baudouin Denis de Senneville PhD","Baudouin Denis de Senneville, Mario Ries, Wilbert Bartels, Chrit
  Moonen","MRI-Guided High Intensity Focused Ultrasound of Liver and Kidney","27 pages, 5 figures",,"10.1007/174_2011_394","Interventional Magnetic Resonance Imaging, 2012","physics.med-ph cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  High Intensity Focused Ultrasound (HIFU) can be used to achieve a local
temperature increase deep inside the human body in a non-invasive way. MRI
guidance of the procedure allows in situ target definition. In addition, MRI
can be used to provide continuous temperature mapping during HIFU for spatial
and temporal control of the heating procedure and prediction of the final
lesion based on the received thermal dose. Temperature mapping of mobile organs
as kidney and liver is challenging, as well as real-time processing methods for
feedback control of the HIFU procedure. In this paper, recent technological
advances are reviewed in MR temperature mapping of these organs, in motion
compensation of the HIFU beam, in intercostal HIFU sonication, and in
volumetric ablation and feedback control strategies. Recent pre-clinical
studies have demonstrated the feasibility of each of these novel methods. The
perspectives to translate those advances into the clinic are addressed. It can
be concluded that MR guided HIFU for ablation in liver and kidney appears
feasible but requires further work on integration of technologically advanced
methods.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:37:06 GMT""}]","2020-11-24"
"2011.10753","Avik Pal","Avik Pal, Jonah Philion, Yuan-Hong Liao and Sanja Fidler","Emergent Road Rules In Multi-Agent Driving Environments","International Conference on Learning Representations (2021)","International Conference on Learning Representations, 2021",,,"cs.LG cs.AI cs.MA","http://creativecommons.org/licenses/by/4.0/","  For autonomous vehicles to safely share the road with human drivers,
autonomous vehicles must abide by specific ""road rules"" that human drivers have
agreed to follow. ""Road rules"" include rules that drivers are required to
follow by law -- such as the requirement that vehicles stop at red lights -- as
well as more subtle social rules -- such as the implicit designation of fast
lanes on the highway. In this paper, we provide empirical evidence that
suggests that -- instead of hard-coding road rules into self-driving algorithms
-- a scalable alternative may be to design multi-agent environments in which
road rules emerge as optimal solutions to the problem of maximizing traffic
flow. We analyze what ingredients in driving environments cause the emergence
of these road rules and find that two crucial factors are noisy perception and
agents' spatial density. We provide qualitative and quantitative evidence of
the emergence of seven social driving behaviors, ranging from obeying traffic
signals to following lanes, all of which emerge from training agents to drive
quickly to destinations without colliding. Our results add empirical support
for the social road rules that countries worldwide have agreed on for safe,
efficient driving.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:43:50 GMT""},{""version"":""v2"",""created"":""Wed, 17 Mar 2021 07:29:41 GMT""}]","2021-11-22"
"2011.10754","Libu\v{s}e Hannah Vep\v{r}ek","Libu\v{s}e Hannah Vep\v{r}ek, Patricia Seymour, Pietro Michelucci","Human computation requires and enables a new approach to ethical review","8 pages, 4 figures. This is a pre-publication draft submitted to 34th
  Conference on Neural Information Processing Systems (NeurIPS 2020),
  Vancouver, Canada",,,,"cs.CY cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  With humans increasingly serving as computational elements in distributed
information processing systems and in consideration of the profit-driven
motives and potential inequities that might accompany the emerging thinking
economy[1], we recognize the need for establishing a set of related ethics to
ensure the fair treatment and wellbeing of online cognitive laborers and the
conscientious use of the capabilities to which they contribute. Toward this
end, we first describe human-in-the-loop computing in context of the new
concerns it raises that are not addressed by traditional ethical research
standards. We then describe shortcomings in the traditional approach to ethical
review and introduce a dynamic approach for sustaining an ethical framework
that can continue to evolve within the rapidly shifting context of disruptive
new technologies.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:44:29 GMT""}]","2020-11-24"
"2011.10755","Jacques Magnaudet","Jacques Magnaudet and Micheline Abbas","Near-wall forces on a neutrally-buoyant spherical particle in an
  axisymmetric stagnation-point flow","35 pages, 6 figures",,"10.1017/jfm.2020.398",,"physics.flu-dyn","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Hydrodynamic forces acting on a neutrally-buoyant spherical particle immersed
in a wall-bounded axisymmetric stagnation point flow (Hiemenz-Homann flow) are
predicted, based on a suitable form of the reciprocal theorem. An approximate
algebraic form of the undisturbed velocity field is set up, mimicking the
gradual transition of the actual carrying flow throughout the boundary layer,
from a pure linear straining flow in the bulk to a parabolic flow at the wall.
The particle Reynolds number is assumed to be small and predictions based on
the creeping-flow assumption are first derived. Then, inertial corrections are
computed, assuming that the particle stands close enough to the wall for the
latter to be in the inner region of the disturbance. Predictions for the
time-dependent slip velocity between the particle and ambient fluid are
obtained in the form of a differential equation, first assuming that the
particle moves along the flow symmetry axis, then extending the analysis to
particles released at an arbitrary radial position. In the former case, these
predictions are compared with results provided by numerical simulations. When
the strain-based Reynolds number (built on the particle radius and strain rate
in the bulk) exceeds $0.1$, finite-inertia effects due to particle-wall
interactions and to the relative acceleration between the particle and fluid
are found to substantially modify the way the slip velocity varies with the
distance to the wall.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:52:51 GMT""}]","2020-11-24"
"2011.10756","Gioele Zardini","Gioele Zardini and Dejan Milojevic and Andrea Censi and Emilio
  Frazzoli","Co-Design of Embodied Intelligence: A Structured Approach","8 pages, 9 figures, To appear in the Proceedings of the 2021 IEEE/RSJ
  International Conference on Intelligent Robots and Systems",,"10.1109/IROS51168.2021.9636513",,"cs.RO cs.SY eess.SY","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We consider the problem of co-designing embodied intelligence as a whole in a
structured way, from hardware components such as propulsion systems and sensors
to software modules such as control and perception pipelines. We propose a
principled approach to formulate and solve complex embodied intelligence
co-design problems, leveraging a monotone co-design theory. The methods we
propose are intuitive and integrate heterogeneous engineering disciplines,
allowing analytical and simulation-based modeling techniques and enabling
interdisciplinarity. We illustrate through a case study how, given a set of
desired behaviors, our framework is able to compute Pareto efficient solutions
for the entire hardware and software stack of a self-driving vehicle.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:54:55 GMT""},{""version"":""v2"",""created"":""Fri, 1 Jan 2021 17:25:14 GMT""},{""version"":""v3"",""created"":""Sat, 6 Mar 2021 11:09:26 GMT""},{""version"":""v4"",""created"":""Fri, 30 Jul 2021 08:05:11 GMT""}]","2021-12-22"
"2011.10757","Xuelei Chen","Shifan Zuo, Jixia Li, Yichao Li, Das Santanu, Albert Stebbins, Kiyoshi
  W. Masui, Richard Shaw, Jiao Zhang, Fengquan Wu, Xuelei Chen","Data Processing Pipeline For Tianlai Experiment","13 pages, 5 figures, accepted for publication on Astronomy and
  Computing","Astronomy and Computing, Vol.34, 100439 (2021)","10.1016/j.ascom.2020.100439",,"astro-ph.IM physics.comp-ph","http://creativecommons.org/licenses/by/4.0/","  The Tianlai project is a 21cm intensity mapping experiment aimed at detecting
dark energy by measuring the baryon acoustic oscillation (BAO) features in the
large scale structure power spectrum. This experiment provides an opportunity
to test the data processing methods for cosmological 21cm signal extraction,
which is still a great challenge in current radio astronomy research. The 21cm
signal is much weaker than the foregrounds and easily affected by the
imperfections in the instrumental responses. Furthermore, processing the large
volumes of interferometer data poses a practical challenge. We have developed a
data processing pipeline software called {\tt tlpipe} to process the drift scan
survey data from the Tianlai experiment. It performs offline data processing
tasks such as radio frequency interference (RFI) flagging, array calibration,
binning, and map-making, etc. It also includes utility functions needed for the
data analysis, such as data selection, transformation, visualization and
others. A number of new algorithms are implemented, for example the eigenvector
decomposition method for array calibration and the Tikhonov regularization for
$m$-mode analysis. In this paper we describe the design and implementation of
the {\tt tlpipe} and illustrate its functions with some analysis of real data.
Finally, we outline directions for future development of this publicly code.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:14:15 GMT""}]","2023-06-05"
"2011.10758","Gioele Zardini","Gioele Zardini and Andrea Censi and Emilio Frazzoli","Co-Design of Autonomous Systems: From Hardware Selection to Control
  Synthesis","8 pages, 6 figures, to appear in the proceedings of the 20th European
  Control Conference (ECC21)",,"10.23919/ECC54610.2021.9654960",,"eess.SY cs.RO cs.SY","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Designing cyber-physical systems is a complex task which requires insights at
multiple abstraction levels. The choices of single components are deeply
interconnected and need to be jointly studied. In this work, we consider the
problem of co-designing the control algorithm as well as the platform around
it. In particular, we leverage a monotone theory of co-design to formalize
variations of the LQG control problem as monotone feasibility relations. We
then show how this enables the embedding of control co-design problems in the
higher level co-design problem of a robotic platform. We illustrate the
properties of our formalization by analyzing the co-design of an autonomous
drone performing search-and-rescue tasks and show how, given a set of desired
robot behaviors, we can compute Pareto efficient design solutions.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:14:44 GMT""},{""version"":""v2"",""created"":""Fri, 1 Jan 2021 17:30:49 GMT""},{""version"":""v3"",""created"":""Sat, 27 Mar 2021 09:46:17 GMT""}]","2022-01-11"
"2011.10759","Faizaan Sakib","Faizaan Sakib and Tilo Burghardt","Visual Recognition of Great Ape Behaviours in the Wild","4 pages, 4 figures, to be published in the proceedings of ICPR 2020
  at the Visual observation and analysis of Vertebrate And Insect Behaviour
  (VAIB) workshop",,,,"cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  We propose a first great ape-specific visual behaviour recognition system
utilising deep learning that is capable of detecting nine core ape behaviours.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:27:21 GMT""}]","2020-11-24"
"2011.10760","Sukrit Mittal","Sukrit Mittal and Dhish Kumar Saxena and Kalyanmoy Deb and Erik
  Goodman","Enhanced Innovized Repair Operator for Evolutionary Multi- and
  Many-objective Optimization",,,,"COIN Lab Report: 2020020","cs.NE cs.LG cs.PF","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  ""Innovization"" is a task of learning common relationships among some or all
of the Pareto-optimal (PO) solutions in multi- and many-objective optimization
problems. Recent studies have shown that a chronological sequence of
non-dominated solutions obtained in consecutive iterations during an
optimization run also possess salient patterns that can be used to learn
problem features to help create new and improved solutions. In this paper, we
propose a machine-learning- (ML-) assisted modelling approach that learns the
modifications in design variables needed to advance population members towards
the Pareto-optimal set. We then propose to use the resulting ML model as an
additional innovized repair (IR2) operator to be applied on offspring solutions
created by the usual genetic operators, as a novel mean of improving their
convergence properties. In this paper, the well-known random forest (RF) method
is used as the ML model and is integrated with various evolutionary multi- and
many-objective optimization algorithms, including NSGA-II, NSGA-III, and
MOEA/D. On several test problems ranging from two to five objectives, we
demonstrate improvement in convergence behaviour using the proposed IR2-RF
operator. Since the operator does not demand any additional solution
evaluations, instead using the history of gradual and progressive improvements
in solutions over generations, the proposed ML-based optimization opens up a
new direction of optimization algorithm development with advances in AI and ML
approaches.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:29:15 GMT""}]","2020-11-24"
"2011.10761","Yanghyun Byun","Yanghyun Byun, Julius Korbas, Peter Zvengrowski","Vector fields on projective Stiefel manifolds and the Browder-Dupont
  invariant","18 pages","Topology and its Application, 284 (2020) 107364",,,"math.GT","http://creativecommons.org/licenses/by/4.0/","  We develop strong lower bounds for the span of the projective Stiefel
manifolds $X_{n,r}=O(n)/(O(n-r)\times \mathbb Z/2)$, which enable very accurate
(in many cases exact) estimates of the span. The technique, for the most part,
involves elementary stability properties of vector bundles. However, the case
$X_{n,2}$ with $n$ odd presents extra difficulties, which are partially
resolved using the Browder-Dupont invariant. In the process, we observe that
the symmetric lift due to Sutherland does not necessarily exist for all odd
dimensional closed manifolds, and therefore the Browder-Dupont invariant, as he
formulated it, is not defined in general. We will characterize those $n$'s for
which the Browder-Dupont invariant is well-defined on $X_{n,2}$. Then the
invariant will be used in this case to obtain the lower bounds for the span as
a corollary of a stronger result.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:45:55 GMT""}]","2020-11-24"
"2011.10762","Charles Maniere","Charles Mani\`ere (SDSU), Elia Saccardo, Geuntak Lee (SDSU), Joanna
  Mckittrick (UCSD), Alberto Molinari, Eugene A. Olevsky (SDSU)","Swelling negation during sintering of sterling silver: An experimental
  and theoretical approach",,"Results in Physics, Elsevier, 2018, 11, pp.79-84","10.1016/j.rinp.2018.08.035",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  One of the main challenges of the sintering of sterling silver is the
phenomenon of swelling causing de-densification and a considerable reduction of
the sintering kinetics. This swelling phenomenon opposes sintering and it needs
to be addressed by a well-controlled processing atmosphere. In the present
study, the pressure-less sintering behavior of sterling silver is investigated
in air, argon, and vacuum. A specially modified spark plasma sintering mold is
designed to study the pressure-less sintering of sterling silver in a high
vacuum environment. The conducted analysis is extended to the new constitutive
equations of sintering enabling the prediction of the swelling phenomena and
the identification of the internal equivalent pressure that opposes the
sintering.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:50:04 GMT""}]","2020-11-24"
"2011.10763","Mingshan Jia","Mingshan Jia, Bogdan Gabrys, Katarzyna Musial","Measuring Quadrangle Formation in Complex Networks",,,,,"cs.SI","http://creativecommons.org/licenses/by/4.0/","  The classic clustering coefficient and the lately proposed closure
coefficient quantify the formation of triangles from two different
perspectives, with the focal node at the centre or at the end in an open triad
respectively. As many networks are naturally rich in triangles, they become
standard metrics to describe and analyse networks. However, the advantages of
applying them can be limited in networks, where there are relatively few
triangles but which are rich in quadrangles, such as the protein-protein
interaction networks, the neural networks and the food webs. This yields for
other approaches that would leverage quadrangles in our journey to better
understand local structures and their meaning in different types of networks.
Here we propose two quadrangle coefficients, i.e., the i-quad coefficient and
the o-quad coefficient, to quantify quadrangle formation in networks, and we
further extend them to weighted networks. Through experiments on 16 networks
from six different domains, we first reveal the density distribution of the two
quadrangle coefficients, and then analyse their correlations with node degree.
Finally, we demonstrate that at network-level, adding the average i-quad
coefficient and the average o-quad coefficient leads to significant improvement
in network classification, while at node-level, the i-quad and o-quad
coefficients are useful features to improve link prediction.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:51:03 GMT""}]","2020-11-24"
"2011.10764","Evangelos Latos","Evangelos A. Latos","Nonlocal reaction preventing blow-up in the supercritical case of
  chemotaxis",,,,,"math.AP","http://creativecommons.org/licenses/by/4.0/","  This paper studies the non-negative solutions of the Keller-Segel model with
a nonlocal nonlinear source in a bounded domain. The competition between the
aggregation and the nonlocal reaction term is highlighted: when the growth
factor is stronger than the dampening effect, with the help of the nonlocal
term, the model admits a classical solution which is uniformly bounded.
Moreover, when the growth factor is of the same order compared to the dampening
effect, the nonlocal nonlinear exponents can prevent the chemotactic collapse.
Global existence of classical solutions is shown for an appropriate range of
the exponents as well as convergence to the constant equilibrium state.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:51:25 GMT""}]","2020-11-24"
"2011.10765","Charles Maniere","Charles Mani\`ere (SDSU), Elisa Torresani (SDSU), Eugene A. Olevsky
  (SDSU)","Simultaneous Spark Plasma Sintering of Multiple Complex Shapes",,"Materials, MDPI, 2019, 12 (4), pp.557","10.3390/ma12040557",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work addresses the two great challenges of the spark plasma sintering
(SPS) process: the sintering of complex shapes and the simultaneous production
of multiple parts. A new controllable interface method is employed to
concurrently consolidate two nickel gear shapes by SPS. A graphite deformable
sub-mold is specifically designed for the mutual densification of the both
complex parts in a unique 40 mm powder deformation space. An energy efficient
SPS configuration is developed to allow the sintering of a large-scale powder
assembly under electric current lower than 900 A. The stability of the
developed process is studied by electro-thermal-mechanical (ETM) simulation.
The ETM simulation reveals that homogeneous densification conditions can be
attained by inserting an alumina powder at the sample/punches interfaces
enabling the energy efficient heating and the thermal confinement of the nickel
powder. Finally, the feasibility of the fabrication of the two near net shape
gears with a very homogeneous microstructure is demonstrated.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:55:32 GMT""}]","2020-11-24"
"2011.10766","Tristan Harder","Tristan H. Harder, Oleg A. Egorov, Constantin Krause, Johannes
  Beierlein, Philipp Gagel, Monika Emmerling, Christian Schneider, Ulf Peschel,
  Sven H\""ofling, and Sebastian Klembt","Kagome Flatbands for Coherent Exciton-Polariton Lasing","19 pages, 5 figures","ACS Photonics 8, 11, 3193-3200 (2021)","10.1021/acsphotonics.1c00950",,"physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Kagome lattices supporting Dirac cone and flatband dispersions are well known
as a highly frustrated, two-dimensional lattice system. Particularly the
flatbands therein are attracting continuous interest based on their link to
topological order, correlations and frustration. In this work, we realize
coupled microcavity implementations of Kagome lattices hosting
exciton-polariton quantum fluids of light. We demonstrate precise control over
the dispersiveness of the flatband as well as selective condensation of
exciton-polaritons into the flatband. Subsequently, we focus on the spatial and
temporal coherence properties of the laser-like emission from these polariton
condensates that are closely connected to the flatband nature of the system.
Notably, we find a drastic increase in coherence time due to the localization
of flatband condensates. Our work illustrates the outstanding suitability of
the exciton-polariton system for detailed studies of flatband states as a
platform for microlaser arrays in compact localized states, including strong
interactions, topology and non-linearity.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:56:36 GMT""}]","2021-11-19"
"2011.10767","Sylvain Marinel","Charles Mani\`ere (CRISMAT), Guillaume Riquet (CRISMAT), Sylvain
  Marinel (CRISMAT)","Dielectric properties of flash spark plasma sintered BaTiO$_3$ and
  CaCu$_3$Ti$_4$O$_{12}$",,"Scripta Materialia, Elsevier, 2019, 173, pp.41-45","10.1016/j.scriptamat.2019.07.048",,"physics.chem-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Flash sintering is an approach allowing reducing the sintering time to mere
seconds. To improve the microstructures and the properties of flash sintered
specimens, this process has been successfully adapted to pressure assisted
sintering such as the spark plasma sintering. This work is the exploration of
the potential of this ultra-rapid sintering process for the improvement of the
dielectric properties of well-known materials such as BaTiO$_3$ and
CaCu$_3$Ti$_4$O$_{12}$. In particular, we focus on the potential improvement of
the dielectric loss,
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:59:05 GMT""}]","2020-11-24"
"2011.10768","Charles Maniere","Charles Mani\`ere (SDSU), Eugene A. Olevsky (SDSU)","Porosity dependence of powder compaction constitutive parameters:
  Determination based on spark plasma sintering tests",,"Scripta Materialia, Elsevier, 2017, 141, pp.62-66","10.1016/j.scriptamat.2017.07.026",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The modeling of powder compaction process, such as spark plasma sintering
(SPS), requires the determination of the visco-plastic deformation behavior of
the particle material including the viscosity moduli. The establishment of
these parameters usually entails a long and difficult experimental campaign
which in particular involves several hot isostatic pressing tests. A more
straightforward method based on the coupled sinter-forging and die compaction
tests, which can be easily carried out in a regular SPS device, is presented.
Compared to classical creep mechanism studies, this comprehensive experimental
approach can reveal the in situ porous structure morphology influence on the
sintering process.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:00:09 GMT""}]","2020-11-24"
"2011.10769","Charles Maniere","Charles Mani\`ere (SDSU), Geuntak Lee (SDSU), Tony Zahrah, Eugene A.
  Olevsky (SDSU)","Microwave flash sintering of metal powders: From experimental evidence
  to multiphysics simulation",,"Acta Materialia, Elsevier, 2018, 147, pp.24-34","10.1016/j.actamat.2018.01.017",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Flash sintering phenomena are predominantly associated with ceramics due to
thermal runaway of their electric conductivity noticeably represented in
materials such as zirconia or silicon carbide. Because of their high electric
conductivity, flash sintering of metals is nearly inexistent. In this work, an
original metal powder flash sintering method based on a microwave approach is
presented. Within the developed approach, an unusually fast (60 s) thermal and
sintering runaway of Ti-6Al-4V powder is experimentally revealed under
microwave illumination. This phenomenon is simulated based on an
electromagnetic-thermal-mechanical (EMTM) model. The developed multiphysics
model reveals that the metal powder specimen's runaway does not result from its
intrinsic material properties, but results from the resonance phenomenon
thermally activated by the surrounding tooling material. The EMTM simulation
predicts with a very good accuracy the microwave repartition and the resulting
densification and powder specimen's shape distortions observed experimentally.
The comparison of the microwave and conventional sintering kinetics indicates
an important acceleration of the sintering behavior under microwave heating.
The developed sintering approach has a potential of the implementation for
time-effective mass production of small metal parts.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:03:19 GMT""}]","2020-11-24"
"2011.10770","V. G. Gurzadyan","M.Samsonyan, A.A.Kocharyan, A.Stepanian, V.G.Gurzadyan","Cosmic voids and induced hyperbolicity","5 pages, 1 figure, Eur. Phys. J. Plus (in press); to match the
  published version","Eur. Phys. J. Plus, 135, 946 (2020)","10.1140/epjp/s13360-020-00963-y",,"gr-qc astro-ph.IM","http://creativecommons.org/licenses/by/4.0/","  Cosmic voids - the low density regions in the Universe - as characteristic
features of the large scale matter distribution, are known for their hyperbolic
properties. The latter implies the deviation of photon beams due to their
underdensity, thus mimicing the negative curvature. We now show that the
hyperbolicity can be induced not only by negative curvature or underdensity but
also depends on the anisotropy of the photon beams.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:07:02 GMT""},{""version"":""v2"",""created"":""Wed, 2 Dec 2020 05:37:38 GMT""}]","2020-12-03"
"2011.10771","Vincent Huin","J\'er\^ome Delplanque, David Devos, Vincent Huin (JPArc), Alexandre
  Genet, Olivier Sand, Caroline Moreau, Cyril Goizet, Perrine Charles, Mathieu
  Anheim, Marie Lorraine Monin, Luc Bu\'ee, Alain Dest\'ee, Guillaume Grolez,
  Christine Delmaire, Kathy Dujardin, Delphine Dellacherie, Alexis Brice,
  Giovanni Stevanin, Isabelle Strubi-Vuillaume, Alexandra Durr, Bernard
  Sablonni\`ere","TMEM240 mutations cause spinocerebellar ataxia 21 with mental
  retardation and severe cognitive impairment",,"Brain - A Journal of Neurology , Oxford University Press (OUP),
  2014, 137 (10), pp.2657-2663","10.1093/brain/awu202",,"q-bio.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Autosomal dominant cerebellar ataxia corresponds to a clinically and
genetically heterogeneous group of neurodegenerative disorders that primarily
affect the cerebellum. Here, we report the identification of the causative gene
in spinocerebellar ataxia 21, an autosomal-dominant disorder previously mapped
to chromosome 7p21.3-p15.1. This ataxia was firstly characterized in a large
French family with slowly progressive cerebellar ataxia, accompanied by severe
cognitive impairment and mental retardation in two young children. Following
the recruitment of 12 additional young family members, linkage analysis enabled
us to definitively map the disease locus to chromosome 1p36.33-p36.32. The
causative mutation, (c.509C4T/p.P170L) in the transmembrane protein gene
TMEM240, was identified by whole exome sequencing and then was confirmed by
Sanger sequencing and co-segregation analyses. Index cases from 368 French
families with autosomal-dominant cerebellar ataxia were also screened for
mutations. In seven cases, we identified a range of missense mutations
(c.509C4T/p.P170L, c.239C4T/p.T80M, c.346C4T/p.R116C, c.445G4A/p.E149K,
c.511C4T/p.R171W), and a stop mutation (c.489C4G/p.Y163*) in the same gene.
TMEM240 is a small, strongly conserved transmembrane protein of unknown
function present in cerebellum and brain. Spinocerebellar ataxia 21 may be a
particular early-onset disease associated with severe cognitive impairment.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:16:05 GMT""}]","2020-11-24"
"2011.10772","Kemal Oksuz","Kemal Oksuz and Baris Can Cam and Sinan Kalkan and Emre Akbas","One Metric to Measure them All: Localisation Recall Precision (LRP) for
  Evaluating Visual Detection Tasks","Accepted to TPAMI",,,,"cs.CV","http://creativecommons.org/licenses/by/4.0/","  Despite being widely used as a performance measure for visual detection
tasks, Average Precision (AP) is limited in (i) reflecting localisation
quality, (ii) interpretability and (iii) robustness to the design choices
regarding its computation, and its applicability to outputs without confidence
scores. Panoptic Quality (PQ), a measure proposed for evaluating panoptic
segmentation (Kirillov et al., 2019), does not suffer from these limitations
but is limited to panoptic segmentation. In this paper, we propose Localisation
Recall Precision (LRP) Error as the average matching error of a visual detector
computed based on both its localisation and classification qualities for a
given confidence score threshold. LRP Error, initially proposed only for object
detection by Oksuz et al. (2018), does not suffer from the aforementioned
limitations and is applicable to all visual detection tasks. We also introduce
Optimal LRP (oLRP) Error as the minimum LRP Error obtained over confidence
scores to evaluate visual detectors and obtain optimal thresholds for
deployment. We provide a detailed comparative analysis of LRP Error with AP and
PQ, and use nearly 100 state-of-the-art visual detectors from seven visual
detection tasks (i.e. object detection, keypoint detection, instance
segmentation, panoptic segmentation, visual relationship detection, zero-shot
detection and generalised zero-shot detection) using ten datasets to
empirically show that LRP Error provides richer and more discriminative
information than its counterparts. Code available at:
https://github.com/kemaloksuz/LRP-Error
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:20:42 GMT""},{""version"":""v2"",""created"":""Tue, 13 Jul 2021 06:24:35 GMT""},{""version"":""v3"",""created"":""Sun, 21 Nov 2021 17:26:03 GMT""}]","2021-11-23"
"2011.10773","Esteban Mart\'inez-Vargas","Esteban Mart\'inez-Vargas, Christoph Hirche, Gael Sent\'is, Michalis
  Skotiniotis, Marta Carrizo, Ramon Mu\~noz-Tapia, John Calsamiglia","Quantum Sequential Hypothesis Testing","17 pages, 8 figures","Phys. Rev. Lett. 126, 180502 (2021)","10.1103/PhysRevLett.126.180502",,"quant-ph","http://creativecommons.org/licenses/by/4.0/","  We introduce sequential analysis in quantum information processing, by
focusing on the fundamental task of quantum hypothesis testing. In particular
our goal is to discriminate between two arbitrary quantum states with a
prescribed error threshold, $\epsilon$, when copies of the states can be
required on demand. We obtain ultimate lower bounds on the average number of
copies needed to accomplish the task. We give a block-sampling strategy that
allows to achieve the lower bound for some classes of states. The bound is
optimal in both the symmetric as well as the asymmetric setting in the sense
that it requires the least mean number of copies out of all other procedures,
including the ones that fix the number of copies ahead of time. For qubit
states we derive explicit expressions for the minimum average number of copies
and show that a sequential strategy based on fixed local measurements
outperforms the best collective measurement on a predetermined number of
copies. Whereas for general states the number of copies increases as $\log
1/\epsilon$, for pure states sequential strategies require a finite average
number of samples even in the case of perfect discrimination, i.e.,
$\epsilon=0$.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:24:07 GMT""},{""version"":""v2"",""created"":""Fri, 7 May 2021 10:08:10 GMT""}]","2021-05-10"
"2011.10774","Son Pham-Ba","Son Pham-Ba, Jean-Fran\c{c}ois Molinari","Creation and evolution of roughness on silica under unlubricated wear","12 pages, 10 figures, published in Wear",,"10.1016/j.wear.2021.203648",,"cond-mat.soft","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Friction and wear are important phenomena occurring in all devices with
moving parts. While their origin and the way they evolve over time are not
fully understood, they are both intimately linked to surface roughness. Guided
by pin-on-disc experiments, we present the steps giving rise to the formation
of surface roughness on silica, first by the creation of roughly spherical wear
particles whose size is related to a critical length scale governing the
transition between ductile and brittle behavior, then by the accumulation of
these small particles into a larger third body layer, or gouge. We show that,
for the explored range of loading conditions, the surface roughness evolves
toward a common steady state under unlubricated wear regardless of the initial
surface roughness, hinting toward the possible predictability of roughness
evolution in wearing components. The friction coefficient is shown to be
related to the surface roughness and its time evolution is discussed as well.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:24:19 GMT""},{""version"":""v2"",""created"":""Fri, 30 Jul 2021 14:36:50 GMT""}]","2021-08-02"
"2011.10775","Liu-Di Lu","Olivier Bernard (BIOCORE), Liu-Di Lu (BIOCORE, ANGE, LJLL
  (UMR\_7598)), Julien Salomon (ANGE, LJLL (UMR\_7598))","Mixing strategies combined with shape design to enhance productivity of
  a raceway pond",,,,,"math.NA cs.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper focuses on mixing strategies and designing shape of the bottom
topographies to enhance the growth of the microalgae in raceway ponds. A
physical-biological coupled model is used to describe the growth of the algae.
A simple model of a mixing device such as a paddle wheel is also considered.
The complete process model was then included in an optimization problem
associated with the maximization of the biomass production. The results show
that non-trivial topographies can be coupled with some specific mixing
strategies to improve the microalgal productivity.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:25:26 GMT""}]","2020-11-24"
"2011.10776","Suping Wu","Lei Li, Suping Wu","DmifNet:3D Shape Reconstruction Based on Dynamic Multi-Branch
  Information Fusion","ICPR 2020 (Oral)",,,,"cs.CV","http://creativecommons.org/licenses/by/4.0/","  3D object reconstruction from a single-view image is a long-standing
challenging problem. Previous work was difficult to accurately reconstruct 3D
shapes with a complex topology which has rich details at the edges and corners.
Moreover, previous works used synthetic data to train their network, but domain
adaptation problems occurred when tested on real data. In this paper, we
propose a Dynamic Multi-branch Information Fusion Network (DmifNet) which can
recover a high-fidelity 3D shape of arbitrary topology from a 2D image.
Specifically, we design several side branches from the intermediate layers to
make the network produce more diverse representations to improve the
generalization ability of network. In addition, we utilize DoG (Difference of
Gaussians) to extract edge geometry and corners information from input images.
Then, we use a separate side branch network to process the extracted data to
better capture edge geometry and corners feature information. Finally, we
dynamically fuse the information of all branches to gain final predicted
probability. Extensive qualitative and quantitative experiments on a
large-scale publicly available dataset demonstrate the validity and efficiency
of our method. Code and models are publicly available at
https://github.com/leilimaster/DmifNet.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:31:27 GMT""}]","2020-11-24"
"2011.10777","Alden Waters","Alden Waters","Observability for Schr\""odinger equations with quadratic Hamiltonians","Theorem 4, Example 1, and Corollary 2, are new to this version, Lemma
  3 has been replaced with a more general argument. Typos corrected and details
  in the computations expanded",,,,"math.AP math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider time dependent harmonic oscillators and construct a parametrix to
the corresponding Schr\""odinger equation using Gaussian wavepackets. This
parametrix of Gaussian wavepackets is precise and tractable. Using this
parametrix we prove $L^2$ and $L^2-L^{\infty}$ observability estimates on
unbounded domains $\omega$ for a restricted class of initial data. This data
includes a class of compactly supported piecewise $C^1$ functions which have
been extended from characteristic functions. Initial data of this form which
has the bulk of its mass away from $\omega^c=\Omega$, a connected bounded
domain, is observable, but data centered over $\Omega$ must be very nearly a
single Gaussian to be observable. We also give counterexamples to established
principles for the simple harmonic oscillator in the case of certain time
dependent harmonic oscillators.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:43:05 GMT""},{""version"":""v2"",""created"":""Mon, 7 Jun 2021 09:49:18 GMT""},{""version"":""v3"",""created"":""Tue, 15 Jun 2021 15:55:03 GMT""},{""version"":""v4"",""created"":""Mon, 30 May 2022 07:03:05 GMT""}]","2022-05-31"
"2011.10778","Lei Li","Shi Jin and Lei Li and Yiqun Sun","On the Random Batch Method for second order interacting particle systems",,,"10.1016/j.jcp.2019.108877",,"math.NA cs.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigate several important issues regarding the Random Batch Method
(RBM) for second order interacting particle systems. We first show the
uniform-in-time strong convergence for second order systems under suitable
contraction conditions. Secondly, we propose the application of RBM for
singular interaction kernels via kernel splitting strategy, and investigate
numerically the application to molecular dynamics.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:48:07 GMT""}]","2020-12-02"
"2011.10779","Wille Liu","Wille Liu","Knizhnik--Zamolodchikov functor for degenerate double affine Hecke
  algebras : algebraic theory","47 pages, accepted by Represent. Theory","Represent. Theory 26 (2022), 906-961","10.1090/ert/614","MPIM-Bonn-2022","math.RT math.QA","http://creativecommons.org/licenses/by-nc-sa/4.0/","  In this article, we define an algebraic version of the
Knizhnik--Zamolodchikov functor for the degenerate double affine Hecke algebras
(a.k.a. trigonometric Cherednik algebras). We compare it with the KZ monodromy
functor constructed by Varagnolo--Vasserot. We prove the double centraliser
property for our functor and give a characterisation of its kernel. We
establish these results for a family of algebras, called quiver double Hecke
algebras, which includes the degenerate double affine Hecke algebras as special
cases.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 12:28:38 GMT""},{""version"":""v2"",""created"":""Mon, 2 May 2022 15:11:37 GMT""}]","2022-09-13"
"2011.10780","Rami Katz","Rami Katz, Idan Basre and Emilia Fridman","Delayed finite-dimensional observer-based control of 1D heat equation
  under Neumann actuation",,,,,"math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently a constructive method was introduced for finite-dimensional
observer-based control of 1D parabolic PDEs. In this paper we present an
improved method in terms of the reduced-order LMIs (that significantly shorten
the computation time) and introduce predictors to manage with larger delays. We
treat the case of a 1D heat equation under Neumann actuation and non-local
measurement, that has not been studied yet. We apply modal decomposition and
prove $L^2$ exponential stability by a direct Lyapunov method. We provide
reduced-order LMI conditions for finding the observer dimension $N$ and
resulting decay rate. The LMI dimension does not grow with $N$. The LMI is
always feasible for large $N$, and feasibility for $N$ implies feasibility for
$N+1$. For the first time we manage with delayed implementation of the
controller in the presence of fast-varying (without any constraints on the
delay-derivative) input and output delays. To manage with larger delays, we
construct classical observer-based predictors. For the known input delay, the
LMIs dimension does not grow with $N$, whereas for unknown one the LMIs
dimension grows, but it is ssentially smaller than in the existing results. A
numerical example demonstrates the efficiency of our method.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 12:35:12 GMT""},{""version"":""v2"",""created"":""Sun, 16 Jan 2022 13:38:47 GMT""}]","2022-01-19"
"2011.10781","Deepak Raina","Aniruddha Singhal, Ayush Kumar, Shivam Thukral, Deepak Raina, Swagat
  Kumar","Chitrakar: Robotic System for Drawing Jordan Curve of Facial Portrait",,,,,"cs.RO","http://creativecommons.org/licenses/by/4.0/","  This paper presents a robotic system (\textit{Chitrakar}) which autonomously
converts any image of a human face to a recognizable non-self-intersecting loop
(Jordan Curve) and draws it on any planar surface. The image is processed using
Mask R-CNN for instance segmentation, Laplacian of Gaussian (LoG) for feature
enhancement and intensity-based probabilistic stippling for the image to points
conversion. These points are treated as a destination for a travelling salesman
and are connected with an optimal path which is calculated heuristically by
minimizing the total distance to be travelled. This path is converted to a
Jordan Curve in feasible time by removing intersections using a combination of
image processing, 2-opt, and Bresenham's Algorithm. The robotic system
generates $n$ instances of each image for human aesthetic judgement, out of
which the most appealing instance is selected for the final drawing. The
drawing is executed carefully by the robot's arm using trapezoidal velocity
profiles for jerk-free and fast motion. The drawing, with a decent resolution,
can be completed in less than 30 minutes which is impossible to do by hand.
This work demonstrates the use of robotics to augment humans in executing
difficult craft-work instead of replacing them altogether.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 12:44:42 GMT""},{""version"":""v2"",""created"":""Tue, 15 Jun 2021 14:04:09 GMT""},{""version"":""v3"",""created"":""Mon, 28 Jun 2021 18:12:47 GMT""}]","2021-06-30"
"2011.10782","Andrey Kudlis","M.V. Kompaniets, A. Kudlis, A.I. Sokolov","Critical behavior of weakly disordered Ising model: Six-loop $\sqrt
  \varepsilon$ expansion study",,"Phys. Rev. E 103, 022134 (2021)","10.1103/PhysRevE.103.022134",,"cond-mat.stat-mech","http://creativecommons.org/licenses/by/4.0/","  The critical behavior of three-dimensional weakly diluted quenched Ising
model is examined on the base of six-loop renormalization group expansions
obtained within the minimal subtraction scheme in $4-\epsilon$ space
dimensions. For this purpose the $\phi^4$ field theory with cubic symmetry was
analyzed in the replica limit $n\rightarrow 0$. Along with renormalization
group expansions in terms of renormalized couplings the $\sqrt{\varepsilon}$
expansions of critical exponents are presented. Corresponding numerical
estimates for the physical, three-dimensional system are obtained by means of
different resummation procedures applied both to the $\sqrt{\varepsilon}$
series and to initial renormalization group expansions. The results given by
the latter approach are in a good agreement with their counterparts obtained
experimentally and within the Monte Carlo simulations, while resumming of
$\sqrt{\varepsilon}$ series themselves turned out to be disappointing.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 12:44:46 GMT""},{""version"":""v2"",""created"":""Wed, 28 Apr 2021 17:10:12 GMT""}]","2021-04-29"
"2011.10783","Shinji Machida","S. Machida, D. J. Kelliher, J-B. Lagrange and C. T. Rogers","Optics Design of Vertical Excursion Fixed-Field Alternating Gradient
  Accelerators","13 pages, 13 figures, to be published in a journal","Phys. Rev. Accel. Beams 24, 021601 (2021)","10.1103/PhysRevAccelBeams.24.021601",,"physics.acc-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Vertical excursion fixed-field alternating gradient accelerators can be
designed with tunes that are invariant with respect to momentum and
trajectories that are scaled images of each other displaced only in the
vertical direction. This is possible using guiding fields that have a vertical
exponential increase, with a skew quadrupole component in the magnet body and a
solenoid component at the magnet ends. Because of the coupling this introduces,
orbit and optics calculations and optimisation of parameters need to be
performed numerically. In this paper, idealised magnetic fields are calculated
from first principles, taking into account end fields. The parameter dependence
of the optics and the dynamic aperture of the ring are calculated for the
example of a ring with an approximately 25 m circumference that accelerates
proton beams from 3 MeV to 12 MeV. The paper reports for the first time the
design of such an accelerator lattice using tools specifically devised to
analyse transverse coupled optics without the need for approximations.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 12:47:19 GMT""}]","2021-03-03"
"2011.10784","Irene Cavallari","Irene Cavallari, Giovanni Federico Gronchi, Giulio Bau'","On the Sun-shadow dynamics",,,,,"math.DS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigate the planar motion of a mass particle in a force field defined
by patching Kepler's and Stark's dynamics. This model is called Sun-shadow
dynamics, referring to the motion of an Earth satellite perturbed by the solar
radiation pressure and considering the Earth shadow effect. The existence of
periodic orbits of brake type is proved, and the Sun-shadow dynamics is
investigated by means of a Poincare'-like map defined by a quantity that is not
conserved along the flow. We also present the results of our numerical
investigations on some properties of the map. Moreover, we construct the
invariant manifolds of the hyperbolic fixed points related to the periodic
orbits of brake type. The global picture of the map shows evidence of regular
and chaotic behaviour.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:02:05 GMT""}]","2020-11-24"
"2011.10785","Chinelo Oribhabor","Chinelo Blessing Oribhabor","Evaluating the Effect of Activity Based Method of Teaching Mathematics
  on Nigerian Secondary School Students Achievement in Mathematics","11 pages","Puissant A Multidisciplinary Journal Vol. 1, (2020) pp. 77-87",,"ISSN print: 2719-0153; ISSN online: 2719-0161","math.HO","http://creativecommons.org/licenses/by/4.0/","  Mathematics is a compulsory subject in Nigerian secondary schools, and the
subject plays an important role in the scientific and technological growth and
development of the nation. A shortfall in the knowledge of the students in
Mathematics means that the goal may not be realized, hence the need to improve
teaching methods for solving the problem of poor performance in the subject.
This study evaluated the effect of the activity-based teaching method on the
students' achievement in secondary school Mathematics. The design of the study
was a quasi-experimental pretest-posttest research design using intact classes.
Finding revealed that there was a significant difference in the Mathematics
performance between the posttest mean scores of the students who were exposed
to activity-based teaching methods (experimental) and those that were taught
with lecture method (control) groups after controlling for the effect of the
pre-test on Mathematics scores. The paper recommends among others that
secondary school Mathematics teachers should be trained and retrained to update
their knowledge in the use of activity-based teaching for making the teaching
and learning of Mathematics more interesting and rewarding.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:05:55 GMT""}]","2020-11-24"
"2011.10786","Martha Liliana Cortes","M. L. Cort\'es, W. Rodriguez, P. Doornenbal, A. Obertelli, J. D. Holt,
  J. Men\'endez, K. Ogata, A. Schwenk, N. Shimizu, J. Simonis, Y. Utsuno, K.
  Yoshida, L. Achouri, H. Baba, F. Browne, D. Calvet, F. Ch\^ateau, S. Chen, N.
  Chiga, A. Corsi, A. Delbart, J-M. Gheller, A. Giganon, A. Gillibert, C.
  Hilaire, T. Isobe, T. Kobayashi, Y. Kubota, V. Lapoux, H. N. Liu, T.
  Motobayashi, I. Murray, H. Otsu, V. Panin, N. Paul, H. Sakurai, M. Sasano, D.
  Steppenbeck, L. Stuhl, Y. L. Sun, Y. Togano, T. Uesaka, K. Wimmer, K. Yoneda,
  O. Aktas, T. Aumann, L. X. Chung, F. Flavigny, S. Franchoo, I.
  Ga\v{s}pari\'c, R.-B. Gerst, J. Gibelin, K. I. Hahn, D. Kim, T. Koiwai, Y.
  Kondo, P. Koseoglou, J. Lee, C. Lehr, B. D. Linh, T. Lokotko, M. MacCormick,
  K. Moschner, T. Nakamura, S. Y. Park, D. Rossi, E. Sahin, P.-A.
  S\""oderstr\""om, D. Sohler, S. Takeuchi, H. Toernqvist, V. Vaquero, V. Wagner,
  S. Wang, V. Werner, X. Xu, H. Yamada, D. Yan, Z. Yang, M. Yasuda, L. Zanetti","$\boldsymbol{N=32}$ shell closure below calcium: Low-lying structure of
  $^{50}$Ar","Accepted for Phys. Rev. C","Phys. Rev. C 102, 064320 (2020)","10.1103/PhysRevC.102.064320",,"nucl-ex","http://creativecommons.org/licenses/by/4.0/","  Low-lying excited states in the $N=32$ isotope $^{50}$Ar were investigated by
in-beam $\gamma$-ray spectroscopy following proton- and neutron-knockout,
multi-nucleon removal, and proton inelastic scattering at the RIKEN Radioactive
Isotope Beam Factory. The energies of the two previously reported transitions
have been confirmed, and five additional states are presented for the first
time, including a candidate for a 3$^-$ state. The level scheme built using
$\gamma\gamma$ coincidences was compared to shell-model calculations in the
$sd-pf$ model space, and to ab initio predictions based on chiral two- and
three-nucleon interactions. Theoretical proton- and neutron-knockout cross
sections suggest that two of the new transitions correspond to $2^+$ states,
while the previously proposed $4^+$ state could also correspond to a $2^+$
state.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:13:38 GMT""}]","2021-01-04"
"2011.10787","Gunel Jahangirova","Gunel Jahangirova, David Clark, Mark Harman, Paolo Tonella","An Empirical Study on Failed Error Propagation in Java Programs with
  Real Faults",,,,,"cs.SE","http://creativecommons.org/licenses/by/4.0/","  During testing, developers can place oracles externally or internally with
respect to a method. Given a faulty execution state, i.e., one that differs
from the expected one, an oracle might be unable to expose the fault if it is
placed at a program point with no access to the incorrect program state or
where the program state is no longer corrupted. In such a case, the oracle is
subject to failed error propagation.
  We conducted an empirical study to measure failed error propagation on
Defects4J, the reference benchmark for Java programs with real faults,
considering all 6 projects available (386 real bugs and 459 fixed methods). Our
results indicate that the prevalence of failed error propagation is negligible
when testing is performed at the unit level. However, when system-level inputs
are provided, the prevalence of failed error propagation increases
substantially. This indicates that it is enough for method postconditions to
predicate only on the externally observable state/data and that intermediate
steps should be checked when testing at system level.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:16:31 GMT""}]","2020-11-24"
"2011.10788","Zipeng Hu","Zipeng Hu, Mark R. Krumholz, Christoph Federrath, Riwaj Pokhrel and
  Robert A. Gutermuth","Reconstructing three-dimensional densities from two-dimensional
  observations of molecular gas","14 pages, 14 figures. Accepted by MNRAS",,"10.1093/mnras/stab356",,"astro-ph.GA astro-ph.IM","http://creativecommons.org/licenses/by/4.0/","  Star formation has long been known to be an inefficient process, in the sense
that only a small fraction $\epsilon_{\rm ff}$ of the mass of any given gas
cloud is converted to stars per cloud free-fall time. However, developing a
successful theory of star formation will require measurements of both the mean
value of $\epsilon_{\rm ff}$ and its scatter from one molecular cloud to
another. Because $\epsilon_{\rm ff}$ is measured relative to the free-fall
time, such measurements require accurate determinations of cloud volume
densities. Efforts to measure the volume density from two-dimensional projected
data, however, have thus far relied on treating molecular clouds as simple
uniform spheres, while their real shapes are likely filamentary and their
density distributions far from uniform. The resulting uncertainty in the true
volume density is likely one of the major sources of error in observational
estimates of $\epsilon_{\rm ff}$. In this paper, we use a suite of simulations
of turbulent, magnetized, radiative, self-gravitating star-forming clouds to
examine whether it is possible to obtain more accurate volume density estimates
and thereby reduce this error. We create mock observations from simulations,
and show that current analysis methods relying on the spherical assumption
likely yield ~ 0.26 dex underestimations and ~ 0.51 dex errors in volume
density estimates, corresponding to a ~ 0.13 dex overestimation and a ~ 0.25
dex scatter in $\epsilon_{\rm ff}$, comparable to the scatter in observed cloud
samples. We build a predictive model that uses information accessible in
two-dimensional measurements -- most significantly the Gini coefficient of the
surface density distribution -- to estimate volume density with ~ 0.3 dex less
scatter. We test our method on a recent observation of the Ophiuchus cloud, and
show that it successfully reduces the $\epsilon_{\rm ff}$ scatter.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:26:47 GMT""},{""version"":""v2"",""created"":""Thu, 4 Feb 2021 20:26:25 GMT""}]","2021-02-17"
"2011.10789","Philipp Krause","Philipp Klaus Krause","lospre in linear time",,,,,"cs.PL cs.DM","http://creativecommons.org/licenses/by-sa/4.0/","  Lifetime-optimal speculative partial redundancy elimination (lospre) is the
most advanced currently known redundancy elimination technique. It subsumes
many previously known approaches, such as common subexpression elimination,
global common subexpression elimination, and loop-invariant code motion.
However, previously known lospre algorithms have high time complexity; faster
but less powerful approaches have been used and developed further instead. We
present a simple linear-time algorithm for lospre for structured programs that
can also handle some more general scenarios compared to previous approaches. We
prove that our approach is optimal and that the runtime is linear in the number
of nodes in the control-flow graph. The condition on programs of being
structured is automatically true for many programming languages and for others,
such as C, is equivalent to a bound on the number of goto labels per function.
An implementation in a mainstream C compiler demonstrates the practical
feasibility of our approach. Our approach is based on graph-structure theory
and uses tree-decompositions. We also show that, for structured programs, the
runtime of deterministic implementations of the previously known MC-PRE and
MC-SSAPRE algorithms can be bounded by $O(n^{2.5})$, improving the previous
bounds of $O(n^3)$.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:35:54 GMT""}]","2020-11-24"
"2011.10790","Gordon Blower","Gordon Blower","Convexity and transport for the isentropic Euler equations on the sphere",,,,,"math.AP math.DS","http://creativecommons.org/licenses/by/4.0/","  The paper considers the Euler system of PDE on a smooth compact Riemannian
manifold of positive curvature without boundary, and the sphere
${\mathbb{S}}^2$ in particular. The paper interprets the Euler equations as a
transport problem for the fluid density under dynamics governed by the gradient
of the internal energy of the fluid. The paper develops the notion of transport
cost in the tangent bundle, and compares its properties with the Wasserstein
transportation cost on the manifold. There are applications to the discrete
approximation to the Euler equations in the style of Gangbo and Wesdickenberg
({\sl Comm. Partial Diff. Equations} {\bf 34} (2009), 1041-1073), except that
the analysis is heavily dependent upon the curvature of the underlying
manifold. The internal energy is assumed to satisfy convexity conditions that
allow analysis via $\Phi$-entropy entropy-production inequalities, and the
results apply to the power law $\rho^\gamma$ where $1<\gamma<3/2$, which
includes the case of a diatomic gas. The paper proves existence of weak
solutions of the continuity equation, and gives a sufficient condition for
existence of weak solutions to the acceleration equation.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:48:28 GMT""}]","2020-11-24"
"2011.10791","Suyun Jiang","Suyun Jiang and Jin Yan","Disjoint cycles covering specified vertices in bipartite graphs with
  partial degrees","19 pages, 4 figures",,,,"math.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Let $k$ be a positive integer. Let $G$ be a balanced bipartite graph of order
$2n$ with bipartition $(X, Y)$, and $S$ a subset of $X$. Suppose that every
pair of nonadjacent vertices $(x,y)$ with $x\in S, y\in Y$ satisfies
$d(x)+d(y)\geq n+1$. We show that if $|S|\geq 2k+2$, then $G$ contains $k$
disjoint cycles covering $S$ such that each of the $k$ cycles contains at least
two vertices of $S$. Here, both the degree condition and the lower bound of
$|S|$ are best possible. And we also show that if $|S|=2k+1$, then $G$ contains
$k$ disjoint cycles such that each of the $k$ cycles contains at least two
vertices of $S$.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:49:35 GMT""}]","2020-11-24"
"2011.10792","Koondanibha Mitra PhD","K. Mitra, A. R\""atz, and B. Schweizer","Travelling wave solutions for gravity fingering in porous media flows","28 pages, 6 figures, prepared for journal submission",,"10.13140/RG.2.2.23096.78083",,"math.AP","http://creativecommons.org/licenses/by/4.0/","  We study an imbibition problem for porous media. When a wetted layer is above
a dry medium, gravity leads to the propagation of the water downwards into the
medium. In experiments, the occurrence of fingers was observed, a phenomenon
that can be described with models that include hysteresis. In the present
paper, we describe a single finger in a moving frame and set up a free boundary
problem to describe the shape and the motion of one finger that propagates with
a constant speed. We show the existence of solutions to the travelling wave
problem and investigate the system numerically.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 13:55:01 GMT""}]","2020-11-24"
"2011.10793","Yuejuan Xi","Yaozhong Hu and Yuejuan Xi","Parameter estimation for threshold Ornstein-Uhlenbeck processes from
  discrete observations",,,,,"math.ST stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Assuming that a threshold Ornstein-Uhlenbeck process is observed at discrete
time instants, we propose generalized moment estimators to estimate the
parameters. Our theoretical basis is the celebrated ergodic theorem. To use
this theorem we need to find the explicit form of the invariant measure. With
the sampling time step arbitrarily fixed, we prove the strong consistency and
asymptotic normality of our estimators as the sample size tends to infinity.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:04:37 GMT""}]","2020-11-24"
"2011.10794","Nandish Chattopadhyay","Nandish Chattopadhyay, Lionell Yip En Zhi, Bryan Tan Bing Xing and
  Anupam Chattopadhyay","Spatially Correlated Patterns in Adversarial Images","Submitted for review",,,,"cs.AI","http://creativecommons.org/licenses/by/4.0/","  Adversarial attacks have proved to be the major impediment in the progress on
research towards reliable machine learning solutions. Carefully crafted
perturbations, imperceptible to human vision, can be added to images to force
misclassification by an otherwise high performing neural network. To have a
better understanding of the key contributors of such structured attacks, we
searched for and studied spatially co-located patterns in the distribution of
pixels in the input space. In this paper, we propose a framework for
segregating and isolating regions within an input image which are particularly
critical towards either classification (during inference), or adversarial
vulnerability or both. We assert that during inference, the trained model looks
at a specific region in the image, which we call Region of Importance (RoI);
and the attacker looks at a region to alter/modify, which we call Region of
Attack (RoA). The success of this approach could also be used to design a
post-hoc adversarial defence method, as illustrated by our observations. This
uses the notion of blocking out (we call neutralizing) that region of the image
which is highly vulnerable to adversarial attacks but is not important for the
task of classification. We establish the theoretical setup for formalising the
process of segregation, isolation and neutralization and substantiate it
through empirical analysis on standard benchmarking datasets. The findings
strongly indicate that mapping features into the input space preserves the
significant patterns typically observed in the feature-space while adding major
interpretability and therefore simplifies potential defensive mechanisms.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:06:59 GMT""}]","2020-11-24"
"2011.10795","Enrico Brehm","Enrico M. Brehm","Defects and Perturbation","28 pages, 3 figures, 1 table, references added, some typos corrected",,"10.1007/JHEP04(2021)300",,"hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigate perturbatively tractable deformations of topological defects
in two-dimensional conformal field theories. We perturbatively compute the
change in the $g$-factor, the reflectivity, and the entanglement entropy of the
conformal defect at the end of these short RG flows. We also give instances of
such flows in the diagonal Virasoro and Super-Virasoro Minimal Models.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:07:58 GMT""},{""version"":""v2"",""created"":""Tue, 5 Jan 2021 11:02:37 GMT""}]","2021-05-19"
"2011.10796","Stanislav Shirokov","Maxim Nikonov, Mikhail Chekal, Stanislav Shirokov, Andrey Baryshev,
  and Vladimir Gorokhov","The line-of-sight analysis of spatial distribution of galaxies in the
  COSMOS2015 catalogue","21 pages, 18 figures, Universe accepted 2020.11.19","Universe 2020, 6(11), 215","10.3390/universe6110215",,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  New observations of high-redshift objects are crucial for the improvement of
the standard $\Lambda$CDM cosmological model and our understanding of the
Universe. One of the main directions of modern observational cosmology is the
analysis of the large-scale structure of Universe, in particular, in deep
fields. We study the large-scale structure of the Universe along the line of
sight using the latest version of the COSMOS2015 catalogue, which contains
518,404 high quality photometric redshifts of galaxies selected in the optical
range of the COSMOS field ($2\times 2$ deg$^2$), with depth up to the redshift
$z \sim 6$. We analyze large-scale fluctuations in the number of galaxies along
the line of sight and provide an estimate of the average linear sizes of the
self-correlating fluctuations (structures) in independent redshift bins of $
\Delta z = 0.1 $ along with the estimate of the standard deviation from
homogeneity (the observed cosmic variance). We suggest a new method of the
line-of-sight analysis based on previous works and formulate further prospects
of method development. For the case of the theoretical form of approximation of
homogeneity in the $\Lambda$CDM framework, the average standard deviation of
detected structures from homogeneity is $ \sigma_\text{mean}^{\Lambda
\text{CDM}} = 0.09 \pm 0.02 $, and the average characteristic size of
structures is $ R_\text{mean}^{\Lambda \text{CDM}} = 790 \pm 150 $ Mpc. For the
case of the empirical approximation of homogeneity, the average standard
deviation of detected structures from homogeneity is $
\sigma_\text{mean}^\text{empiric} = 0.08 \pm 0.01 $, and the average
characteristic size of structures is $ R_\text{mean}^\text{empiric} = 640 \pm
140 $ Mpc.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:08:15 GMT""}]","2020-11-24"
"2011.10797","Nicolas Garcia Trillos","Nicolas Garcia Trillos and Ryan Murray","Adversarial Classification: Necessary conditions and geometric flows",,,,,"cs.LG cs.CR math.AP stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study a version of adversarial classification where an adversary is
empowered to corrupt data inputs up to some distance $\varepsilon$, using tools
from variational analysis. In particular, we describe necessary conditions
associated with the optimal classifier subject to such an adversary. Using the
necessary conditions, we derive a geometric evolution equation which can be
used to track the change in classification boundaries as $\varepsilon$ varies.
This evolution equation may be described as an uncoupled system of differential
equations in one dimension, or as a mean curvature type equation in higher
dimension. In one dimension, and under mild assumptions on the data
distribution, we rigorously prove that one can use the initial value problem
starting from $\varepsilon=0$, which is simply the Bayes classifier, in order
to solve for the global minimizer of the adversarial problem for small values
of $\varepsilon$. In higher dimensions we provide a similar result, albeit
conditional to the existence of regular solutions of the initial value problem.
In the process of proving our main results we obtain a result of independent
interest connecting the original adversarial problem with an optimal transport
problem under no assumptions on whether classes are balanced or not. Numerical
examples illustrating these ideas are also presented.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:14:12 GMT""},{""version"":""v2"",""created"":""Fri, 11 Mar 2022 18:46:40 GMT""}]","2022-03-14"
"2011.10798","Bo Li","Bo Li, Anmol Gulati, Jiahui Yu, Tara N. Sainath, Chung-Cheng Chiu,
  Arun Narayanan, Shuo-Yiin Chang, Ruoming Pang, Yanzhang He, James Qin, Wei
  Han, Qiao Liang, Yu Zhang, Trevor Strohman, Yonghui Wu","A Better and Faster End-to-End Model for Streaming ASR","Accepted in ICASSP 2021",,,,"eess.AS cs.SD","http://creativecommons.org/licenses/by/4.0/","  End-to-end (E2E) models have shown to outperform state-of-the-art
conventional models for streaming speech recognition [1] across many
dimensions, including quality (as measured by word error rate (WER)) and
endpointer latency [2]. However, the model still tends to delay the predictions
towards the end and thus has much higher partial latency compared to a
conventional ASR model. To address this issue, we look at encouraging the E2E
model to emit words early, through an algorithm called FastEmit [3]. Naturally,
improving on latency results in a quality degradation. To address this, we
explore replacing the LSTM layers in the encoder of our E2E model with
Conformer layers [4], which has shown good improvements for ASR. Secondly, we
also explore running a 2nd-pass beam search to improve quality. In order to
ensure the 2nd-pass completes quickly, we explore non-causal Conformer layers
that feed into the same 1st-pass RNN-T decoder, an algorithm called Cascaded
Encoders [5]. Overall, we find that the Conformer RNN-T with Cascaded Encoders
offers a better quality and latency tradeoff for streaming ASR.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:17:40 GMT""},{""version"":""v2"",""created"":""Thu, 11 Feb 2021 14:07:45 GMT""}]","2021-02-12"
"2011.10799","Boris Chidlovskii","Leonid Antsfeld, Boris Chidlovskii, Emilio Sansano-Sansano","Deep Smartphone Sensors-WiFi Fusion for Indoor Positioning and Tracking",,,,,"cs.LG cs.NI cs.RO","http://creativecommons.org/licenses/by/4.0/","  We address the indoor localization problem, where the goal is to predict
user's trajectory from the data collected by their smartphone, using inertial
sensors such as accelerometer, gyroscope and magnetometer, as well as other
environment and network sensors such as barometer and WiFi. Our system
implements a deep learning based pedestrian dead reckoning (deep PDR) model
that provides a high-rate estimation of the relative position of the user.
Using Kalman Filter, we correct the PDR's drift using WiFi that provides a
prediction of the user's absolute position each time a WiFi scan is received.
Finally, we adjust Kalman Filter results with a map-free projection method that
takes into account the physical constraints of the environment (corridors,
doors, etc.) and projects the prediction on the possible walkable paths. We
test our pipeline on IPIN'19 Indoor Localization challenge dataset and
demonstrate that it improves the winner's results by 20\% using the challenge
evaluation protocol.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:20:49 GMT""}]","2020-11-24"
"2011.10800","Colin Petitjean","Arafat Abbar, Cl\'ement Coine and Colin Petitjean","On the dynamics of Lipschitz operators",,,,,"math.FA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  By the linearization property of Lipschitz-free spaces, any Lipschitz map $f
: M \to N$ between two pointed metric spaces may be extended uniquely to a
bounded linear operator $\widehat{f} : \mathcal F(M) \to \mathcal F(N)$ between
their corresponding Lipschitz-free spaces. In this note, we explore the
connections between the dynamics of Lipschitz self-maps $f : M \to M$ and the
linear dynamics of their extensions $\widehat{f} : \mathcal F(M) \to \mathcal
F(M)$. This not only allows us to relate topological dynamical systems to
linear dynamical systems but also provide a new class of hypercyclic operators
acting on Lipschitz-free spaces.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:28:25 GMT""}]","2020-11-24"
"2011.10801","Gi-Ren Liu","Gi-Ren Liu, Yuan-Chung Sheu, Hau-Tieng Wu","Central and Non-central Limit Theorems arising from the Scattering
  Transform and its Neural Activation Generalization",,,,,"stat.ML cs.LG math.PR","http://creativecommons.org/licenses/by/4.0/","  Motivated by analyzing complicated and non-stationary time series, we study a
generalization of the scattering transform (ST) that includes broad neural
activation functions, which is called neural activation ST (NAST). On the
whole, NAST is a transform that comprises a sequence of ``neural processing
units'', each of which applies a high pass filter to the input from the
previous layer followed by a composition with a nonlinear function as the
output to the next neuron. Here, the nonlinear function models how a neuron
gets excited by the input signal. In addition to showing properties like
non-expansion, horizontal translational invariability and insensitivity to
local deformation, the statistical properties of the second order NAST of a
Gaussian process with various dependence and (non-)stationarity structure and
its interaction with the chosen high pass filters and activation functions are
explored and central limit theorem (CLT) and non-CLT results are provided.
Numerical simulations are also provided. The results explain how NAST processes
complicated and non-stationary time series, and pave a way towards statistical
inference based on NAST under the non-null case.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:31:57 GMT""}]","2020-11-24"
"2011.10802","Juri Fiaschi","Juri Fiaschi, Michael Klasen, Miguel Vargas, Christian Weinheimer and
  Sybrand Zeinstra","MeV neutrino dark matter in the SLIM model","6 pages, 2 figures, Proceedings of The 40th International Conference
  on High Energy Physics, ICHEP-2020; Jul 28-Aug 6, 2020, Prague, Czech
  Republic",,,"MS-TP-20-39","hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We explore the parameter space of a variant of the SLIM model, which extends
the SM with a singlet and a doublet of complex scalars and two generations of
right-handed neutrinos, the lightest of which has a mass in the MeV to GeV
region and plays the role of Dark Matter candidate. We impose the current
collider and astrophysical constrains, as well as bounds from Lepton Flavour
Violating experiments. We also consider the discovery potential in the XENON
experiment exploiting the electron recoil as a possible direct detection
signal. Despite the DM in this model being leptophilic, the predicted cross
sections are too low due to the heavy charged mediator.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:35:40 GMT""}]","2020-11-24"
"2011.10803","Juri Fiaschi","Juri Fiaschi and Michael Klasen","Resummation effects in weak SUSY processes","6 pages, 6 figures, Proceedings of The 40th International Conference
  on High Energy Physics, ICHEP-2020; Jul 28-Aug 6, 2020, Prague, Czech
  Republic",,,"MS-TP-20-38","hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present updated results for the production cross sections of slepton pairs
and neutralino-chargino pairs at the LHC with next-to-next-to logarithmic
precision matched at approximate QCD next-to-next-to leading order. The
explored range of masses of the supersymmetric particles are chosen to be
relevant for current and future searches at the LHC. We find moderate increases
in the invariant mass distributions and integrated cross sections, and
substantial reductions in the scale uncertainty of the results.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:36:25 GMT""}]","2020-11-24"
"2011.10804","Tianchen Zhao","Tianchen Zhao, Xuefei Ning, Xiangsheng Shi, Songyi Yang, Shuang Liang,
  Peng Lei, Jianfei Chen, Huazhong Yang, Yu Wang","BARS: Joint Search of Cell Topology and Layout for Accurate and
  Efficient Binary ARchitectures",,,,,"cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Binary Neural Networks (BNNs) have received significant attention due to
their promising efficiency. Currently, most BNN studies directly adopt
widely-used CNN architectures, which can be suboptimal for BNNs. This paper
proposes a novel Binary ARchitecture Search (BARS) flow to discover superior
binary architecture in a large design space. Specifically, we analyze the
information bottlenecks that are related to both the topology and layout
architecture design choices. And we propose to automatically search for the
optimal information flow. To achieve that, we design a two-level (Macro &
Micro) search space tailored for BNNs and apply a differentiable neural
architecture search (NAS) to explore this search space efficiently. The
macro-level search space includes width and depth decisions, which is required
for better balancing the model performance and complexity. We also design the
micro-level search space to strengthen the information flow for BNN. %A notable
challenge of BNN architecture search lies in that binary operations exacerbate
the ""collapse"" problem of differentiable NAS, for which we incorporate various
search and derive strategies to stabilize the search process. On CIFAR-10, BARS
achieves 1.5% higher accuracy with 2/3 binary operations and 1/10
floating-point operations comparing with existing BNN NAS studies. On ImageNet,
with similar resource consumption, BARS-discovered architecture achieves a 6%
accuracy gain than hand-crafted binary ResNet-18 architectures and outperforms
other binary architectures while fully binarizing the architecture backbone.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:38:44 GMT""},{""version"":""v2"",""created"":""Mon, 21 Dec 2020 07:38:32 GMT""},{""version"":""v3"",""created"":""Sat, 27 Mar 2021 05:54:26 GMT""}]","2021-03-30"
"2011.10805","Mark Anthony Carroll","Mark Anthony Carroll, Giampaolo D'Alessandro, Gian Luca Lippi,
  Gian-Luca Oppo, and Francesco Papoff","Thermal, quantum anti-bunching and lasing thresholds from single
  emitters to macroscopic devices","12 pages, 9 figures, Supplementary Material is found at the end of
  the main paper","Phys. Rev. Lett. 126, 063902 (2021)","10.1103/PhysRevLett.126.063902",,"physics.optics","http://creativecommons.org/licenses/by/4.0/","  Starting from a fully quantized Hamiltonian for an ensemble of identical
emitters coupled to the modes of an optical cavity, we determine analytically
regimes of thermal, collective anti-bunching and laser emission that depend
explicitly on the number of emitters. The lasing regime is reached for a number
of emitters above a critical number (which depends on the light-matter
coupling, detuning and the dissipation rates) via a universal transition from
thermal emission to collective anti-bunching to lasing as the pump increases.
Cases where the second order intensity correlation fails to predict laser
action are also presented.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:52:50 GMT""},{""version"":""v2"",""created"":""Wed, 23 Dec 2020 13:50:43 GMT""}]","2021-02-17"
"2011.10806","Michael H\""ogele","G. Barrera, Michael A. H\""ogele, J.C. Pardo","The cutoff phenomenon in total variation for nonlinear Langevin systems
  with small layered stable noise","68p","Electronic Journal of Probability 2021, Volume 26, Number 119,
  2021, 1-76","10.1214/21-EJP685",,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper provides an extended case study of the cutoff phenomenon for a
prototypical class of nonlinear Langevin systems with a single stable state
perturbed by an additive pure jump L\'evy noise of small amplitude
$\varepsilon>0$, where the driving noise process is of layered stable type.
Under a drift coercivity condition the associated family of processes
$X^\varepsilon$ turns out to be exponentially ergodic with equilibrium
distribution $\mu^{\varepsilon}$ in total variation distance which extends a
result from Peng and Zhang (2018) to arbitrary polynomial moments. The main
results establish the cutoff phenomenon with respect to the total variation,
under a sufficient smoothing condition of Blumenthal-Getoor index $\alpha>3/2$.
That is to say, in this setting we identify a deterministic time scale
$\mathfrak{t}_{\varepsilon}^{\mathrm{cut}}$ satisfying $\mathfrak{t}_
\varepsilon^{\mathrm{cut}} \rightarrow \infty$, as $\varepsilon \rightarrow 0$,
and a respective time window, $\mathfrak{t}_\varepsilon^{\mathrm{cut}} \pm
o(\mathfrak{t}_\varepsilon^{\mathrm{cut}})$, during which the total variation
distance between the current state and its equilibrium $\mu^{\varepsilon}$
essentially collapses as $\varepsilon$ tends to zero. In addition, we extend
the dynamical characterization under which the latter phenomenon can be
described by the convergence of such distance to a unique profile function
first established in Barrera and Jara (2020) to the L\'evy case for nonlinear
drift. This leads to sufficient conditions, which can be verified in examples,
such as gradient systems subject to small symmetric $\alpha$-stable noise for
$\alpha>3/2$. The proof techniques differ completely from the Gaussian case due
to the absence of respective Girsanov transforms which couple the nonlinear
equation and the linear approximation asymptotically even for short times.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:55:02 GMT""},{""version"":""v2"",""created"":""Fri, 16 Jul 2021 22:26:12 GMT""}]","2023-05-05"
"2011.10807","Lukasz Stawarz","Anna Wojtowicz, Lukasz Stawarz, Jerzy Machalski, Luisa Ostorero","A Novel Method for Estimating the Ambient Medium Density Around Distant
  Radio Sources from Their Observed Radio Spectra","Accepted for publication in the Astrophysical Journal",,"10.3847/1538-4357/ac116c",,"astro-ph.GA astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The dynamical evolution and radiative properties of luminous radio galaxies
and quasars of the FRII type, are well understood. As a result, through the use
of detailed modeling of the observed radio emission of such sources, one can
estimate various physical parameters of the systems, including the density of
the ambient medium into which the radio structure evolves. This, however,
requires rather comprehensive observational information, i.e. sampling the
broad-band radio continua of the targets at several frequencies, and imaging
their radio structures with high resolution. Such observations are, on the
other hand, not always available, especially for high-redshift objects. Here we
analyze the best-fit values of the source physical parameters, derived from an
extensive modeling of the largest currently available sample of FRII radio
sources, for which good-quality multi-wavelength radio flux measurements could
be collected. In the analyzed dataset, we notice a significant and non-obvious
correlation between the spectral index of the non-thermal radio emission
continuum, and density of the ambient medium. We derive the corresponding
correlation parameters, and quantify the intrinsic scatter by means of Bayesian
analysis. We propose that the discovered correlation could be used as a
cosmological tool to estimate the density of ambient medium for large samples
of distant radio galaxies. Our method does not require any detailed modeling of
individual sources, and relies on limited observational information, namely the
slope of the radio continuum between the rest-frame frequencies 0.4GHz and
5GHz, possibly combined with the total linear size of the radio structure.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:55:36 GMT""},{""version"":""v2"",""created"":""Tue, 22 Jun 2021 09:24:43 GMT""},{""version"":""v3"",""created"":""Thu, 1 Jul 2021 10:52:12 GMT""}]","2021-12-08"
"2011.10808","Themis Mavrogordatos","Th. K. Mavrogordatos","Atom-field correlations in the weak-excitation limit of absorptive
  optical bistability","8 pages, 4 figures, 24 references","JETP Letters, Vol. 112, No. 5, pp. 274-282 (2020)","10.1134/S0021364020170014",,"quant-ph cond-mat.mes-hall cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We calculate the steady-state and first-order time varying atom-field
correlation functions in the weak-excitation limit of absorptive optical
bistability from a linearized theory of quantum fluctuations. We formulate a
Fokker-Planck equation in the positive P representation following the
phase-space analysis of [H. J. Carmichael, Phys. Rev. A 33, 3262 (1986)] which
is suitable for the determination of cross-correlations as it does not resort
to adiabatic elimination. Special emphasis is placed on the limit of collective
strong coupling as attained from a vanishing photon-loss rate. We compare to
the cavity-transmission spectrum with reference to experimental results
obtained for macroscopic dissipative systems, discussing the role of anomalous
correlations arising as distinct nonclassical features.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:59:11 GMT""}]","2020-11-24"
"2011.10809","Sophie Morier-Genoud","Sophie Morier-Genoud, Valentin Ovsienko","Quantum real numbers and $q$-deformed Conway-Coxeter friezes","12 pages, 2 figures",,,,"math.QA math.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We explain the notion of ""$q$-deformed real numbers"" introduced in our
previous work and overview their main properties. We will also introduce
$q$-deformed Conway-Coxeter friezes.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 15:03:56 GMT""},{""version"":""v2"",""created"":""Mon, 22 Feb 2021 10:42:29 GMT""}]","2021-02-23"
"2011.10810","Anatoly Khina","Anatoly Khina, Arie Yeredor, Ram Zamir","Monotonicity of the Trace-Inverse of Covariance Submatrices and
  Two-Sided Prediction","25 pages, 3 figures",,,,"eess.SP cs.IT cs.SY eess.SY math.IT math.ST stat.TH","http://creativecommons.org/licenses/by/4.0/","  It is common to assess the ""memory strength"" of a stationary process looking
at how fast the normalized log-determinant of its covariance submatrices (i.e.,
entropy rate) decreases. In this work, we propose an alternative
characterization in terms of the normalized trace-inverse of the covariance
submatrices. We show that this sequence is monotonically non-decreasing and is
constant if and only if the process is white. Furthermore, while the entropy
rate is associated with one-sided prediction errors (present from past), the
new measure is associated with two-sided prediction errors (present from past
and future). This measure can be used as an alternative to Burg's
maximum-entropy principle for spectral estimation. We also propose a
counterpart for non-stationary processes, by looking at the average
trace-inverse of subsets.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 15:24:53 GMT""}]","2020-11-24"
"2011.10811","Nikita Ustinov","Nikita Ustinov","On the constancy of the extremal function in the embedding theorem of
  fractional order","12 pages",,,,"math.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider the problem of the minimizer constancy in the fractional
embedding theorem $\mathcal{H}^s(\Omega) \hookrightarrow L_q(\Omega)$ for a
bounded Lipschitz domain $\Omega,$ depending on the domain size. For the family
of domains $\varepsilon \Omega,$ we prove that for small dilation coefficients
$\varepsilon$ a unique minimizer is constant, whereas for large $\varepsilon$ a
constant function is not even a local minimizer. We also discuss whether a
constant function is a global minimizer if it is a local one.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 15:30:08 GMT""}]","2020-11-24"
"2011.10812","Fan Lu","Fan Lu, Guang Chen, Yinlong Liu, Zhijun Li, Sanqing Qu, Tianpei Zou","MoNet: Motion-based Point Cloud Prediction Network",,,,,"cs.CV","http://creativecommons.org/licenses/by/4.0/","  Predicting the future can significantly improve the safety of intelligent
vehicles, which is a key component in autonomous driving. 3D point clouds
accurately model 3D information of surrounding environment and are crucial for
intelligent vehicles to perceive the scene. Therefore, prediction of 3D point
clouds has great significance for intelligent vehicles, which can be utilized
for numerous further applications. However, due to point clouds are unordered
and unstructured, point cloud prediction is challenging and has not been deeply
explored in current literature. In this paper, we propose a novel motion-based
neural network named MoNet. The key idea of the proposed MoNet is to integrate
motion features between two consecutive point clouds into the prediction
pipeline. The introduction of motion features enables the model to more
accurately capture the variations of motion information across frames and thus
make better predictions for future motion. In addition, content features are
introduced to model the spatial content of individual point clouds. A recurrent
neural network named MotionRNN is proposed to capture the temporal correlations
of both features. Besides, we propose an attention-based motion align module to
address the problem of missing motion features in the inference pipeline.
Extensive experiments on two large scale outdoor LiDAR datasets demonstrate the
performance of the proposed MoNet. Moreover, we perform experiments on
applications using the predicted point clouds and the results indicate the
great application potential of the proposed method.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 15:43:31 GMT""}]","2020-11-24"
"2011.10813","Zirui Xu","Zirui Xu, Jinjun Xiong, Fuxun Yu, Xiang Chen","Efficient Neural Network Implementation with Quadratic Neuron","2 pages, 1 figure, accepted by 2020 3rd IBM IEEE CAS/EDS AI Compute
  Symposium",,,,"cs.NI","http://creativecommons.org/licenses/by/4.0/","  Previous works proved that the combination of the linear neuron network with
nonlinear activation functions (e.g. ReLu) can achieve nonlinear function
approximation. However, simply widening or deepening the network structure will
introduce some training problems. In this work, we are aiming to build a
comprehensive second-order CNN implementation framework that includes
neuron/network design and system deployment optimization.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 15:46:46 GMT""}]","2020-11-24"
"2011.11438","Priya Malpani","Priya Malpani","Let's Burn a hole in the Binomial state of the radiation field","Nonclassicality of vacuum filtered binomial state has been studied",,,,"quant-ph","http://creativecommons.org/licenses/by/4.0/","  In quantum optics, nonclassical properties of various quantum states of
radiation field are frequently studied. Some of those states are finite
dimensional and referred to as qudits. These states are important because of
their potential applications in quantum information processing. Further,
nonclassical states are those which do not have any classical counterpart.
Consequently, to establish quantum supremacy, we always require nonclassical
state. Recently, Sivakumar and Meher have studied the nonclassical properties
of the number state filtered coherent state, and shown that the number state
filtering introduces nonclassical features into coherent state which is
otherwise classical. This observation motivated us to investigate the role of
hole burning (state filtering) on a state which is already nonclassical.
Specifically, we have selected a Binomial state which is known to be
nonclassical as our test bed and burnt a hole at vacuum (equivalently filtered
the vacuum state). To check the nonclassical properties of vacuum filtered
binomial state, we have used Vogel's criterion, criterion of higher- and
lower-order antibunching, criterion of higher-order sub-Poissonian photon
statistics, Linear entropy etc. The investigation results show that vacuum
filtered binomial state studied here is highly nonclassical, and the hole
burning process enhances the nonclassical depth.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 18:36:17 GMT""}]","2020-11-24"
"2011.11632","Faiq Khalid","Faiq Khalid, Syed Rafay Hasan, Sara Zia, Osman Hasan, Falah Awwad,
  Muhammad Shafique","MacLeR: Machine Learning-based Run-Time Hardware Trojan Detection in
  Resource-Constrained IoT Edge Devices",,"IEEE Transactions on Computer-Aided Design of Integrated Circuits
  and Systems ( Volume: 39, Issue: 11, Nov. 2020)","10.1109/TCAD.2020.3012236",,"cs.CR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Traditional learning-based approaches for run-time Hardware Trojan detection
require complex and expensive on-chip data acquisition frameworks and thus
incur high area and power overhead. To address these challenges, we propose to
leverage the power correlation between the executing instructions of a
microprocessor to establish a machine learning-based run-time Hardware Trojan
(HT) detection framework, called MacLeR. To reduce the overhead of data
acquisition, we propose a single power-port current acquisition block using
current sensors in time-division multiplexing, which increases accuracy while
incurring reduced area overhead. We have implemented a practical solution by
analyzing multiple HT benchmarks inserted in the RTL of a system-on-chip (SoC)
consisting of four LEON3 processors integrated with other IPs like vga_lcd,
RSA, AES, Ethernet, and memory controllers. Our experimental results show that
compared to state-of-the-art HT detection techniques, MacLeR achieves 10\%
better HT detection accuracy (i.e., 96.256%) while incurring a 7x reduction in
area and power overhead (i.e., 0.025% of the area of the SoC and <0.07% of the
power of the SoC). In addition, we also analyze the impact of process variation
and aging on the extracted power profiles and the HT detection accuracy of
MacLeR. Our analysis shows that variations in fine-grained power profiles due
to the HTs are significantly higher compared to the variations in fine-grained
power profiles caused by the process variations (PV) and aging effects.
Moreover, our analysis demonstrates that, on average, the HT detection accuracy
drop in MacLeR is less than 1% and 9% when considering only PV and PV with
worst-case aging, respectively, which is ~10x less than in the case of the
state-of-the-art ML-based HT detection technique.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 00:45:25 GMT""}]","2020-11-25"
"2011.11633","Charles Maniere","Charles Mani\`ere (SDSU), Geuntak Lee (SDSU), Eugene A. Olevsky (SDSU)","Proportional integral derivative, modeling and ways of stabilization for
  the spark plasma sintering process",,"Results in Physics, Elsevier, 2017, 7, pp.1494-1497","10.1016/j.rinp.2017.04.020",,"physics.app-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The stability ofthe proportional--integral--derivative (PID)controlof
temperature in the spark plasma sintering (SPS) process is investigated.ThePID
regulationsof this process are tested fordifferent SPS toolingdimensions,
physical parameters conditions,andareas of temperature control. It isshown
thatthe PID regulation quality strongly depends on the heating time lag between
the area of heat generation and the area of the temperature control. Tooling
temperature rate maps arestudied to revealpotential areas forhighlyefficientPID
control.The convergence of the model and experiment indicatesthat even with
non-optimal initial PIDcoefficients, it is possible to reduce the temperature
regulation inaccuracy to less than 4K by positioning the temperature control
location in highlyresponsiveareas revealed by the finite-element calculationsof
the temperature spatial distribution.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:11:34 GMT""}]","2020-11-25"
"2011.11634","Charles Maniere","Gabriel Kerbart (CRISMAT), Charles Mani\`ere (CRISMAT), Christelle
  Harnois (CRISMAT), Sylvain Marinel (CRISMAT)","Master sintering curve with dissimilar grain growth trajectories: A case
  study on MgAl2O4",,"Journal of the European Ceramic Society, Elsevier, 2021, 41 (1),
  pp.1048-1051","10.1016/j.jeurceramsoc.2020.09.003",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Sintering is a key step in the processing of high performance ceramics. Both
the density and the grain size play a crucial role on the ceramic sintering
kinetics and the final material properties. The master sintering curve (MSC) is
a well-known tool for exploring sintering models kinetics. However, the
conventional MSC theory assumes a unique sintering trajectory, while our study
on MgAl2O4 spinel shows dissimilar growth response. Park's MSC theory has been
applied and compared with the conventional MSC approach for obtaining the
activation energy with and without dissimilar grain growth trajectories.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:13:46 GMT""}]","2020-11-25"
"2011.11635","Sebastian Friedemann","Sebastian Friedemann (DATAMOVE), Bruno Raffin (DATAMOVE)","An elastic framework for ensemble-based large-scale data assimilation",,,,,"cs.CE cs.DC physics.comp-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Prediction of chaotic systems relies on a floating fusion of sensor data
(observations) with a numerical model to decide on a good system trajectory and
to compensate nonlinear feedback effects. Ensemble-based data assimilation (DA)
is a major method for this concern depending on propagating an ensemble of
perturbed model realizations.In this paper we develop an elastic, online,
fault-tolerant and modular framework called Melissa-DA for large-scale
ensemble-based DA. Melissa-DA allows elastic addition or removal of compute
resources for state propagation at runtime. Dynamic load balancing based on
list scheduling ensuresefficient execution. Online processing of the data
produced by ensemble members enables to avoid the I/O bottleneck of file-based
approaches. Our implementation embeds the PDAF parallel DA engine, enabling the
use of various DA methods. Melissa-DA can support extra ensemble-based
DAmethods by implementing the transformation of member background states into
analysis states. Experiments confirm the excellent scalability of Melissa-DA,
running on up to 16,240 cores, to propagate 16,384 members for a regional
hydrological critical zone assimilation relying on theParFlow model on a domain
with about 4 M grid cells.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:23:43 GMT""},{""version"":""v2"",""created"":""Wed, 25 Nov 2020 08:23:29 GMT""}]","2020-11-26"
"2011.11844","Naoya Takahashi","Naoya Takahashi, Yuki Mitsufuji","Densely connected multidilated convolutional networks for dense
  prediction tasks","Accepted to CVPR 2021. arXiv admin note: text overlap with
  arXiv:2010.01733",,,,"cs.CV cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Tasks that involve high-resolution dense prediction require a modeling of
both local and global patterns in a large input field. Although the local and
global structures often depend on each other and their simultaneous modeling is
important, many convolutional neural network (CNN)-based approaches interchange
representations in different resolutions only a few times. In this paper, we
claim the importance of a dense simultaneous modeling of multiresolution
representation and propose a novel CNN architecture called densely connected
multidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution
that has different dilation factors in a single layer to model different
resolutions simultaneously. By combining the multidilated convolution with the
DenseNet architecture, D3Net incorporates multiresolution learning with an
exponentially growing receptive field in almost all layers, while avoiding the
aliasing problem that occurs when we naively incorporate the dilated
convolution in DenseNet. Experiments on the image semantic segmentation task
using Cityscapes and the audio source separation task using MUSDB18 show that
the proposed method has superior performance over state-of-the-art methods.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 05:15:12 GMT""},{""version"":""v2"",""created"":""Wed, 9 Jun 2021 00:31:49 GMT""}]","2021-06-10"
"2011.11846","Tien Dung Nguyen","Tien-Dung Nguyen, Bogdan Gabrys and Katarzyna Musial","AutoWeka4MCPS-AVATAR: Accelerating Automated Machine Learning Pipeline
  Composition and Optimisation","arXiv admin note: substantial text overlap with arXiv:2001.11158",,,,"cs.LG stat.CO","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Automated machine learning pipeline (ML) composition and optimisation aim at
automating the process of finding the most promising ML pipelines within
allocated resources (i.e., time, CPU and memory). Existing methods, such as
Bayesian-based and genetic-based optimisation, which are implemented in
Auto-Weka, Auto-sklearn and TPOT, evaluate pipelines by executing them.
Therefore, the pipeline composition and optimisation of these methods
frequently require a tremendous amount of time that prevents them from
exploring complex pipelines to find better predictive models. To further
explore this research challenge, we have conducted experiments showing that
many of the generated pipelines are invalid in the first place, and attempting
to execute them is a waste of time and resources. To address this issue, we
propose a novel method to evaluate the validity of ML pipelines, without their
execution, using a surrogate model (AVATAR). The AVATAR generates a knowledge
base by automatically learning the capabilities and effects of ML algorithms on
datasets' characteristics. This knowledge base is used for a simplified mapping
from an original ML pipeline to a surrogate model which is a Petri net based
pipeline. Instead of executing the original ML pipeline to evaluate its
validity, the AVATAR evaluates its surrogate model constructed by capabilities
and effects of the ML pipeline components and input/output simplified mappings.
Evaluating this surrogate model is less resource-intensive than the execution
of the original pipeline. As a result, the AVATAR enables the pipeline
composition and optimisation methods to evaluate more pipelines by quickly
rejecting invalid pipelines. We integrate the AVATAR into the sequential
model-based algorithm configuration (SMAC). Our experiments show that when SMAC
employs AVATAR, it finds better solutions than on its own.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 14:05:49 GMT""}]","2020-11-25"
"2011.12202","Alain Rapaport","Nik Cunniffe, Fr\'ed\'eric Hamelin, Abderrahman Iggidr, Alain
  Rapaport, Gauthier Sallet","Observability, Identifiability and Epidemiology -- A survey",,,,,"math.DS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this review, we recall the concepts of Identifiability and Observability
of dynamical systems, and analyse them in the framework of Mathematical
Epidemiology. We show that, even for simple and well known models of the
literature, these properties are not always fulfilled. We then consider the
problem of practical identifiability and observability, which are connected to
sensitivity and numerical condition numbers. We also recall the concept of
observers to reconstruct state variable of the model which are not observed,
and show how it can used with epidemiological models.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 16:16:50 GMT""},{""version"":""v2"",""created"":""Thu, 7 Oct 2021 12:53:17 GMT""},{""version"":""v3"",""created"":""Fri, 19 May 2023 20:40:15 GMT""}]","2023-05-23"
"2011.12402","Charles Maniere","Gabriel Kerbart (CRISMAT), Charles Mani\`ere (CRISMAT), Christelle
  Harnois (CRISMAT), Sylvain Marinel (CRISMAT)","Predicting final stage sintering grain growth affected by porosity",,"Applied Materials Today, Elsevier, 2020, 20, pp.100759","10.1016/j.apmt.2020.100759",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Grain growth has a definitive impact on the quality of transparent sintered
materials in areas such as ballistics, biomaterials, jewelry, etc. Controlling
the sintering trajectory at the precise moment of final stage sintering is one
of the main sintering challenges for obtaining highperformance, fully-dense
nano-ceramics. However, the final stage of sintering involves a very complex
coupling between the rate of porosity elimination/grain growth and transition
mechanisms. This complexity makes predicting the sintering trajectory very
difficult, and most transparent material production escapes this problem by
using expensive high-pressure methods such as hot isostatic pressing (HIP). In
the quest for a pressureless transparent material process, this paper addresses
the challenge of predicting grain growth in the transition domain from the
grain growth onset (in a high porosity region) to full density for MgAl 2 O 4
spinel. We present a comprehensive modeling approach linking theoretical models
such as Zhao \& Harmer's and Olevsky's equations to accurately predict the
complex grain growth transition region of final stage sintering. This modeling
approach opens up the possibility for numerical exploration of microstructure
development via underlying kinetics experimental identification.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:48:21 GMT""}]","2020-11-26"
"2011.12403","Charles Maniere","Charles Mani\`ere (SDSU), Tony Zahrah, Eugene A. Olevsky (SDSU)","Inherent heating instability of direct microwave sintering process:
  Sample analysis for porous 3Y-ZrO2",,"Scripta Materialia, Elsevier, 2017, 128, pp.49-52","10.1016/j.scriptamat.2016.10.008",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Direct microwave heating of 3Y-ZrO 2 is studied at frequency of 2.45 GHz.
Different conditions of input power, sample position and size are tested. For
the first time, the experimentally known instability of microwave sintering is
explained coupling the effective medium approximation and finite-element
method. We show how the material dielectric permittivity imaginary part which
increases with temperature and relative density encourages high hot spot
phenomena. It is shown that the sample location has a great impact on the
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:53:20 GMT""}]","2020-11-26"
"2011.12404","Charles Maniere","Charles Mani\`ere (SDSU), Shirley Chan (SDSU), Eugene A. Olevsky
  (SDSU)","Microwave sintering of complex shapes: From multiphysics simulation to
  improvements of process scalability",,"Journal of the American Ceramic Society, Wiley, 2019, 102 (2),
  pp.611-620","10.1111/jace.15892",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The microwave sintering homogeneity of large and complex shape specimens is
analyzed. A new approach enabling the fabrication of complex shapes ceramics
via 3D printing and microwave sintering is presented. The use of a dental
microwave cavity is shown to enable a substantial level of densification of
complex shape components while restricting the grain growth. The homogeneity of
the processed samples during microwave sintering is studied by an
electromagnetic-thermal-mechanical simulation. The realistic densification
behavior, that phenomenologically takes into account the microwave effect, is
included in the modeling framework. The simulation indicates the sharp
correlation between the microwave field distribution in the cavity, the
temperature profile, and the specimen's shape distortion.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:57:15 GMT""}]","2020-11-26"
"2011.12709","Andrew McGough","Amir Atapour-Abarghouei, Andrew Stephen McGough, David Stanley Wall","Resolving the cybersecurity Data Sharing Paradox to scale up
  cybersecurity via a co-production approach towards data sharing","10 pages",,,,"cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  As cybercriminals scale up their operations to increase their profits or
inflict greater harm, we argue that there is an equal need to respond to their
threats by scaling up cybersecurity. To achieve this goal, we have to develop a
co-productive approach towards data collection and sharing by overcoming the
cybersecurity data sharing paradox. This is where we all agree on the
definition of the problem and end goal (improving cybersecurity and getting rid
of cybercrime), but we disagree about how to achieve it and fail to work
together efficiently. At the core of this paradox is the observation that
public interests differ from private interests. As a result, industry and law
enforcement take different approaches to the cybersecurity problem as they seek
to resolve incidents in their own interests, which manifests in different data
sharing practices between both and also other interested parties, such as
cybersecurity researchers. The big question we ask is can these interests be
reconciled to develop an interdisciplinary approach towards co-operation and
sharing data. In essence, all three will have to co-own the problem in order to
co-produce a solution. We argue that a few operational models with good
practices exist that provide guides to a possible solution, especially multiple
third-party ownership organisations which consolidate, anonymise and analyse
data. To take this forward, we suggest the practical solution of organising
co-productive data collection on a sectoral basis, but acknowledge that common
standards for data collection will also have to be developed and agreed upon.
We propose an initial set of best practices for building collaborations and
sharing data and argue that these best practices need to be developed and
standardised in order to mitigate the paradox.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 22:23:31 GMT""}]","2021-08-23"
"2011.14008","Charles Maniere","Charles Mani\`ere (SDSU), Tony Zahrah, Eugene A. Olevsky (SDSU)","Fully coupled electromagnetic-thermal-mechanical comparative simulation
  of direct vs hybrid microwave sintering of 3Y-ZrO 2",,"Journal of the American Ceramic Society, Wiley, 2017, 100 (6),
  pp.2439-2450","10.1111/jace.14762",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Direct and hybrid microwave sintering of 3Y-ZrO 2 are comparatively studied
at frequency of 2.45 GHz. Using the continuum theory of sintering, a fully
coupled electromagnetic-thermalmechanical (EMTM) finite element simulation is
carried out to predict powder samples deformation during their microwave
processing. Direct and hybrid heating configurations are computationally tested
using advanced heat transfer simulation tools including the surface to surface
thermal radiation boundary conditions and a numeric
proportional-integral-derivative (PID) regulation. The developed modeling
framework shows a good agreement of the calculation results with the known
experimental data on the microwave sintering of 3Y-ZrO 2 in terms of the
densification kinetics. It is shown that the direct heating configuration
renders highly hot spot effects resulting in non-homogenous densification
causing processed specimen's final shape distortions. Compared to the direct
heating, the hybrid heating configuration provides a reduction of the thermal
inhomogeneity along with a densification homogenization. As a result of the
hybrid heating, the total densification of the specimen is attained without
specimen distortions. It is also shown that the reduction of the sample size
has a stabilization effect on the temperature and relative density spatial
distributions.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:10:28 GMT""}]","2020-12-01"
"2011.14009","Charles Maniere","Charles Mani\`ere (SDSU), Geuntak Lee (SDSU), Elisa Torresani (SDSU),
  John F. Gerling, Vadim V. Yakovlev, Darold Martin, Eugene Olevsky (SDSU)","Flash microwave pressing of zirconia",,"Journal of the American Ceramic Society, Wiley, 2020","10.1111/jace.17072",,"physics.app-ph cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Microwave Pressing is a promising way to reduce microwave sintering
temperatures and stabilize microwave powder materials processing. A
multi-physics simulation was conducted of the regulated pressure-assisted
microwave cavity. This simulation took into consideration resonance phenomena
and the nonlinear temperature-dependent material parameters of zirconia. The
intrinsic behaviors of microwave systems and zirconia make the regulation of
the microwave pressing difficult. However, the same phenomena can be used to
activate flash sintering. Flash microwave sintering uses high electric fields
of the resonant microwave profile, the Negative Temperature Behavior (NTC) of
zirconia resistivity, and the mechanical pressure applied to the powder via a
die compaction configuration. The resulting flash microwave pressing still
needs improvement in terms of the processed material structure homogeneity, but
it has the capacity to become the fastest sintering treatment as it allows room
temperature activation where the total process time only takes a few seconds.
In addition, this 10-20s processing technique has shown good potential for
improving the transparency of alumina pre-sintered specimens.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:09:28 GMT""}]","2020-12-01"
"2011.14010","Charles Maniere","Geuntak Lee (SDSU), Charles Mani\`ere (SDSU), Joanna Mckittrick
  (UCSD), Eugene A. Olevsky (SDSU)","Electric current effects in spark plasma sintering: From the evidence of
  physical phenomenon to constitutive equation formulation",,"Scripta Materialia, Elsevier, 2019, 170, pp.90-94","10.1016/j.scriptamat.2019.05.040",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The 3 heating modes are utilized to make ZrN powders have 3 different levels
of the electric current density at the same temperature during spark plasma
sintering (SPS). The constitutive equation of sintering for SPS is applied to
the experimental porosity evolution of ZrN from three SPS modes, and this
showed high electric current density increase the electric current assisted
deformability value of ZrN pellets, resulting in a reduction of the flow
stress. The electric current flow enhances the dislocation motion, which was
experimentally proved and analyzed by modified Williamson-Hall equation
applying to X-ray diffraction results, and the mechanical strength test of ZrN
pellets.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:07:02 GMT""}]","2020-12-01"
"2011.14011","Charles Maniere","Charles Mani\`ere (SDSU), Geuntak Lee (SDSU), Joanna Mckittrick
  (UCSD), Andrey Maximenko (SDSU), Eugene A. Olevsky (SDSU)","Graphite creep negation during flash spark plasma sintering under
  temperatures close to 2000$^\circ$C",,"Carbon, Elsevier, 2020, 162, pp.106-113","10.1016/j.carbon.2020.02.027",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Graphite creep has high importance for applications using high pressures (100
MPa) and temperatures close to 2000 {\textdegree}C. In particular, the new
flash spark plasma sintering process (FSPS) is highly sensitive to graphite
creep when applied to ultra-high temperature materials such as silicon carbide.
In this flash process taking only a few seconds, the graphite tooling reaches
temperatures higher than 2000 {\textdegree}C resulting in its irreversible
deformation. The graphite tooling creep prevents the flash spark plasma
sintering process from progressing further. In this study, a finite element
model is used to determine FSPS tooling temperatures. In this context, we
explore the graphite creep onset for temperatures above 2000 {\textdegree}C and
for high pressures. Knowing the graphite high temperature limit, we modify the
FSPS process so that the sintering occurs outside the graphite creep range of
temperatures/pressures. 95 % dense silicon carbide compacts are obtained in
about 30 s using the optimized FSPS.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:05:45 GMT""}]","2020-12-01"
"2011.14012","Charles Maniere","Charles Mani\`ere (SDSU), Geuntak Lee (SDSU), Eugene A. Olevsky (SDSU)","All-Materials-Inclusive Flash Spark Plasma Sintering",,"Scientific Reports, Nature Publishing Group, 2017, 7 (1)","10.1038/s41598-017-15365-x",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A new flash (ultra-rapid) spark plasma sintering method applicable to various
materials systems, regardless of their electrical resistivity, is developed. A
number of powders ranging from metals to electrically insulative ceramics have
been successfully densified resulting in homogeneous microstructures within
sintering times of 8-35 s. A finite element simulation reveals that the
developed method, providing an extraordinary fast and homogeneous heating
concentrated in the sample's volume and punches, is applicable to all the
different samples tested. The utilized uniquely controllable flash phenomenon
is enabled by the combination of the electric current concentration around the
sample and the confinement of the heat generated in this area by the lateral
thermal contact resistance. The presented new method allows: extending flash
sintering to nearly all materials, controlling sample shape by an added
graphite die, and an energy efficient mass production of small and intermediate
size objects. This approach represents also a potential venue for future
investigations of flash sintering of complex shapes.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:01:29 GMT""}]","2020-12-01"
"2011.14014","Arshia Anjum","Arshia Anjum, Sriman Srisa Saran Mishra","The Timeline Of Gravity","25 Pages",,,,"physics.pop-ph physics.hist-ph","http://creativecommons.org/licenses/by/4.0/","  Gravity plays an important part in the experiments and discoveries of the
modern world. But how was it discovered? Surely Newton and Einstein were not
the only people to observe it and account for it. It had been a long path
before the full theory for Gravitation could be formulated with open ends for
more add-ons and modifications. All the contributions from across the world and
different eras helped in the discovery of gravity as a whole new concept and
area of research with a major contribution from the Greeks. This 3 article
series lists out the important curves in the carefully carved path of
gravitational discovery. The first article summarises the development of
interest in the cosmos and the growth of scientific knowledge through ancient
theories and observations.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 20:09:40 GMT""}]","2020-12-01"
"2011.14878","Ian Covert","Ian Covert, Scott Lundberg, Su-In Lee","Explaining by Removing: A Unified Framework for Model Explanation","Updated with JMLR revisions, arXiv admin note: text overlap with
  arXiv:2011.03623",,,,"cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Researchers have proposed a wide variety of model explanation approaches, but
it remains unclear how most methods are related or when one method is
preferable to another. We describe a new unified class of methods,
removal-based explanations, that are based on the principle of simulating
feature removal to quantify each feature's influence. These methods vary in
several respects, so we develop a framework that characterizes each method
along three dimensions: 1) how the method removes features, 2) what model
behavior the method explains, and 3) how the method summarizes each feature's
influence. Our framework unifies 26 existing methods, including several of the
most widely used approaches: SHAP, LIME, Meaningful Perturbations, and
permutation tests. This newly understood class of explanation methods has rich
connections that we examine using tools that have been largely overlooked by
the explainability literature. To anchor removal-based explanations in
cognitive psychology, we show that feature removal is a simple application of
subtractive counterfactual reasoning. Ideas from cooperative game theory shed
light on the relationships and trade-offs among different methods, and we
derive conditions under which all removal-based explanations have
information-theoretic interpretations. Through this analysis, we develop a
unified framework that helps practitioners better understand model explanation
tools, and that offers a strong theoretical foundation upon which future
explainability research can build.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 00:47:48 GMT""},{""version"":""v2"",""created"":""Fri, 13 May 2022 03:43:44 GMT""}]","2022-05-16"
"2012.00676","Aranyak Chakravarty","Aranyak Chakravarty, Mahesh V. Panchagnula, Alladi Mohan and Neelesh
  A. Patankar","Pulmonary drug delivery and retention: a computational study to identify
  plausible parameters based on a coupled airway-mucus flow model","24 pages",,,,"q-bio.TO physics.bio-ph physics.flu-dyn","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pulmonary drug delivery systems rely on inhalation of drug-laden aerosols
produced from aerosol generators such as inhalers, nebulizers etc. On
deposition, the drug molecules diffuse in the mucus layer and are also
subjected to mucociliary advection which transports the drugs away from the
initial deposition site. The availability of the drug at a particular region of
the lung is, thus, determined by a balance between these two phenomena. A
mathematical analysis of drug deposition and retention in the lungs is
developed through a coupled mathematical model of aerosol transport in air as
well as drug molecule transport in the mucus layer. The mathematical model is
solved computationally to identify suitable conditions for the transport of
drug-laden aerosols to the deep lungs. This study identifies the conditions
conducive for delivering drugs to the deep lungs which is crucial for achieving
systemic drug delivery. The effect of different parameters on drug retention is
also characterized for various regions of the lungs, which is important in
determining the availability of the inhaled drugs at a target location. Our
analysis confirms that drug delivery efficacy remains highest for aerosols in
the size range of 1-5 $\mu$m. Moreover, it is observed that amount of drugs
deposited in the deep lung increases by a factor of 2 when the breathing time
period is doubled, with respect to normal breathing, suggesting breath control
as a means to increase the efficacy of drug delivery to the deep lung. A higher
efficacy also reduces the drug load required to be inhaled to produce the same
health effects and hence, can help in minimizing the side effects of a drug.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 10:25:42 GMT""},{""version"":""v2"",""created"":""Fri, 15 Jan 2021 17:56:23 GMT""},{""version"":""v3"",""created"":""Wed, 28 Apr 2021 16:11:18 GMT""},{""version"":""v4"",""created"":""Thu, 13 Jan 2022 11:03:01 GMT""}]","2022-01-14"
"2012.01167","Albert Paytaren","Albert V. Paytaren","Seminar and Training Programs Recommender System for Faculty Members of
  Higher Education Institution",,"International Journal of Computing Sciences Research (ISSN print:
  2546-0552; ISSN online: 2546-115X) Vol. 4, No. 4, pp. 359-369, 2020","10.25147/ijcsr.2017.001.1.43",,"cs.CY","http://creativecommons.org/licenses/by/4.0/","  This study aims to develop a personalized Recommender System that helps to
address the problems encountered by the faculty members of Higher Education
Institutions in the selection of Seminar and Training Programs (STP). The
researcher used the Descriptive Developmental Method of research to gather
information relevant to the current problems and challenges encountered and
used these to develop software that addresses the identified challenges. For
the development of the software, the researcher adopted a step-wise approach
defined in the Incremental Developmental Model. The level of acceptance of the
developed system was evaluated by 24 faculty respondents. The level of
acceptance of the developed system was classified into functionality,
reliability, and usability and the study garnered an evaluation score of 4.65,
4.67, and 4.67 respectively. The overall interpretation of the results of the
evaluation is Highly Acceptable. The study created a system that provides
seminars and training program recommendations. The developed recommender system
was rated Highly Acceptable, respondents were very satisfied with the features
of the system and agreed that it was functional, reliable, and usable.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 00:53:51 GMT""}]","2020-12-03"
"2012.01188","Chinelo Oribhabor","Chinelo Blessing Oribhabor","Investigating the Influence of Computer Anxiety on the Academic
  Performance of Junior Secondary School Students in Computer Studies in
  Nigeria","12 pages","International Journal of Computing Sciences Research Vol. 4, No.
  4, (2020) pp. 370-382",,"ISSN print: 2546-0552; ISSN online: 2546-115X","cs.CY","http://creativecommons.org/licenses/by/4.0/","  This study examined the influence of computer anxiety on the academic
performance of junior secondary school students in Computer Studies in Nigeria.
The research instrument that was used in the study was computer anxiety scale
which was validated by the Guidance and Counseling lecturers and Educational
Measurement and Evaluation experts. The result of the study showed that most of
the students used in the study were mildly anxious when dealing with computer.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 12:32:23 GMT""}]","2020-12-03"
"2012.01470","Chris Cummins","Chris Cummins, Hugh Leather, Zacharias Fisches, Tal Ben-Nun, Torsten
  Hoefler, Michael O'Boyle","Deep Data Flow Analysis","9 pages, plus appendices. arXiv admin note: text overlap with
  arXiv:2003.10536",,,,"cs.PL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Compiler architects increasingly look to machine learning when building
heuristics for compiler optimization. The promise of automatic heuristic
design, freeing the compiler engineer from the complex interactions of program,
architecture, and other optimizations, is alluring. However, most machine
learning methods cannot replicate even the simplest of the abstract
interpretations of data flow analysis that are critical to making good
optimization decisions. This must change for machine learning to become the
dominant technology in compiler heuristics.
  To this end, we propose ProGraML - Program Graphs for Machine Learning - a
language-independent, portable representation of whole-program semantics for
deep learning. To benchmark current and future learning techniques for compiler
analyses we introduce an open dataset of 461k Intermediate Representation (IR)
files for LLVM, covering five source programming languages, and 15.4M
corresponding data flow results. We formulate data flow analysis as an MPNN and
show that, using ProGraML, standard analyses can be learned, yielding improved
performance on downstream compiler optimization tasks.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 03:29:14 GMT""}]","2020-12-04"
"2012.02033","Baohua Sun","Baohua Sun, Michael Lin, Hao Sha, Lin Yang","SuperOCR: A Conversion from Optical Character Recognition to Image
  Captioning","8 pages, 2 figures, 2 tables",,,,"cs.CV eess.IV","http://creativecommons.org/licenses/by/4.0/","  Optical Character Recognition (OCR) has many real world applications. The
existing methods normally detect where the characters are, and then recognize
the character for each detected location. Thus the accuracy of characters
recognition is impacted by the performance of characters detection. In this
paper, we propose a method for recognizing characters without detecting the
location of each character. This is done by converting the OCR task into an
image captioning task. One advantage of the proposed method is that the labeled
bounding boxes for the characters are not needed during training. The
experimental results show the proposed method outperforms the existing methods
on both the license plate recognition and the watermeter character recognition
tasks. The proposed method is also deployed into a low-power (300mW) CNN
accelerator chip connected to a Raspberry Pi 3 for on-device applications.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 06:40:04 GMT""}]","2020-12-04"
"2012.02237","Mark Philip Sy","Mark Philip M. Sy, Christian James M. Historillo, Allen Cris T. Conde,
  and Ma. Yvonne Czarina R. Costelo","Query Game 2.0: Improvement of a Web-Based Query Game for Cavite State
  University Main Campus",,"International Journal of Computing Sciences Research, 4(2),
  304-318","10.25147/ijcsr.2017.001.1.41",,"cs.CY cs.SE","http://creativecommons.org/licenses/by/4.0/","  Purpose: The study aimed to improve the previous study covering a web-based
query game for Cavite State University. The study created a new mechanic and
gameplay for the students to learn Structured Query Language (SQL). The
enhancements also focused on the interactions of one or more students playing
the game.
  Method: The researchers used iterative development process methodology in the
development of the study. The system was assessed and evaluated using different
testing methods: unit, integration, and system testing. After passing the
tests, 90 students of Information Technology and Computer Science program along
with 10 IT experts evaluated the system.
  Results: The respondents were classified into technical and non-technical
respondents and the study garnered an evaluation score of 4.69 and 4.70
respectively. The overall interpretation of the results of the evaluation is
Excellent.
  Conclusion: The study created a system where the user can read and watch
lectures and experience the tutorial. Instructors and students may communicate
in the system, hence, promoting better relations and healthy competition
between students.
  Recommendations: Based on the conclusions of the study, the system can be
used as a supplementary tool in teaching courses with Database Management. To
enhance the analysis of query construction of the students, it is recommended
to add a module that can analyze the pattern of creating queries and answering
questions for every user.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 06:43:05 GMT""}]","2020-12-07"
"2012.02242","Fateme Sarkohaki","Mina Zaminkar, Fateme Sarkohaki, Reza Fotohi","A method based on encryption and node rating for securing the RPL
  protocol communications in the IoT ecosystem","24 pages, 11 figures, 6 tables","Int J Commun Syst. 2020;e4693","10.1002/dac.4693",,"cs.CR cs.CY","http://creativecommons.org/licenses/by/4.0/","  Internet of Things (IoT) provides the possibility for milliards of devices
throughout the world to communicate with each other, and data is collected
autonomously. The big data generated by the devices should be managed securely.
Due to security challenges, like malicious nodes, many approaches cannot
respond to these concerns. In this paper, a robust hybrid method, including
encryption, is used as an efficient approach for resolving the RPL protocol
concerns so that the devices are connected securely. Therefore, the proposed
DSH-RPL method for securing the RPL protocol comprises the four following
phases: The first phase creates a reliable RPL. The second phase detects the
sinkhole attack. The third phase quarantines the detected malicious node, and
the fourth phase transmits data through encryption. The simulation results show
that the DSH-RPL reduces the false-positive rate more than 18.2% and 23.1%, and
reduces the false-negative rate more than 16.1% and 22.78%, it also increases
the packet delivery rate more than 19.68% and 25.32% and increases the
detection rate more than 26% and 31% compared to SecTrust-RPL and IBOOS-RPL.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 09:27:38 GMT""}]","2020-12-07"
"2012.03669","Challiz Omorog","C. D. Omorog, R. P. Medina","Internet Security Awareness of Filipinos: A Survey Paper","13 pages","IJCSR. Vol 1 No. 4 (2018) 14-26","10.25147/ijcsr.2017.001.1.18",,"cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Purpose. This paper examines the Internet security perception of Filipinos to
establish a need and sense of urgency on the part of the government to create a
culture of cybersecurity for every Filipino. Method. A quantitative survey was
conducted through traditional, online, and phone interviews among 252
respondents using a two-page questionnaire that covers basic demographic
information and two key elements (1) Internet usage and (2) security practices.
Results. Based on findings, there is a sharp increase in Internet users for the
last three years (50%), and most access to the Internet through mobile (94.4%).
Although at home is the most frequent location for Internet access (94.4%), a
good percentage still use free WiFi access points available in malls (22.2%),
restaurants (11.1%), and other public areas (38.9%) doing Internet services
(email and downloading) that are vulnerable to cyberattacks. The study also
revealed that although respondents may have good knowledge of Internet security
software, proper implementation is very limited. Conclusion. Filipinos are
susceptible to cyberattacks, particularly to phishing and malware attacks.
Also, the majority of the respondents' Internet security perception is
derivative: they practice online measures but with a limited understanding of
the purpose. Therefore proper education, through training and awareness, is an
effective approach to remedy the situation. Recommendations. The Philippine
government must now take actions and tap industries to educate Filipinos about
Internet security before any negative consequences happen in the future.
Research Implications. The information collected sets a clear picture of the
importance of cybersecurity awareness from a regional to a global perspective.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 02:56:27 GMT""}]","2020-12-08"
"2012.06311","Oluwafemi Azeez","Ibrahim Yusuf, George Igwegbe, Oluwafemi Azeez","Differentiable Histogram with Hard-Binning","Accepted at Blacks in AI Workshop, NeurIPS 2020",,,,"cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  The simplicity and expressiveness of a histogram render it a useful feature
in different contexts including deep learning. Although the process of
computing a histogram is non-differentiable, researchers have proposed
differentiable approximations, which have some limitations. A differentiable
histogram that directly approximates the hard-binning operation in conventional
histograms is proposed. It combines the strength of existing differentiable
histograms and overcomes their individual challenges. In comparison to a
histogram computed using Numpy, the proposed histogram has an absolute
approximation error of 0.000158.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:52:03 GMT""}]","2020-12-14"
"2012.07601","Atul Pokharel","Atul Pokharel (1), Robert Soul\'e (2), Avi Silberschatz (2) ((1) New
  York University, (2) Yale University)","A case for location based contact tracing","14 pages, 7 figures, submitted to Healthcare Management Science",,,,"physics.soc-ph q-bio.PE stat.AP","http://creativecommons.org/licenses/by/4.0/","  We present an evaluation of the effectiveness of manual contact tracing
compared to bulletin board contact tracing. We show that bulletin board contact
tracing gives comparable results in terms of the reproductive number, duration,
prevalence and incidence but is less resource intensive, easier to implement
and offers a wider range of privacy options. Classical contact tracing focuses
on contacting individuals whom an infectious person has been in proximity to. A
bulletin board approach focuses on identifying locations visited by an
infectious person, and then contacting those who were at those locations. We
present results comparing their effects on the overall reproductive number as
well as the incidence and prevalence of disease. We evaluate them by building a
new discrete time stochastic model based on the Susceptible Exposed Infectious
and Recovered (SEIR) framework for disease spread. We conduct simulation
experiments to quantify the effectiveness of these two models of contact
tracing by calibrating the model to be compatible with SARS-CoV-2. Our
experiments show that location-based bulletin board contact tracing can improve
manual contact tracing.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 17:19:48 GMT""}]","2020-12-15"
"2012.09613","Ying Fan","Ying Fan, Yifei Ming","Model-based Reinforcement Learning for Continuous Control with Posterior
  Sampling","Accepted to ICML 2021","Proceedings of the 38th International Conference on Machine
  Learning, PMLR 139:3078-3087, 2021",,,"cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Balancing exploration and exploitation is crucial in reinforcement learning
(RL). In this paper, we study model-based posterior sampling for reinforcement
learning (PSRL) in continuous state-action spaces theoretically and
empirically. First, we show the first regret bound of PSRL in continuous spaces
which is polynomial in the episode length to the best of our knowledge. With
the assumption that reward and transition functions can be modeled by Bayesian
linear regression, we develop a regret bound of $\tilde{O}(H^{3/2}d\sqrt{T})$,
where $H$ is the episode length, $d$ is the dimension of the state-action
space, and $T$ indicates the total time steps. This result matches the
best-known regret bound of non-PSRL methods in linear MDPs. Our bound can be
extended to nonlinear cases as well with feature embedding: using linear
kernels on the feature representation $\phi$, the regret bound becomes
$\tilde{O}(H^{3/2}d_{\phi}\sqrt{T})$, where $d_\phi$ is the dimension of the
representation space. Moreover, we present MPC-PSRL, a model-based posterior
sampling algorithm with model predictive control for action selection. To
capture the uncertainty in models, we use Bayesian linear regression on the
penultimate layer (the feature representation layer $\phi$) of neural networks.
Empirical results show that our algorithm achieves the state-of-the-art sample
efficiency in benchmark continuous control tasks compared to prior model-based
algorithms, and matches the asymptotic performance of model-free algorithms.
","[{""version"":""v1"",""created"":""Fri, 20 Nov 2020 21:00:31 GMT""},{""version"":""v2"",""created"":""Tue, 16 Nov 2021 20:29:49 GMT""}]","2021-11-18"
"2012.12872","Charles Maniere","Charles Mani\`ere (SDSU), Geuntak Lee (SDSU), Joanna Mckittrick
  (UCSD), Eugene A. Olevsky (SDSU)","Energy efficient spark plasma sintering: Breaking the threshold of large
  dimension tooling energy consumption",,"Journal of the American Ceramic Society, Wiley, 2018","10.1111/jace.16046",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An energy efficient spark plasma sintering method enabling the densification
of large size samples assisted by very low electric current levels is
developed. In this method, the electric current is concentrated in the graphite
foils around the sample. High energy dissipation is then achieved in this area
enabling the heating and full densification of large (alumina) parts ({{\O}} 40
mm) at relatively low currents (800 A). The electrothermal mechanical
simulation reveals that the electric current needed to heat the large samples
is 70 % lower in the energy efficient configuration compared to the traditional
configuration. The presence of thermal and densification gradients is also
revealed for the larger size samples. Potential solutions for this problem are
discussed. The experiments confirm the possibility of full densification (96-99
%) of large alumina samples. This approach allows using small (and low cost)
SPS devices (generally limited to 10-15 mm samples) for large size samples
(40-50 mm). The developed technique enables also an optimized energy
consumption by large scale SPS systems.
","[{""version"":""v1"",""created"":""Sat, 21 Nov 2020 11:02:20 GMT""}]","2020-12-24"
